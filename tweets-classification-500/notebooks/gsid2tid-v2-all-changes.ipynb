{"cells":[{"cell_type":"markdown","metadata":{"id":"PEWY6iMCOrxf"},"source":["# google scholar search"]},{"cell_type":"markdown","metadata":{"id":"VlhwyVP3QbSU"},"source":["### Imports"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":477205,"status":"ok","timestamp":1678875453539,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"X4OwlpQLgPhH","outputId":"0a5d8ba9-b41d-4558-e15d-b9afc15c95ff"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70148,"status":"ok","timestamp":1678875523671,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"GMEIh6fFkiLk","outputId":"7b3112d1-ed1a-4b03-9679-991433fca47e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Executing: /tmp/apt-key-gpghome.UyioIJfRcn/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n","gpg: key DCC9EFBF77E11517: public key \"Debian Stable Release Key (10/buster) <debian-release@lists.debian.org>\" imported\n","gpg: Total number processed: 1\n","gpg:               imported: 1\n","Executing: /tmp/apt-key-gpghome.RtNw31tTdC/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n","gpg: key DC30D7C23CBBABEE: public key \"Debian Archive Automatic Signing Key (10/buster) <ftpmaster@debian.org>\" imported\n","gpg: Total number processed: 1\n","gpg:               imported: 1\n","Executing: /tmp/apt-key-gpghome.9nyVyW0f8l/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n","gpg: key 4DFAB270CAA96DFA: public key \"Debian Security Archive Automatic Signing Key (10/buster) <ftpmaster@debian.org>\" imported\n","gpg: Total number processed: 1\n","gpg:               imported: 1\n","Warning: apt-key output should not be parsed (stdout is not a terminal)\n","Warning: apt-key output should not be parsed (stdout is not a terminal)\n","Warning: apt-key output should not be parsed (stdout is not a terminal)\n","Get:1 http://deb.debian.org/debian buster InRelease [122 kB]\n","Get:2 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n","Hit:3 http://archive.ubuntu.com/ubuntu focal InRelease\n","Get:4 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n","Get:5 http://deb.debian.org/debian buster-updates InRelease [56.6 kB]\n","Get:6 http://deb.debian.org/debian-security buster/updates InRelease [34.8 kB]\n","Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n","Get:8 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n","Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1,581 B]\n","Hit:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n","Get:11 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n","Get:12 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB]\n","Get:13 http://deb.debian.org/debian buster/main amd64 Packages [10.7 MB]\n","Hit:14 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n","Get:15 http://deb.debian.org/debian buster-updates/main amd64 Packages [9,745 B]\n","Get:16 http://deb.debian.org/debian-security buster/updates/main amd64 Packages [597 kB]\n","Get:17 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [920 kB]\n","Hit:18 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n","Hit:20 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n","Get:21 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3,019 kB]\n","Hit:22 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n","Get:23 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,312 kB]\n","Get:24 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,544 kB]\n","Get:25 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,397 kB]\n","Get:26 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,136 kB]\n","Fetched 23.2 MB in 4s (6,397 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  chromium-common chromium-sandbox libevent-2.1-6 libicu63 libimobiledevice6\n","  libjpeg62-turbo libplist3 libre2-5 libu2f-udev libusbmuxd6 libvpx5\n","  libxxf86dga1 upower usbmuxd x11-utils\n","Suggested packages:\n","  chromium-l10n chromium-shell libusbmuxd-tools mesa-utils\n","The following NEW packages will be installed:\n","  chromium chromium-common chromium-driver chromium-sandbox libevent-2.1-6\n","  libicu63 libimobiledevice6 libjpeg62-turbo libplist3 libre2-5 libu2f-udev\n","  libusbmuxd6 libvpx5 libxxf86dga1 upower usbmuxd x11-utils\n","0 upgraded, 17 newly installed, 0 to remove and 27 not upgraded.\n","Need to get 74.6 MB of archives.\n","After this operation, 256 MB of additional disk space will be used.\n","Get:1 http://deb.debian.org/debian buster/main amd64 libevent-2.1-6 amd64 2.1.8-stable-4 [177 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libre2-5 amd64 20200101+dfsg-1build1 [162 kB]\n","Get:3 http://deb.debian.org/debian buster/main amd64 libicu63 amd64 63.1-6+deb10u3 [8,293 kB]\n","Get:4 http://deb.debian.org/debian buster/main amd64 libjpeg62-turbo amd64 1:1.5.2-2+deb10u1 [133 kB]\n","Get:5 http://deb.debian.org/debian buster/main amd64 libvpx5 amd64 1.7.0-3+deb10u1 [800 kB]\n","Get:6 http://deb.debian.org/debian buster/main amd64 chromium-common amd64 90.0.4430.212-1~deb10u1 [1,423 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu1 [12.0 kB]\n","Get:8 http://deb.debian.org/debian buster/main amd64 chromium amd64 90.0.4430.212-1~deb10u1 [58.3 MB]\n","Get:9 http://archive.ubuntu.com/ubuntu focal/main amd64 x11-utils amd64 7.7+5 [199 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu focal/main amd64 libplist3 amd64 2.1.0-4build2 [31.6 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 libusbmuxd6 amd64 2.0.1-2 [19.1 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 libimobiledevice6 amd64 1.2.1~git20191129.9f79242-1build1 [65.2 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu focal/main amd64 libu2f-udev all 1.1.10-1 [6,108 B]\n","Get:14 http://archive.ubuntu.com/ubuntu focal/main amd64 upower amd64 0.99.11-1build2 [104 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu focal/main amd64 usbmuxd amd64 1.1.1~git20191130.9af2b12-1 [38.4 kB]\n","Get:16 http://deb.debian.org/debian buster/main amd64 chromium-driver amd64 90.0.4430.212-1~deb10u1 [4,703 kB]\n","Get:17 http://deb.debian.org/debian buster/main amd64 chromium-sandbox amd64 90.0.4430.212-1~deb10u1 [146 kB]\n","Fetched 74.6 MB in 1s (76.1 MB/s)\n","Selecting previously unselected package libevent-2.1-6:amd64.\n","(Reading database ... 128275 files and directories currently installed.)\n","Preparing to unpack .../00-libevent-2.1-6_2.1.8-stable-4_amd64.deb ...\n","Unpacking libevent-2.1-6:amd64 (2.1.8-stable-4) ...\n","Selecting previously unselected package libicu63:amd64.\n","Preparing to unpack .../01-libicu63_63.1-6+deb10u3_amd64.deb ...\n","Unpacking libicu63:amd64 (63.1-6+deb10u3) ...\n","Selecting previously unselected package libjpeg62-turbo:amd64.\n","Preparing to unpack .../02-libjpeg62-turbo_1%3a1.5.2-2+deb10u1_amd64.deb ...\n","Unpacking libjpeg62-turbo:amd64 (1:1.5.2-2+deb10u1) ...\n","Selecting previously unselected package libre2-5:amd64.\n","Preparing to unpack .../03-libre2-5_20200101+dfsg-1build1_amd64.deb ...\n","Unpacking libre2-5:amd64 (20200101+dfsg-1build1) ...\n","Selecting previously unselected package libvpx5:amd64.\n","Preparing to unpack .../04-libvpx5_1.7.0-3+deb10u1_amd64.deb ...\n","Unpacking libvpx5:amd64 (1.7.0-3+deb10u1) ...\n","Selecting previously unselected package libxxf86dga1:amd64.\n","Preparing to unpack .../05-libxxf86dga1_2%3a1.1.5-0ubuntu1_amd64.deb ...\n","Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu1) ...\n","Selecting previously unselected package x11-utils.\n","Preparing to unpack .../06-x11-utils_7.7+5_amd64.deb ...\n","Unpacking x11-utils (7.7+5) ...\n","Selecting previously unselected package chromium-common.\n","Preparing to unpack .../07-chromium-common_90.0.4430.212-1~deb10u1_amd64.deb ...\n","Unpacking chromium-common (90.0.4430.212-1~deb10u1) ...\n","Selecting previously unselected package chromium.\n","Preparing to unpack .../08-chromium_90.0.4430.212-1~deb10u1_amd64.deb ...\n","Unpacking chromium (90.0.4430.212-1~deb10u1) ...\n","Selecting previously unselected package chromium-driver.\n","Preparing to unpack .../09-chromium-driver_90.0.4430.212-1~deb10u1_amd64.deb ...\n","Unpacking chromium-driver (90.0.4430.212-1~deb10u1) ...\n","Selecting previously unselected package chromium-sandbox.\n","Preparing to unpack .../10-chromium-sandbox_90.0.4430.212-1~deb10u1_amd64.deb ...\n","Unpacking chromium-sandbox (90.0.4430.212-1~deb10u1) ...\n","Selecting previously unselected package libplist3:amd64.\n","Preparing to unpack .../11-libplist3_2.1.0-4build2_amd64.deb ...\n","Unpacking libplist3:amd64 (2.1.0-4build2) ...\n","Selecting previously unselected package libusbmuxd6:amd64.\n","Preparing to unpack .../12-libusbmuxd6_2.0.1-2_amd64.deb ...\n","Unpacking libusbmuxd6:amd64 (2.0.1-2) ...\n","Selecting previously unselected package libimobiledevice6:amd64.\n","Preparing to unpack .../13-libimobiledevice6_1.2.1~git20191129.9f79242-1build1_amd64.deb ...\n","Unpacking libimobiledevice6:amd64 (1.2.1~git20191129.9f79242-1build1) ...\n","Selecting previously unselected package libu2f-udev.\n","Preparing to unpack .../14-libu2f-udev_1.1.10-1_all.deb ...\n","Unpacking libu2f-udev (1.1.10-1) ...\n","Selecting previously unselected package upower.\n","Preparing to unpack .../15-upower_0.99.11-1build2_amd64.deb ...\n","Unpacking upower (0.99.11-1build2) ...\n","Selecting previously unselected package usbmuxd.\n","Preparing to unpack .../16-usbmuxd_1.1.1~git20191130.9af2b12-1_amd64.deb ...\n","Unpacking usbmuxd (1.1.1~git20191130.9af2b12-1) ...\n","Setting up libplist3:amd64 (2.1.0-4build2) ...\n","Setting up libu2f-udev (1.1.10-1) ...\n","Failed to send reload request: No such file or directory\n","Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu1) ...\n","Setting up chromium-sandbox (90.0.4430.212-1~deb10u1) ...\n","Setting up libicu63:amd64 (63.1-6+deb10u3) ...\n","Setting up libjpeg62-turbo:amd64 (1:1.5.2-2+deb10u1) ...\n","Setting up libevent-2.1-6:amd64 (2.1.8-stable-4) ...\n","Setting up libusbmuxd6:amd64 (2.0.1-2) ...\n","Setting up x11-utils (7.7+5) ...\n","Setting up libre2-5:amd64 (20200101+dfsg-1build1) ...\n","Setting up chromium-common (90.0.4430.212-1~deb10u1) ...\n","Setting up libimobiledevice6:amd64 (1.2.1~git20191129.9f79242-1build1) ...\n","Setting up libvpx5:amd64 (1.7.0-3+deb10u1) ...\n","Setting up upower (0.99.11-1build2) ...\n","Setting up usbmuxd (1.1.1~git20191130.9af2b12-1) ...\n","Warning: The home dir /var/lib/usbmux you specified can't be accessed: No such file or directory\n","Adding system user `usbmux' (UID 107) ...\n","Adding new user `usbmux' (UID 107) with group `plugdev' ...\n","Not creating home directory `/var/lib/usbmux'.\n","Setting up chromium (90.0.4430.212-1~deb10u1) ...\n","update-alternatives: using /usr/bin/chromium to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n","update-alternatives: using /usr/bin/chromium to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n","Setting up chromium-driver (90.0.4430.212-1~deb10u1) ...\n","Processing triggers for man-db (2.9.1-1) ...\n","Processing triggers for dbus (1.12.16-2ubuntu2.3) ...\n","Processing triggers for mime-support (3.64ubuntu1) ...\n","Processing triggers for hicolor-icon-theme (0.17-2) ...\n","Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting selenium\n","  Downloading selenium-4.8.2-py3-none-any.whl (6.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3[socks]~=1.26 in /usr/local/lib/python3.9/dist-packages (from selenium) (1.26.15)\n","Collecting trio~=0.17\n","  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 KB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting trio-websocket~=0.9\n","  Downloading trio_websocket-0.10.0-py3-none-any.whl (17 kB)\n","Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.9/dist-packages (from selenium) (2022.12.7)\n","Collecting async-generator>=1.9\n","  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.9/dist-packages (from trio~=0.17->selenium) (2.4.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.9/dist-packages (from trio~=0.17->selenium) (2.10)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.9/dist-packages (from trio~=0.17->selenium) (22.2.0)\n","Collecting sniffio\n","  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n","Collecting outcome\n","  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n","Collecting exceptiongroup>=1.0.0rc9\n","  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n","Collecting wsproto>=0.14\n","  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n","Collecting h11<1,>=0.9.0\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sniffio, outcome, h11, exceptiongroup, async-generator, wsproto, trio, trio-websocket, selenium\n","Successfully installed async-generator-1.10 exceptiongroup-1.1.1 h11-0.14.0 outcome-1.2.0 selenium-4.8.2 sniffio-1.3.0 trio-0.22.0 trio-websocket-0.10.0 wsproto-1.2.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting jsonlines\n","  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.9/dist-packages (from jsonlines) (22.2.0)\n","Installing collected packages: jsonlines\n","Successfully installed jsonlines-3.1.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting twitter\n","  Downloading twitter-1.19.6-py2.py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.9/dist-packages (from twitter) (2022.12.7)\n","Installing collected packages: twitter\n","Successfully installed twitter-1.19.6\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tweepy in /usr/local/lib/python3.9/dist-packages (3.10.0)\n","Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.9/dist-packages (from tweepy) (2.25.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tweepy) (1.3.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.9/dist-packages (from tweepy) (1.15.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.2.2)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests[socks]>=2.11.1->tweepy) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests[socks]>=2.11.1->tweepy) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.26.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.9/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting fuzzysearch\n","  Downloading fuzzysearch-0.7.3.tar.gz (112 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 KB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: attrs>=19.3 in /usr/local/lib/python3.9/dist-packages (from fuzzysearch) (22.2.0)\n","Building wheels for collected packages: fuzzysearch\n","  Building wheel for fuzzysearch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fuzzysearch: filename=fuzzysearch-0.7.3-cp39-cp39-linux_x86_64.whl size=355083 sha256=93cdfccde7a14863db2f144e3d5106ecfded53f659190846ee6156e4fbdb3483\n","  Stored in directory: /root/.cache/pip/wheels/20/29/ab/55ae4e35024221de1fc9f14d4fafe1ba9dc461623225e120ec\n","Successfully built fuzzysearch\n","Installing collected packages: fuzzysearch\n","Successfully installed fuzzysearch-0.7.3\n"]},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":2}],"source":["%%shell\n","# Ubuntu no longer distributes chromium-browser outside of snap\n","#\n","# Proposed solution: https://askubuntu.com/questions/1204571/how-to-install-chromium-without-snap\n","\n","# Add debian buster\n","cat > /etc/apt/sources.list.d/debian.list <<'EOF'\n","deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main\n","deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main\n","deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main\n","EOF\n","\n","# Add keys\n","apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n","apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n","apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n","\n","apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg\n","apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg\n","apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg\n","\n","# Prefer debian repo for chromium* packages only\n","# Note the double-blank lines between entries\n","cat > /etc/apt/preferences.d/chromium.pref << 'EOF'\n","Package: *\n","Pin: release a=eoan\n","Pin-Priority: 500\n","\n","\n","Package: *\n","Pin: origin \"deb.debian.org\"\n","Pin-Priority: 300\n","\n","\n","Package: chromium*\n","Pin: origin \"deb.debian.org\"\n","Pin-Priority: 700\n","EOF\n","\n","# Install chromium and chromium-driver\n","apt-get update\n","apt-get install chromium chromium-driver\n","\n","# Install selenium\n","pip install selenium\n","pip install jsonlines\n","pip install twitter\n","pip install tweepy\n","pip install fuzzysearch"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":808,"status":"ok","timestamp":1678875524457,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"DljCawwaOrxf"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import pickle\n","import jsonlines\n","import csv\n","import tweepy\n","import re\n","from twitter import *\n","import json\n","import time\n","from tqdm import tqdm\n","import fuzzysearch\n","import random"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1678875524458,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"nyoJylT1BWYy"},"outputs":[],"source":["import random"]},{"cell_type":"markdown","metadata":{"id":"CK0fRZjArvz4"},"source":["###Utils"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":19,"status":"ok","timestamp":1678875524458,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"vRap3bc7UrUx"},"outputs":[],"source":["api_keys = {'api_key' : \"ADrRng0LqCv475XZqF6Lu2olc\",\\\n","'api_secrets' : \"IcnWhE1joWkn2hVjmrgT98sCi3OXq0TTxaLazHsIy35nEqw7zF\",\\\n","'bearer_token' : \"AAAAAAAAAAAAAAAAAAAAALMjPAEAAAAALve%2BAcWX%2Fpn0eAhm%2FR%2BT9i0I3Zw%3DYsHcrGc96BxCJThdtE9iul8xu0cfeDClfBI8kLkPnSqgT6GND5\",\\\n","'access_token' : \"1009495181882773506-XJsxpDikrg3zqcgH6PO9QFfAUpj8dW\",\\\n","'access_secret' : \"mXHhc09AVPJL4x6UYx6RoQFevdIAoxYMGPSY6p5XKWCYk\"}\n","driver_path = '/usr/bin/chromedriver'\n","# TODO - crawl from https://aideadlin.es/?sub=ML,CV,CG,NLP,RO,SP,DM,AP,KR\n","domain_keywords = ['nips', 'neurips', 'cvpr', 'aaai']\n","\n","def generate_or_keyword_list(query_dict: dict):\n","    \"\"\"Generate necessary keyword lists to help selecting final candidates.\"\"\"\n","    or_keyword_list = []\n","    or_keyword_dict = {}\n","    or_keyword_dict['gs_sid'] = ''\n","    domain_labels = []\n","    if 'expertise' in query_dict['profile']['content']:\n","        for keyword in query_dict['profile']['content']['expertise']:\n","            for key in keyword['keywords']:\n","                key = key.strip().lower()\n","                domain_labels.append(key)\n","    or_keyword_dict['domain_labels'] = domain_labels\n","\n","    coauthors = []\n","    if 'relations' in query_dict['profile']['content'] and len(query_dict['profile']['content']['relations']) > 0:\n","        for relation in query_dict['profile']['content']['relations']:\n","            coauthors.append(relation['name'])\n","    or_keyword_dict['coauthors'] = coauthors\n","\n","    if 'history' in query_dict['profile']['content'] and len(query_dict['profile']['content']['history']) > 0:\n","        tmp_dict = query_dict['profile']['content']['history'][0]\n","        if 'position' in tmp_dict:\n","            or_keyword_dict['position'] = tmp_dict['position']\n","        if 'institution' in tmp_dict:\n","            if 'domain' in tmp_dict['institution']:\n","                or_keyword_dict['email_suffix'] = tmp_dict['institution']['domain']\n","            if 'name' in tmp_dict['institution']:\n","                or_keyword_dict['organization'] = tmp_dict['institution']['name']\n","\n","    or_keyword_list.append(or_keyword_dict)\n","\n","    return or_keyword_list\n","\n","from difflib import SequenceMatcher\n","\n","def get_str_similarity(a: str, b: str) -> float:\n","    \"\"\"Calculate the similarity of two strings and return a similarity ratio.\"\"\"\n","    return SequenceMatcher(None, a, b).ratio()\n","\n","def combine_dicts_for_max_value(dict_1, dict_2):\n","    if dict_1 is None:\n","      return dict_2\n","    if dict_2 is None:\n","      return dict_1\n","    for item in dict_2.items():\n","        if item[0] in dict_1:\n","            dict_1[item[0]] = max(dict_1[item[0]], item[1])\n","        else:\n","            dict_1[item[0]] = item[1]\n","    return dict_1"]},{"cell_type":"markdown","metadata":{"id":"sgM6xDo9P47w"},"source":["### Scholar78kSearch"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1678875524459,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"iFhE-0YePq3U"},"outputs":[],"source":["import pandas as pd\n","import os\n","import re\n","import numpy as np\n","from typing import Union, List\n","\n","\n","class Scholar78kSearch():\n","    def __init__(self):\n","        self.get_78kdata()\n","        self.simple = False\n","        self.verbose = False\n","        self.print_true = False\n","\n","    def get_78kdata(self, source='gdrive'):\n","        \"\"\"Download and load the 78k dataset data.\n","        \n","        Parameters\n","        ----------\n","        source : default is 'gdrive'.\n","        \"\"\"\n","        # path_name = 'gs_scholars_all.npy'\n","        path_name = 'gs_scholars_new.npy'\n","        if source == 'gdrive':\n","            # import gdown\n","            # if not os.path.exists('source'):\n","            #     os.mkdir('source')\n","            # if not os.path.exists(f'source/{path_name}'):\n","            #     gdown.download(\n","            #         'https://self.drive.google.com/uc?id=1NTvn_HiGX3Lr0FtTw5ot3UxcdeNLsv7h',\n","            #         f'source/{path_name}'\n","            #     )\n","            # self.df = pd.DataFrame.from_records(np.load(f'source/{path_name}', allow_pickle=True))\n","            self.df = pd.DataFrame.from_records(np.load(\"/content/drive/My Drive/tweets-dataset/gs_scholars_new.npy\", allow_pickle=True))\n","        else:\n","            raise NotImplementedError\n","    \n","    def search_name(self, name: Union[str, list], query_dict: dict = None) -> List[dict]:\n","        \"\"\"Search scholar candidates given name in the 78k AI scholar dataset.\n","        \n","        Parameters\n","        ----------\n","        name : name of the scholar.\n","        query_dict : if this is given, the method will run <self._search_name_others_helper()>\n","\n","        Returns\n","        -------\n","        df_row_list : a list of response dictionaries.\n","        \n","        \"\"\"\n","        if type(name) is list:\n","            name_list = [name[0], name[-1]]\n","            name = f'{name[0]} {name[-1]}' \n","        elif type(name) is str:\n","            name_list = re.sub('[0-9_\\.\\(\\)\\[\\],]', '', name).split(' ')\n","        else:\n","            raise TypeError(f'Argument \"name\" passed to Scholar78kSearch.search_name has the wrong type.')\n","        df_row = self._search_name_only_helper(name, name_list)\n","        if df_row.shape[0] > 0 and query_dict is not None:\n","            df_row = self._search_name_others_helper(df_row, query_dict)\n","        if self.print_true:\n","            print(f'[Info] Found {df_row.shape[0]} scholars are in 78k data.')\n","            print(f'[Debug] Names: {df_row[\"name\"]}')\n","        if self.verbose:\n","            print(df_row)\n","        return self._deal_with_simple(df_row)\n","        # return df_row\n","\n","    def _deal_with_simple(self, df_row):\n","        if self.simple:\n","            df_row = df_row.loc[:, df_row.columns != 'papers']\n","        df_row = df_row.drop(['co_authors_all'], axis=1)\n","        return df_row.to_dict(orient='records')\n","\n","    def _search_name_only_helper(self, name, name_list):\n","        \"\"\"Helper function of search_name\n","\n","        Returns\n","        -------\n","        Boolean : found or not.\n","        DataFrame : if find else None.\n","        \"\"\"\n","        # find the scholar in our dataset\n","        name_df = self.df.loc[self.df['name'] == name].copy()\n","        name_list_df = self.df.loc[self.df['name'].str.contains(pat = f'^{name_list[0].capitalize()} .*{name_list[-1].capitalize()}', regex=True, case=False)].copy()\n","        return pd.concat([name_df, name_list_df]).drop_duplicates(subset=['url']).reset_index(drop=True)\n","\n","    def _search_name_others_helper(self, df_row, query_dict):\n","        # TODO: add a better filter more than by name\n","        return df_row"]},{"cell_type":"markdown","metadata":{"id":"rt7MW4cCP-Jo"},"source":["### GoogleSearch"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1678875524459,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"kEFrgb4FPupy"},"outputs":[],"source":["import re\n","import time\n","from typing import Union\n","# from selenium import webdriver\n","# from selenium.webdriver.chrome.options import ChromiumOptions\n","from selenium.webdriver.chromium.webdriver import ChromiumDriver\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.remote.errorhandler import NoSuchElementException\n","# from webdriver_manager.chrome import ChromeDriverManager\n","\n","from selenium import webdriver\n","from selenium.webdriver.chrome.options import Options\n","\n","# url = \"https://github.com/googlecolab/colabtools/issues/3347\" \n","\n","class GoogleSearch():\n","    \"\"\"Base class for performing web search on Google using REST API.\"\"\"\n","    def __init__(self, driver_path):\n","        self.setup_webdriver(driver_path)\n","    \n","    def setup_webdriver(self, driver_path):\n","        \"\"\"Setup the webdriver object.\"\"\"\n","\n","        options = Options()\n","        options.add_argument(\"--headless\")\n","        options.add_argument(\"--no-sandbox\")\n","        options.headless = True\n","        # options = ChromiumOptions()\n","        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n","        options.add_experimental_option('useAutomationExtension', False)\n","        # options.add_argument('--headless')\n","        # options.add_argument('--no-sandbox')\n","        options.add_argument('--disable-dev-shm-usage')\n","\n","        self.driver = webdriver.Chrome(driver_path, options=options)\n","\n","        # self.driver = webdriver.Chrome(driver_path, options=options)\n","        # # self.driver = webdriver.Chrome(ChromeDriverManager().install())\n","\n","        # self.driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n","        #     \"source\": \"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\"\n","        # })\n","\n","\n","class ScholarGsSearch(GoogleSearch):\n","    \"\"\"Class that handling searching on Google Scholar webpage using REST GET API.\"\"\"\n","    def __init__(self, driver_path):\n","        super().__init__(driver_path)\n","        self._authsearch = 'https://scholar.google.com/citations?hl=en&view_op=search_authors&mauthors={0}'\n","        self._gsidsearch = 'https://scholar.google.com/citations?hl=en&user={0}'\n","        self.print_true = False\n","        # print(\"ScholarGsSearch initiated\")\n","    \n","    def change_name(self, name):\n","        new_name = name[1:].split('_')\n","        new_name[-1] = re.sub(r'[0-9]+', '', new_name[-1])\n","        new_name = ' '.join(new_name)\n","        return new_name\n","\n","    def search_gsid(self, gs_sid: str, simple: bool = True):\n","        \"\"\"Search scholar on Google Scholar based given gs_sid.\n","        \n","        Parameters\n","        ----------\n","        gs_sid : google scholar sid\n","        simple : whether return simple information without paper list.\n","\n","        Returns\n","        -------\n","        scholar_dict_list : a list of dicts of responses.\n","        \n","        \"\"\"\n","        url = self._gsidsearch.format(gs_sid)\n","        self.search_gs_url(url, simple=simple)\n","        \n","    def search_gs_url(self, url: str, simple: bool = True):\n","        self.driver.get(url)\n","        scholar_dict = self._search_gsid_helper(self.driver, url, simple=simple)\n","        time.sleep(5)\n","        if scholar_dict is not None:\n","            \n","            return [scholar_dict]\n","        else:\n","            if self.print_true:\n","                print('[Info] No scholars found given gs_sid in search_gs.')\n","            return []\n","\n","        \n","    def _search_gsid_helper(self, driver: ChromiumDriver, url: str, simple: bool = True):\n","        \"\"\"Helper function for search_gsid.\"\"\"\n","\n","        def get_single_author(element):\n","            li=[]\n","            li.append(element.find_elements(By.TAG_NAME, \"a\")[0].get_attribute('href'))\n","            li.append(element.find_elements(By.TAG_NAME, \"a\")[0].get_attribute('textContent'))\n","            for i in element.find_elements(By.CLASS_NAME, \"gsc_rsb_a_ext\"):\n","                li.append(i.get_attribute('textContent'))\n","            return li\n","\n","        html_first_class = driver.find_elements(By.CLASS_NAME, \"gsc_g_hist_wrp\")\n","        if (len(html_first_class)==0):\n","            if self.print_true:\n","                print(\"[Info] len(html_first_class)==0\")\n","            return None\n","        idx_list = html_first_class[0].find_elements(By.CLASS_NAME, \"gsc_md_hist_b\")[0]\n","        years =  [i.get_attribute('textContent') for i in idx_list.find_elements(By.CLASS_NAME, \"gsc_g_t\")]\n","        cites =  [i.get_attribute('innerHTML') for i in idx_list.find_elements(By.CLASS_NAME, \"gsc_g_al\")]\n","        rsb = driver.find_elements(By.CLASS_NAME, \"gsc_rsb\")[0]\n","        Citations_table=[i.get_attribute('textContent') for i in  rsb.find_elements(By.CLASS_NAME, \"gsc_rsb_std\")]\n","        Co_authors = rsb.find_elements(By.CLASS_NAME, \"gsc_rsb_a\")\n","        if len(Co_authors) == 0:\n","            Co_authors = None\n","        else:\n","            Co_authors = [get_single_author(i) for i in rsb.find_element(By.CLASS_NAME, \"gsc_rsb_a\").find_elements(By.CLASS_NAME, \"gsc_rsb_a_desc\")]\n","\n","        Researcher = {\"url\": url}\n","        gs_sid = None\n","        if 'user=' in url:\n","            tmp_gs_sid = url.split('user=', 1)[1]\n","            if len(tmp_gs_sid) >= 12:\n","                gs_sid = tmp_gs_sid[:12]\n","        # gs_sid\n","        Researcher['gs_sid'] = gs_sid\n","        # coauthors that are listed at the lower right of the profile page\n","        Researcher[\"coauthors\"] = Co_authors\n","        # citation table\n","        Researcher[\"citation_table\"] = [Citations_table[0], Citations_table[2]]\n","        # time series citations\n","        Researcher[\"cites\"] = {\"years\":years, \"cites\":cites}\n","        # name\n","        nameList = driver.find_elements(By.ID, \"gsc_prf_in\")\n","        if (len(nameList) != 1):\n","            if self.print_true:\n","                print(\"len(nameList)!=1\")\n","            return None\n","        Researcher[\"name\"] = nameList[0].text\n","        # organization\n","        infoList = driver.find_elements(By.CLASS_NAME, 'gsc_prf_il')\n","        Researcher['organization'] = infoList[0].get_attribute('textContent')\n","        # homepage\n","        homepage_url = infoList[1].find_elements(By.TAG_NAME, 'a')\n","        if len(homepage_url) == 0:\n","            Researcher['homepage_url'] = None\n","        else:\n","            Researcher['homepage_url'] = homepage_url[0].get_attribute('href')\n","        # email address\n","        email_str_match = re.search(r'[\\w-]+\\.[\\w.-]+', infoList[1].text)\n","        if email_str_match is not None:\n","            Researcher['email_info'] = email_str_match.group(0)\n","        # domain labels\n","        Researcher['domain_labels'] = [i.get_attribute('textContent').strip().lower() for i in infoList[2].find_elements(By.CLASS_NAME, 'gsc_prf_inta')]\n","        # if not simple, get paper lists\n","        if not simple:\n","            button = driver.find_elements(By.CLASS_NAME, 'gs_btnPD')\n","            if (len(button) != 1):\n","                if self.print_true:\n","                    print(\"len(button)!=1\")\n","                return None\n","            while (button[0].is_enabled()):\n","                while (button[0].is_enabled()):\n","                    while (button[0].is_enabled()):\n","                        button[0].click()\n","                        time.sleep(5)\n","                    time.sleep(1)\n","                time.sleep(2)\n","            papers = []\n","            items = driver.find_elements(By.CLASS_NAME, 'gsc_a_tr')\n","            for i in items:\n","                item = i.find_element(By.CLASS_NAME, 'gsc_a_at')\n","                url = item.get_attribute(\"href\")\n","                paper_info=[j.text for j in i.find_elements(By.CLASS_NAME, 'gs_gray')]\n","                cite = i.find_element(By.CLASS_NAME, 'gsc_a_ac')\n","                year = i.find_element(By.CLASS_NAME, 'gsc_a_y').find_element(By.CLASS_NAME, \"gsc_a_h\").text\n","                papers.append([url, item.text, \n","                                paper_info,\n","                            cite.text, cite.get_attribute(\"href\"),\n","                            year])\n","            Researcher[\"papers\"] = papers\n","\n","        def generate_single_coauthor(element):\n","            coauthor_dict = {\n","                \"name\":element.find_elements(By.CLASS_NAME, 'gs_ai_name')[0].get_attribute('textContent'),\n","                \"url\":element.find_elements(By.CLASS_NAME, 'gs_ai_pho')[0].get_attribute('href'),\n","                \"description\":element.get_attribute('innerHTML'),\n","            }\n","            return coauthor_dict\n","        extra_coauthors = driver.find_elements(By.CLASS_NAME, \"gsc_ucoar\")\n","        Researcher['extra_co_authors'] = [generate_single_coauthor(i) for i in extra_coauthors]\n","        return Researcher\n","\n","    def search_name(self, name: Union[str, list], query_dict: dict = None, top_n=3, simple=True):\n","        \"\"\"Search on Google Scholar webpage given name.\n","        \n","        Parameters\n","        ----------\n","        name : name of the scholar.\n","        query_dict : a dict containing information of the scholar.\n","        top_n : select <top_n> candidates.\n","        simple : whether return simple information without paper list.\n","\n","        Returns\n","        -------\n","        resp : list of candidate scholars, empty if no candidates are found.\n","\n","        \"\"\"\n","        if type(name) is list:\n","            # current case\n","            name_list = [name[0], name[-1]]\n","            name = f'{name[0]} {name[-1]}' \n","        elif type(name) is str:\n","            name_list = name.split(' ')\n","        else:\n","            raise TypeError('Argument \"name\" passed to ScholarGsSearch.search_name has the wrong type.')\n","        url_fragment = f'{name} '\n","        if query_dict is not None:\n","            # first try (name, email_suffix, position, organization) as url\n","            keyword_list = generate_or_keyword_list(query_dict)[0]\n","            url_fragment_new = url_fragment\n","            # if 'email_suffix' in keyword_list:\n","            #     url_fragment_new = url_fragment_new + keyword_list['email_suffix'] + ' '\n","            # if 'position' in keyword_list:\n","            #     url_fragment_new = url_fragment_new + keyword_list['position'] + ' '\n","            # if 'organization' in keyword_list:\n","            #     url_fragment_new = url_fragment_new + keyword_list['organization'] + ' '\n","\n","            # url = self._authsearch.format(url_fragment_new)\n","            # self.driver.get(url)\n","            # time.sleep(5)\n","            # scholar_list = self._search_name_helper(self.driver, name_list)\n","            # if len(scholar_list) > 0:\n","            #     if wo_full:\n","            #         return scholar_list\n","            #     else:\n","            #         return self._search_name_list_expand(scholar_list, simple=simple)\n","            \n","            # second try (name, email_suffix)\n","            if 'email_suffix' in keyword_list:\n","                url_fragment_new = url_fragment + keyword_list['email_suffix'] # + ' '\n","            url = self._authsearch.format(url_fragment_new)\n","            self.driver.get(url)\n","            time.sleep(5)\n","            scholar_list = self._search_name_helper(self.driver, name_list)\n","            # return scholar_list\n","            if len(scholar_list) > 0:\n","                if self.print_true:\n","                    print(f'[Info] Find {len(scholar_list)} scholars using query without gs_sid in step 1.')\n","                # return self._search_name_list_expand(scholar_list, simple=simple)\n","                return scholar_list\n","        \n","            # third try (name, position)\n","            if 'position' in keyword_list:\n","                url_fragment_new = url_fragment + keyword_list['position'] # + ' '\n","            url = self._authsearch.format(url_fragment_new)\n","            self.driver.get(url)\n","            time.sleep(5)\n","            scholar_list = self._search_name_helper(self.driver, name_list)\n","            # return scholar_list\n","            if len(scholar_list) > 0:\n","                if self.print_true:\n","                    print(f'[Info] Find {len(scholar_list)} scholars using query without gs_sid in step 2.')\n","                # return self._search_name_list_expand(scholar_list, simple=simple)\n","                return scholar_list\n","\n","            # fourth try (name, organization)\n","            if 'organization' in keyword_list:\n","                url_fragment_new = url_fragment + keyword_list['organization'] # + ' '\n","            url = self._authsearch.format(url_fragment_new)\n","            self.driver.get(url)\n","            time.sleep(5)\n","            scholar_list = self._search_name_helper(self.driver, name_list)\n","            # return scholar_list\n","            if len(scholar_list) > 0:\n","                if self.print_true:\n","                    print(f'[Info] Find {len(scholar_list)} scholars using query without gs_sid in step 3.')\n","                # return self._search_name_list_expand(scholar_list, simple=simple)\n","                return scholar_list\n","\n","        # finally, only search (name: firstname and lastname). If only one response returns, mark it as candidate\n","        url = self._authsearch.format(url_fragment)\n","        self.driver.get(url)\n","        time.sleep(5)\n","        scholar_list = self._search_name_helper(self.driver, name_list)\n","        if len(scholar_list) > 0:\n","        # if len(scholar_list) > 0 and len(scholar_list) <= top_n:\n","            if self.print_true:\n","                print(f'[Info] Find {len(scholar_list)} scholars using query without gs_sid in step 4.')\n","            # return self._search_name_list_expand(scholar_list, simple=simple)\n","            return scholar_list\n","        \n","        return []\n","\n","    def _search_name_helper(self, driver, name_list):\n","        \"\"\"Helper function of <self.search_name()>.\"\"\"\n","        # iterate over searched list, find dicts that contains the name (including)\n","        useful_info_list = driver.find_elements(By.CLASS_NAME, 'gs_ai_t')\n","        useful_info_ext_list = []\n","        if len(useful_info_list) != 0:\n","            for scholar_webdriver in useful_info_list:\n","                name = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_name').get_attribute('textContent').strip()\n","                # check whether name is correct\n","                not_a_candidate = False\n","                for name_fragment in name_list:\n","                    if name_fragment.lower() not in name.lower():\n","                        not_a_candidate = True\n","                        break\n","                if not_a_candidate:\n","                    continue\n","                \n","                # grab all the other information\n","                pos_org = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_aff').get_attribute('textContent').strip()\n","                email_str = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_eml').get_attribute('textContent').strip()\n","                cite = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_cby').get_attribute('textContent').strip()\n","                url = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_name').find_element(By.TAG_NAME, 'a').get_attribute('href').strip()\n","                domain_labels = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_int').find_elements(By.CLASS_NAME, 'gs_ai_ont_int')\n","                for idx, domain in enumerate(domain_labels):\n","                    domain_labels[idx] = domain.get_attribute('textContent').strip().lower()\n","\n","                # continue processing\n","                gs_sid = None\n","                if 'user=' in url:\n","                    tmp_gs_sid = url.split('user=', 1)[1]\n","                    if len(tmp_gs_sid) >= 12:\n","                        gs_sid = tmp_gs_sid[:12]\n","\n","                if email_str is not None and email_str != '':\n","                    match = re.search(r'[\\w-]+\\.[\\w.-]+', email_str)\n","                    email_str = match.group(0)\n","\n","                cites = [int(s) for s in cite.split() if s.isdigit()]\n","                useful_info_ext_list.append({\n","                    'name': name,\n","                    'pos_org': pos_org,\n","                    'email': email_str,\n","                    'cite': cites[0] if len(cites)>0 else None,\n","                    'url': url,\n","                    'gs_sid': gs_sid,\n","                    'domain_labels': domain_labels\n","                })\n","        return useful_info_ext_list\n","        \n","    def _search_name_list_expand(self, scholar_list, simple=True):\n","        \"\"\"Expand the name_list to full_name_list.\"\"\"\n","        new_scholar_list = []\n","        for scholar in scholar_list:\n","            if 'gs_sid' in scholar:\n","                url = self._gsidsearch.format(scholar['gs_sid'])\n","                self.driver.get(url)\n","                scholar_dict = self._search_gsid_helper(self.driver, url, simple=simple)\n","                if scholar_dict is not None:\n","                    new_scholar_list.append(scholar_dict)\n","                time.sleep(5)\n","        return new_scholar_list"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1678875524459,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"-F3mp0N7QIro"},"outputs":[],"source":["from difflib import SequenceMatcher\n","\n","def get_str_similarity(a: str, b: str) -> float:\n","    \"\"\"Calculate the similarity of two strings and return a similarity ratio.\"\"\"\n","    return SequenceMatcher(None, a, b).ratio()"]},{"cell_type":"markdown","metadata":{"id":"43Ozad0mQCKZ"},"source":["### ScholarSearch"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1678875524460,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"DVB-Cz04PNOv"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import pickle\n","import json\n","import typing\n","from typing import List, Union\n","import os\n","import re\n","import time\n","import sys\n","import requests\n","from bs4 import BeautifulSoup\n","\n","\n","class ScholarSearch():\n","    \"\"\"A class that handles searching over Google Scholar profiles and the 78k AI scholar dataset.\"\"\"\n","    def __init__(self):\n","        # attributes\n","        self.similarity_ratio = 0.8\n","        # self.driver_path = '../chromedriver'\n","        self.driver_path = '/usr/bin/chromedriver'\n","    \n","    def setup(self):\n","        # self.get_profiles(['review_data/area_chair_id_to_profile.json', 'review_data/reviewer_id_to_profile.json'])\n","        # self.get_profiles(None)\n","        self.search_78k = Scholar78kSearch()\n","        # print(\"Scholar78kSearch initiated\" )\n","        self.search_gs = ScholarGsSearch(self.driver_path)\n","\n","    def reset(self):\n","        pass\n","\n","    def get_profiles(self, filepath_list: List[str] = None) -> None:\n","        \"\"\"In case that you want to get responses of a list of scholars, \n","        the method is implemented for you to load (could be multiple) json data files.\n","\n","        Parameters\n","        ----------\n","        filepath_list : list of json data filepaths to load.\n","\n","        \"\"\"\n","        if filepath_list is None:\n","            return\n","        # set of json data dicts\n","        self.profile = {}\n","        for filepath in filepath_list:\n","            with open(filepath) as file:\n","                profile = json.load(file)\n","                self.profile.update(profile)\n","        # number of unique json data dicts in total\n","        # print(f'Number of unique json data dicts in total: {len(self.profile)}')\n","\n","    def get_scholar(\n","        self,\n","        query: Union[str, dict],\n","        field: List[str] = None,\n","        simple: bool = True,\n","        top_n: int = 3,\n","        print_true: bool = True) -> List[dict]:\n","        \"\"\"Get up to <top_n> relevant candidate scholars by searching over Google Scholar profiles and the 78k AI scholar dataset.\n","        \n","        Parameters\n","        ----------\n","        query : a query containing the known scholar information.\n","        field : a list of fields wants to return. If not given, by default full information will be returned.\n","        simple : whether return simple information without paper list. This works only if the argument <field> is not specified.\n","        top_n : return at most <top_n> scholars if the result is not considered as determined.\n","        print_true : print info / debug info of the search process.\n","\n","        Returns\n","        -------\n","        resp : list of candidate scholars, empty if no candidates are found.\n","\n","        \"\"\"\n","\n","        self.search_78k.simple = simple\n","        self.search_78k.print_true = print_true\n","        self.search_gs.print_true = print_true\n","        self.print_true = print_true\n","        self.reset()\n","\n","        scholar_cnt = 0\n","        if type(query) is dict:\n","            # query is dict\n","            resp = self.search_dict(query, simple=simple, top_n=top_n)\n","        elif type(query) is str:\n","            # query is str\n","            resp = self.search_name(query, simple=simple, top_n=top_n)                \n","        else:\n","            raise TypeError(f'[Error] The argument \"query\" must be str or dict, not {type(query)}.')\n","\n","        \n","        # select specific features\n","        if field is not None:\n","            resp_final = []\n","            for resp_item in resp:\n","                resp_dict = {}\n","                for field_item in field:\n","                    if field_item not in resp_item:\n","                        raise KeyError(f'The key {field_item} is not in the response dictionary')\n","                    \n","                    resp_dict[field_item] = resp_item[field_item]\n","                resp_dict['gs_sid'] = resp_item['gs_sid']\n","                resp_dict['url'] = resp_item['url']\n","                resp_dict['citation_table'] = resp_item['citation_table']\n","                resp_final.append(resp_dict)\n","            if print_true:\n","                scholar_cnt = len(resp_final)\n","                if scholar_cnt == 1:\n","                    print(f'[Info] In total 1 scholar is found:')\n","                else:\n","                    print(f'[Info] In total {scholar_cnt} scholars are found:')\n","                resp_str = json.dumps(resp_final, indent=2)\n","                print(resp_str)\n","            return resp_final\n","        else:\n","            if print_true:\n","                scholar_cnt = len(resp)\n","                if scholar_cnt == 1:\n","                    print(f'[Info] In total 1 scholar is found:')\n","                else:\n","                    print(f'[Info] In total {scholar_cnt} scholars are found:')\n","                resp_str = json.dumps(resp, indent=2)\n","                print(resp_str)\n","            return resp\n","    \n","    def search_name(self, name: str, simple: bool = True, top_n: int = 3, from_dict: bool = False, query_dict: dict = None) -> List[dict]:\n","        \"\"\"Search gs profile given name or OpenReview id.\n","        \n","        Parameters\n","        ----------\n","        name : the name of the scholar ([first_name last_name]).\n","        simple : whether return simple information without paper list. This works only if the argument <field> is not specified.\n","        top_n : return at most <top_n> scholars if the result is not considered as determined.\n","        from_dict : default = False. Should be true only if using <get_scholar()> class method.\n","        query_dict : default = None. Should be a dict only if using <get_scholar()> class method.\n","\n","        Returns\n","        -------\n","        resp : list of candidate scholars, empty if no candidates are found.\n","        \"\"\"\n","\n","        self.search_78k.simple = simple\n","        name = name.strip()\n","        dict = None\n","        real_name = True\n","        # OpenReview id\n","        if ' ' not in name and name[0] == '~':\n","            # search over chair id\n","            if name in self.profile:\n","                dict = self.profile[name]\n","            # crawl http api response\n","            if dict is not None and not from_dict:\n","                # name\n","                real_name = False\n","                resp = self.search_dict(dict, simple=simple, top_n=top_n)\n","            else:\n","                # get real name\n","                or_name = name # string\n","                name = name[1:].split('_')\n","                name[-1] = re.sub(r'[0-9]+', '', name[-1]) # list\n","                # name = ' '.join(name) # e.g., Rachel K. E. Bellamy\n","        else:\n","            or_name = name.split(' ') # list\n","            # name string\n","        if real_name:\n","            if from_dict:\n","                print('Not find by gs_sid, search from_dict')\n","                # it inputs a real name (firstname, lastname)\n","                resp = self.search_78k.search_name(name, query_dict)\n","                resp_gs = self.search_gs.search_name(name, query_dict, top_n=top_n, simple=simple)\n","                resp = self.select_final_cands(resp, top_n, query_dict=query_dict, resp_gs_prop={'resp_gs': resp_gs})\n","            else:\n","                # or_resp = self.get_or_scholars(or_name)\n","                # TODO: resp_gs for only searching name is not implemented\n","                # resp = self.select_final_cands(resp, or_resp, top_n, simple=simple)\n","                resp = self.search_78k.search_name(name)\n","                resp_gs = self.search_gs.search_name(name, query_dict=None, top_n=top_n, simple=simple)\n","                resp = self.select_final_cands(resp, top_n, query_dict=None, resp_gs_prop={'resp_gs': resp_gs})\n","        return resp\n","    \n","\n","    def get_or_scholars(self, or_name: Union[str, list]):\n","        \"\"\"Get OpenReview candidate scholars list by name through http api response.\"\"\"\n","        # format the name list to get OpenReview rest api response\n","        if type(or_name) is list:\n","            or_name_list = []\n","            if len(or_name) >= 2:\n","                id_list = []\n","                for idx, name_part in enumerate(or_name):\n","                    if idx == 0 or idx == len(or_name) - 1:\n","                        id_list.append(name_part.capitalize())\n","                    else:\n","                        if len(name_part) > 1:\n","                            id_list.append(f'{name_part[0].upper()}.') # middle name in abbreviate form\n","                        else:\n","                            id_list.append(name_part.upper())\n","                if len(id_list) == 2:\n","                    or_name_list.append(f'~{id_list[0]}_{id_list[-1]}')\n","                elif len(id_list) > 2:\n","                    or_name_list.append(f'~{id_list[0]}_{id_list[-1]}')\n","                    tmp_str = '_'.join(id_list)\n","                    or_name_list.append(f'~{tmp_str}')\n","            else:\n","                raise ValueError('Argument \"or_name\" passed to get_or_scholars is not a valid name list.')\n","        elif type(or_name) is str:\n","            or_name_list = [or_name]\n","        else:\n","            raise TypeError(f'Argument \"or_name\" passed to get_or_scholars has the wrong type.')\n","        del or_name\n","\n","        # get request response\n","        go_ahead = True\n","        resp_list = []\n","        for name in or_name_list:\n","            if name[-1].isnumeric():\n","                name_cur = name\n","                go_ahead = False\n","                name_cur_cnt = 1\n","            else:\n","                name_cur_cnt = 1\n","                name_cur = f'{name}{name_cur_cnt}'\n","\n","            # set accumulative count\n","            acc_cnt = 0\n","            while acc_cnt <= 1:\n","                response = requests.get(f'https://openreview.net/profile?id={name_cur}')\n","                time.sleep(1)\n","\n","                if not response.ok:\n","                    acc_cnt += 1\n","                else:\n","                    soup = BeautifulSoup(response.content.decode('utf-8'), 'html.parser')\n","                    resp_list.append(json.loads(soup.find_all('script', id=\"__NEXT_DATA__\")[0].string))\n","                name_cur_cnt += 1\n","                name_cur = f'{name}{name_cur_cnt}'\n","                if not go_ahead:\n","                    break\n","        if self.print_true:\n","            if len(resp_list) != 1:\n","                print(f'[Info] Found {len(resp_list)} scholars using OpenReview REST API.')\n","            else:\n","                print(f'[Info] Found 1 scholar using OpenReview REST API.')\n","        return resp_list \n","        # NOTE: the dict in this list is in a different format than the dict from OpenReview dataset.\n","\n","    def select_final_cands(self, resp: List[dict], top_n: int, query_dict: dict = None, resp_gs_prop: dict = None, simple: bool = True) -> List[dict]:\n","        \"\"\"Select final candidates according to the response from OpenReview and 78k data.\n","        \n","        Parameters\n","        ----------\n","        resp : response from 78k dataset.\n","        or_resp : prepare the necessary key-value pairs to help filtering.\n","        top_n : return at most <top_n> scholars if the result is not considered as determined.\n","        query_dict : default = None. Should be a dict only if using <get_scholar()> class method.\n","        resp_gs_prop : dict containing the response from Google Scholar webpage.\n","        simple : whether return simple information without paper list. This works only if the argument <field> is not specified.\n","\n","        Returns\n","        -------\n","        resp : list of candidate scholars, empty if no candidates are found.\n","        \n","        \"\"\"\n","        # get useful data from or_resp\n","        if query_dict is not None:\n","            or_keyword_list = generate_or_keyword_list(query_dict)\n","\n","        # merge resp with resp_gs\n","        if resp_gs_prop is not None:\n","            resp_gs = resp_gs_prop['resp_gs']\n","            # if there are one candidate from google scholar pages, we throw out resp from 78k data.\n","            if len(resp_gs) == 1:\n","                resp = []\n","            # iterate over resp_gs\n","            for resp_gs_item in resp_gs:\n","                find_flag = False\n","                # gs_sid\n","                for resp_item in resp:\n","                    if resp_gs_item['gs_sid'] == resp_item['gs_sid']:\n","                        find_flag = True\n","                        break\n","                if find_flag:\n","                    continue\n","                # construct new prep\n","                # generate full dict\n","                self.search_gs.driver.get(resp_gs_item['url'])\n","                time.sleep(5)\n","                if query_dict is not None or (query_dict is None and len(resp) <= top_n):\n","                    resp_gs_full_item = self.search_gs._search_gsid_helper(self.search_gs.driver, resp_gs_item['url'], simple=simple)\n","                    if resp_gs_full_item is not None:\n","                        resp.append(resp_gs_full_item)\n","        \n","        if query_dict is None:\n","            return resp[:top_n]\n","\n","        # calculate rankings\n","        rank = {}\n","        for idx_cand, cand in enumerate(resp):\n","            rank[idx_cand] = []\n","            gs_sid_flag = 0\n","            cnt_true = [0] * len(or_keyword_list) \n","            cnt_all = 0\n","            cnt_true_rel = [0] * len(or_keyword_list) \n","            cnt_all_rel = 0\n","            for idx_or_scholar, or_scholar in enumerate(or_keyword_list):\n","                # gs_sid\n","                if 'gs_sid' in cand:\n","                    if cand['gs_sid'] == or_scholar['gs_sid']: \n","                        gs_sid_flag = 1\n","\n","                # domain_labels\n","                if cand['domain_labels'] is not None:\n","                    for cand_domain_tag in cand['domain_labels']:\n","                        cnt_all += 1\n","                        for or_domain_tag in or_scholar['domain_labels']:\n","                            if get_str_similarity(cand_domain_tag, or_domain_tag) >= self.similarity_ratio:\n","                                cnt_true[idx_or_scholar] += 1\n","                \n","                \n","                # relations\n","                cnt_all_rel = 0\n","                # print(cand)\n","                if cand['coauthors'] is not None:\n","                    for cand_coauth in cand['coauthors']:\n","                        cnt_all_rel += 1\n","                        for or_coauth in or_scholar['coauthors']:\n","                            if get_str_similarity(or_coauth, cand_coauth[1]) >= self.similarity_ratio:\n","                                cnt_true_rel[idx_or_scholar] += 1\n","                \n","            # get the rank list\n","            # gs_sid\n","            if gs_sid_flag:\n","                rank[idx_cand].append(1)\n","            else:\n","                rank[idx_cand].append(0)\n","            \n","            # domain_labels\n","            for i in range(len(cnt_true)):\n","                if cnt_all == 0:\n","                    cnt_true[i] = 0\n","                else:\n","                    cnt_true[i] = cnt_true[i] / cnt_all\n","            rank[idx_cand].append(max(cnt_true))\n","\n","            # relations\n","            for i in range(len(cnt_true_rel)):\n","                if cnt_all_rel == 0:\n","                    cnt_true_rel[i] = 0\n","                else:\n","                    cnt_true_rel[i] = cnt_true_rel[i] / cnt_all_rel\n","            rank[idx_cand].append(max(cnt_true_rel))\n","        \n","        # select final candidate\n","        final_idx = []\n","        for rank_idx in rank:\n","            if rank[rank_idx][0] == 1:\n","                final_idx.append(rank_idx)\n","        \n","        # TODO: or we can set weights to (relations, domain_tags) to rank the scholar candidates\n","        if len(final_idx) < top_n:\n","            domain_tag_rank = []\n","            relation_rank = []\n","            for rank_idx in sorted(rank.keys()):\n","                # print(rank_idx)\n","                domain_tag_rank.append(rank[rank_idx][1])\n","                relation_rank.append(rank[rank_idx][2])\n","            # print(domain_tag_rank, relation_rank)\n","            domain_tag_idxes = np.argsort(domain_tag_rank)[::-1]\n","            relation_idxes = np.argsort(relation_rank)[::-1]\n","            for idx in relation_idxes:\n","                if relation_rank[idx] == 0:\n","                    break\n","                if len(final_idx) < top_n:\n","                    if idx not in final_idx:\n","                        final_idx.append(idx)\n","                else:\n","                    break\n","            for idx in domain_tag_idxes:\n","                if domain_tag_rank[idx] == 0:\n","                    break\n","                if len(final_idx) < top_n:\n","                    if idx not in final_idx:\n","                        final_idx.append(idx)\n","                else:\n","                    break\n","            if len(final_idx) == 0 and len(rank.keys()) > 0:\n","                    for rank_idx in sorted(rank.keys()):\n","                        if len(final_idx) >= top_n:\n","                            break\n","                        else:\n","                            final_idx.append(rank_idx)\n","        # print(resp)\n","        # print(or_keyword_list)\n","        # print(rank)\n","        # print(final_idx)\n","        resp = [resp[i] for i in final_idx]\n","        return resp\n","\n","    def search_dict(self, query_dict: dict, simple: bool = True, top_n: int = 3):\n","        \"\"\"Search candidates given a dictionary.\n","        \n","        Parameters\n","        ----------\n","        query_dict : default = None. Should be a dict only if using <get_scholar()> class method.\n","        simple : whether return simple information without paper list. This works only if the argument <field> is not specified.\n","        top_n : return at most <top_n> scholars if the result is not considered as determined.\n","\n","        Returns\n","        -------\n","        resp : list of candidate scholars, empty if no candidates are found.\n","\n","        \"\"\"\n","        self.search_78k.simple = simple\n","        # gs_sid\n","        if 'gscholar' in query_dict['profile']['content'] and 'user=' in query_dict['profile']['content']['gscholar']:\n","            tmp_gs_sid = query_dict['profile']['content']['gscholar'].split('user=', 1)[1]\n","            if len(tmp_gs_sid) >= 12:\n","                gs_sid = tmp_gs_sid[:12]\n","                name_df = self.search_78k.df.loc[self.search_78k.df['gs_sid'] == gs_sid].copy()\n","                if name_df.shape[0] != 0:\n","                    print(f'[Info] Found a scholar using 78k gs_sid')\n","                    return self.search_78k._deal_with_simple(name_df)\n","                else:\n","                    print(f'[Info] Found a scholar using query dict gs_sid')\n","                    resp = self.search_gs.search_gsid(gs_sid, simple=simple)\n","                    if len(resp) > 0:\n","                        return resp\n","                    \n","        \n","        # search_name\n","        return self.search_name(query_dict['profile']['id'], simple=simple, top_n=top_n, from_dict=True, query_dict=query_dict)"]},{"cell_type":"markdown","metadata":{"id":"UpEts2ATBZ2v"},"source":["### TwitterUserSearch"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":632,"status":"ok","timestamp":1678875525078,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"8wiEo5c1A7k1"},"outputs":[],"source":["class TwitterUserSearch():\n","    \"\"\"Class that handling searching on Twitter using Tweepy\"\"\"\n","\n","    def __init__(self, api_keys, domain_keywords):\n","        self.domain_keywords = domain_keywords\n","        auth = tweepy.OAuthHandler(api_keys['api_key'], api_keys['api_secrets'])\n","        auth.set_access_token(api_keys['access_token'], api_keys['access_secret']) \n","        self.api = tweepy.API(auth)\n","        try:\n","            self.api.verify_credentials()\n","            print('Successful Authentication')\n","        except:\n","            print('Failed authentication')\n","    \n","    def get_tweets(self, twitter_name, num_tweets):\n","      try:\n","          tweets = self.api.user_timeline(screen_name=twitter_name, count=num_tweets, tweet_mode=\"extended\")\n","          tweets_list = []\n","          for i, tweet in enumerate(tweets):\n","              json_object = tweet._json\n","              tweet = json_object[\"full_text\"]\n","              if \"retweeted_status\" in json_object:\n","                  tweet = json_object['full_text'][:json_object['full_text'].find(\":\")+2]+json_object['retweeted_status']['full_text']\n","              tweets_list.append(tweet)\n","          return tweets_list\n","      except:\n","        return None\n","\n","    def get_user_description(self, user):\n","        return user._json['description'] if len(user._json['description'])>0 else None\n","        \n","    def search_users_through_name(self, name):\n","        usernames = {}\n","        for user in self.api.search_users(name):\n","            usernames[user._json['screen_name']] = 1\n","        return usernames if len(usernames.keys())>0 else None\n","    \n","    def rank_users_through_info(self, usernames, name, email, org, homepage_url):\n","        ranked_users = {}\n","        for username in usernames:\n","            try:\n","                user = self.api.get_user(username)\n","            except:\n","                print(f'[DEBUG] user {username} not found on Twitter')\n","            ranked_users[user._json['screen_name']] = 1\n","            user_desc = self.get_user_description(user)\n","            homepage_match = False\n","            email_match = False\n","            org_match = False\n","            if org is not None and user_desc is not None and len(fuzzysearch.find_near_matches(org, user_desc, max_l_dist=1))>0:\n","                ranked_users[user._json['screen_name']] += 1\n","            if 'entities' in user._json and 'url' in user._json['entities'] and 'urls' in user._json['entities']['url'] and len(user._json['entities']['url']['urls'])>0:\n","                if 'display_url' in user._json['entities']['url']['urls'][0]:\n","                    if homepage_url is not None and\\\n","                    user._json['entities']['url']['urls'][0]['display_url'] is not None and\\\n","                     len(fuzzysearch.find_near_matches(user._json['entities']['url']['urls'][0]['display_url'], homepage_url, max_l_dist=1))>0:\n","                        homepage_match = True\n","                        ranked_users[user._json['screen_name']] += 2\n","                    if email is not None and\\\n","                    user._json['entities']['url']['urls'][0]['display_url'] is not None and\\\n","                     len(fuzzysearch.find_near_matches(user._json['entities']['url']['urls'][0]['display_url'], email, max_l_dist=1))>0:\n","                        email_match = True\n","                        ranked_users[user._json['screen_name']] += 1\n","                if 'expanded_url' in user._json['entities']['url']['urls'][0]:\n","                    if homepage_url is not None and\\\n","                    user._json['entities']['url']['urls'][0]['expanded_url'] is not None and\\\n","                     len(fuzzysearch.find_near_matches(homepage_url, user._json['entities']['url']['urls'][0]['expanded_url'], max_l_dist=1))>0:\n","                        ranked_users[user._json['screen_name']] += 2\n","                    if email is not None and\\\n","                    user._json['entities']['url']['urls'][0]['expanded_url'] is not None and\\\n","                     len(fuzzysearch.find_near_matches(user._json['entities']['url']['urls'][0]['expanded_url'], email, max_l_dist=1))>0:\n","                        ranked_users[user._json['screen_name']] += 1\n","        # TODO - match twitter bio with GS research interest fields\n","        ranked_users = dict(sorted(ranked_users.items(), key=lambda item: item[1], reverse=True))\n","        return ranked_users \n","    \n","    def rank_users_through_tweets(self, usernames):\n","        ranked_users = {}\n","        for user in usernames:\n","            ranked_users[user] = 1\n","            tweets = self.get_tweets(user, 10)\n","            if tweets is None:\n","                continue\n","            has_domain_keywords = False\n","            for tweet in tweets:\n","                if has_domain_keywords: \n","                    break\n","                for keyword in self.domain_keywords:\n","                    if keyword.lower() in tweet.lower():\n","                        has_domain_keywords = True\n","                        ranked_users[user] += 1\n","                        break\n","        ranked_users = dict(sorted(ranked_users.items(), key=lambda item: item[1], reverse=True))      \n","        return ranked_users"]},{"cell_type":"markdown","metadata":{"id":"fTs792JvydZW"},"source":["###TwitterSearch - edited - v2 (with improvements)"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":632,"status":"ok","timestamp":1678875525079,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"AaDSu28Vyao0"},"outputs":[],"source":["import time\n","from typing import Union\n","from collections import defaultdict\n","from selenium import webdriver\n","from selenium.webdriver.chromium.webdriver import ChromiumDriver\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.remote.errorhandler import WebDriverException\n","from bs4 import BeautifulSoup\n","\n","url_search_dict = {\n","    'google': 'https://www.google.com/search?q={0}'\n","}\n","\n","\n","def get_search_result(**kwargs):\n","    return kwargs\n","\n","\n","class TwitterSearch(GoogleSearch):\n","    \"\"\"Class that handling searching on Google search bar using REST API.\"\"\"\n","\n","    def __init__(self, driver_path, api_keys, domain_keywords, print_true):\n","        super().__init__(driver_path)\n","        self._urlsearch = url_search_dict['google']\n","        self.print_true = print_true \n","        self.scholar_search = ScholarSearch()\n","        self.scholar_search.setup()\n","        self.twitter_user_search = TwitterUserSearch(api_keys, domain_keywords)\n","\n","    def search_scholar_twitter(self, term: str):\n","        scholar_search_result = self.scholar_search.get_scholar(query=term, simple=True, top_n=1, print_true=False)\n","        branch_type = \"\"\n","\n","        if len(scholar_search_result) == 0:\n","            twitter_ids = {}\n","            # directly google search (name \"twitter\") and ranked users acc to frequency in google search\n","            url_fragment = self._urlsearch.format(f'{term} \"twitter\"')\n","            result = self._search_google_helper(url_fragment)\n","            new_ids = self.filter_result(result, term, web_source='google')\n","            if new_ids is not None:\n","                twitter_ids = {**twitter_ids, **new_ids}\n","                branch_type += 'directly google search (name \"twitter\"), '\n","\n","            # directly twitter search with name and combine both results by taking max rank for each key\n","            new_ids = self.twitter_user_search.search_users_through_name(term)\n","            if new_ids is not None:\n","                twitter_ids = combine_dicts_for_max_value(twitter_ids, new_ids)\n","                branch_type += 'twitter search (name \"twitter\")'\n","\n","            # rank all twitter ids according to domain keywords in latest tweets\n","            twitter_ids = self.twitter_user_search.rank_users_through_tweets(twitter_ids)\n","            if twitter_ids is None or len(twitter_ids) == 0:\n","                print(f'[INFO] branch_type: {branch_type}')\n","                print(f'[INFO] twitter_ids: {twitter_ids}')\n","                return None, twitter_ids\n","            # if some IDs have same ranks, rank ids based on similarity with firstnamelastname \n","            twitter_ids_list = list(twitter_ids.keys())\n","            highest_rank = twitter_ids[twitter_ids_list[0]]\n","            candidate_list = {}\n","            for item in twitter_ids:\n","                if item[1] == highest_rank:\n","                    rank = self._rank_by_similarity(item[0], name=term)\n","                    if rank >= 0.2:\n","                        candidate_list[item[0]] = rank\n","            if len(candidate_list.keys()) > 0:\n","                branch_type += 'highest ranked based on string similarity'\n","                print(f'[INFO] branch_type: {branch_type}')\n","                print(f'[INFO] twitter_ids: {twitter_ids}')\n","                sorted_candidate_list = sorted(candidate_list.items(), key=lambda item: item[1], reverse=True)\n","                return dict(sorted_candidate_list)[0], list(sorted_candidate_list)\n","            print(f'[INFO] branch_type: {branch_type}')\n","            print(f'[INFO] twitter_ids: {twitter_ids}')\n","            return twitter_ids[0], twitter_ids[:10]\n","        \n","        else:\n","            # preprocess name to include only tokens which have length greater than one\n","            name = \" \".join([n for n in scholar_search_result[0][\"name\"].split(\" \") if len(n.replace(\".\",\"\"))>1])\n","            homepage_url = scholar_search_result[0]['homepage_url'] if 'homepage_url' in scholar_search_result[0] else None\n","            email = scholar_search_result[0][\"email_info\"] if \"email_info\" in scholar_search_result[0] else None\n","            if 'email' in scholar_search_result[0]:\n","                if homepage_url is None and re.search(\"href=\\\"([^\\s]*)\\\"\", scholar_search_result[0]['email']) is not None:\n","                    homepage_url = re.search(\"href=\\\"([^\\s]*)\\\"\", scholar_search_result[0]['email']).group(0).split(\"href=\")[-1].replace(\"\\\"\", \"\")\n","                if email is None and re.search(\"Verified email at ([^\\s]*)\", scholar_search_result[0]['email']) is not None:\n","                    email = re.search(\"Verified email at ([^\\s]*)\", scholar_search_result[0]['email']).group(0).split(\"Verified email at \")[-1]\n","            org = scholar_search_result[0][\"organization\"] if \"organization\" in scholar_search_result[0] else None\n","            \n","            # try to directly get twitter account through homepage (should give 100% correctness)\n","            if homepage_url is not None:\n","                twitter_ids = self._search_twitter_from_homepage(homepage_url, name=term, name_from_gs=name)\n","                if twitter_ids is not None and len(twitter_ids)>0:\n","                    branch_type += 'found from homepage'\n","                    print(f'[INFO] branch_type: {branch_type}')\n","                    print(f'[INFO] twitter_ids: {twitter_ids}')\n","                    return twitter_ids[0][0], list((dict(twitter_ids)).keys())[:10]\n","\n","            # Google Search and get IDs ranked according to frequency in search {username:rank}\n","            branch_type_gs_search, twitter_ids_gs_dict = self.google_search_twitter_id(name, email, org, homepage_url)\n","            branch_type += f\"GSearch: {branch_type_gs_search}\"\n","            # Twitter Search and get IDs each with rank 1 {username:rank}; combine both lists for google and twitter search\n","            twitter_ids_dict = combine_dicts_for_max_value(self.twitter_user_search.search_users_through_name(name), twitter_ids_gs_dict)\n","            branch_type += 'Twitter search, '\n","            if twitter_ids_dict is None:\n","              print(f'[INFO] branch_type: {branch_type}')\n","              print(f'[INFO] twitter_ids: {twitter_ids}')\n","              return None, list(twitter_ids_dict.keys())[:10]\n","            \n","            # rank all twitter IDs by matching twitter profile and google scholar profile\n","            twitter_ids_ranked = self.twitter_user_search.rank_users_through_info(twitter_ids_dict.keys(), name, email, org, homepage_url)\n","            # filter ranked IDs to get candidates with highest rank \n","            if len(twitter_ids_ranked) == 0:\n","              return None, None\n","            highest_rank = twitter_ids_ranked[list(twitter_ids_ranked.keys())[0]]\n","            highest_rank_candidates = [t_id for t_id in twitter_ids_ranked.keys() if twitter_ids_ranked[t_id]>=highest_rank]\n","            if len(highest_rank_candidates) == 1:# and\\\n","            #  ts._rank_by_similarity(highest_rank_candidates[0], name=term, name_from_gs=name) >= 0.35:\n","              branch_type += 'highest ranked based on info match, '\n","              print(f'[INFO] branch_type: {branch_type}')\n","              print(f'[INFO] twitter_ids: {twitter_ids_ranked}')\n","              return highest_rank_candidates[0], list(twitter_ids_ranked.keys())[:10]\n","\n","            # if there are more candidates with highest rank, use frequency to filter candidates and string matching for ranking\n","            if len(twitter_ids_gs_dict) > 0:\n","                highest_rank = twitter_ids_gs_dict[list(twitter_ids_gs_dict.keys())[0]]\n","                highest_rank_candidates = [t_id for t_id in twitter_ids_gs_dict.keys() if twitter_ids_gs_dict[t_id]>=highest_rank]\n","                candidate_list = {}\n","                # if len(twitter_ids_gs_dict) == len(highest_rank_candidates):\n","                #   continue\n","                # rank through name matching needed\n","                for item in highest_rank_candidates:\n","                    rank = self._rank_by_similarity(item, name=term, name_from_gs=name)\n","                    if rank >= 0.2:\n","                        candidate_list[item] = rank\n","                if len(candidate_list.keys()) > 0:\n","                    sorted_candidate_list = sorted(candidate_list.items(), key=lambda item: item[1], reverse=True)\n","                    branch_type += 'highest ranked based on gsearch freq & string similarity, '\n","                    print(f'[INFO] branch_type: {branch_type}')\n","                    print(f'[INFO] twitter_ids: {twitter_ids_gs_dict}')\n","                    return sorted_candidate_list[0][0], list(twitter_ids_gs_dict.keys())[:10]\n","            # rank using latest tweets \n","            twitter_ids_ranked_by_tweets = self.twitter_user_search.rank_users_through_tweets(twitter_ids_ranked)\n","            # filter ranked IDs to get candidates with highest rank \n","            highest_rank = twitter_ids_ranked_by_tweets[list(twitter_ids_ranked_by_tweets.keys())[0]]\n","            highest_rank_candidates = [t_id for t_id in twitter_ids_ranked_by_tweets.keys() if twitter_ids_ranked_by_tweets[t_id]>=highest_rank]\n","            if len(highest_rank_candidates) == 1 and\\\n","            ts._rank_by_similarity(highest_rank_candidates[0], name=term, name_from_gs=name) >= 0.20:\n","              branch_type += 'rank by tweets'\n","              print(f'[INFO] branch_type: {branch_type}')\n","              print(f'[INFO] twitter_ids: {twitter_ids_ranked_by_tweets}')\n","              return highest_rank_candidates[0], list(twitter_ids_ranked_by_tweets.keys())[:10]\n","            return None, None\n","              \n","\n","    def google_search_twitter_id(self, name, email, org, homepage_url):\n","        twitter_ids = {}\n","        len_twitter_ids = 0\n","        branch_type = \"\"\n","        # try (google_name email_suffix \"twitter\")\n","        if email is not None and len(email) > 0:\n","            url_fragment = self._urlsearch.format(f'{name} {email} \"twitter\"')\n","            result = self._search_google_helper(url_fragment)\n","            new_ids = self.filter_result(result, name, web_source='google')\n","            if new_ids is not None:\n","                twitter_ids = combine_dicts_for_max_value(twitter_ids, new_ids)\n","        if len(twitter_ids.keys()) > len_twitter_ids:\n","            branch_type += 'name + email, '\n","            len_twitter_ids = len(twitter_ids.keys())\n","        # then try (google_name organization \"twitter\")\n","        if org is not None and len(org) > 0:\n","            url_fragment = self._urlsearch.format(f'{name} {org} \"twitter\"')\n","            result = self._search_google_helper(url_fragment)\n","            new_ids = self.filter_result(result, name, web_source='google')\n","            if new_ids is not None:\n","                twitter_ids = combine_dicts_for_max_value(twitter_ids, new_ids)\n","        if len(twitter_ids.keys()) > len_twitter_ids:\n","            branch_type += 'name + organization,'\n","            len_twitter_ids = len(twitter_ids.keys())\n","        # then try (google_name \"twitter\")\n","        url_fragment = self._urlsearch.format(f'{name} \"twitter\"')\n","        result = self._search_google_helper(url_fragment)\n","        new_ids = self.filter_result(result, name, web_source='google')\n","        if new_ids is not None:\n","            twitter_ids = combine_dicts_for_max_value(twitter_ids, new_ids)\n","        if len(twitter_ids.keys()) > len_twitter_ids:\n","            branch_type += 'name, '\n","            len_twitter_ids = len(twitter_ids.keys())\n","        # then try (google_first_name website(after http://) \"twitter\")\n","        if homepage_url is not None:\n","            first_name = name.split(\" \")[0]\n","            website_name = homepage_url.split(\"://\")[-1].strip(\"/\")\n","            url_fragment = self._urlsearch.format(f'{first_name} {website_name} \"twitter\"')\n","            result = self._search_google_helper(url_fragment)\n","            new_ids = self.filter_result(result, name, web_source='google')\n","            if new_ids is not None:\n","                twitter_ids = combine_dicts_for_max_value(twitter_ids, new_ids)\n","        if len(twitter_ids.keys()) > len_twitter_ids:\n","            branch_type += 'first_name + trimmed_homepage, '\n","            len_twitter_ids = len(twitter_ids.keys())\n","        return branch_type, dict(sorted(twitter_ids.items(), key=lambda item: item[1], reverse=True))\n","\n","\n","    def _rank_by_similarity(self, twitter_url_origin_list: Union[list, str], name: str=None, name_from_gs: str=None):\n","        # process name and name_from_gs\n","        if name is not None:\n","            name = re.sub('[0-9_\\., ]', '', name.lower())\n","        if name_from_gs is not None:\n","            name_from_gs = re.sub('[0-9_\\., ]', '', name_from_gs.lower())\n","        if type(twitter_url_origin_list) == list:\n","            twitter_url_map_dict = {t_id:-1 for t_id in twitter_url_origin_list}\n","            if name is not None:\n","              for t_id in twitter_url_origin_list:\n","                clean_t_id = re.sub('[0-9_\\., ]', '', t_id)\n","                twitter_url_map_dict[t_id] = max(twitter_url_map_dict[t_id], get_str_similarity(clean_t_id, name))\n","            if name_from_gs is not None:\n","              for t_id in twitter_url_origin_list:\n","                clean_t_id = re.sub('[0-9_\\., ]', '', t_id)\n","                twitter_url_map_dict[t_id] = max(twitter_url_map_dict[t_id], get_str_similarity(clean_t_id, name_from_gs))\n","            return list(sorted(twitter_url_map_dict.items(), key=lambda item: item[1], reverse=True))\n","        else:\n","            twitter_url_origin_str = twitter_url_origin_list\n","            twitter_url_str = re.sub('[0-9_\\., ]', '', twitter_url_origin_str)\n","            rank = 0\n","            if name is not None and name_from_gs is not None:\n","                rank = max(get_str_similarity(twitter_url_str, name), get_str_similarity(twitter_url_origin_list, name_from_gs))\n","            elif name is not None:\n","                rank = get_str_similarity(twitter_url_str, name)\n","            elif name_from_gs is not None:\n","                rank = get_str_similarity(twitter_url_str, name_from_gs)\n","            return rank\n","\n","\n","    def _search_twitter_from_homepage(self, homepage_url: str, name: str=None, name_from_gs: str=None):\n","        # get content of scholar homepage using chromedriver\n","        try:\n","            self.driver.implicitly_wait(5)\n","            self.driver.get(homepage_url)\n","        except WebDriverException as e:\n","            if self.print_true:\n","                print('[DEBUG] WebDriverException while getting homepage: %s' % homepage_url)\n","                print(e)\n","        time.sleep(8)\n","        page = self.driver.page_source\n","        soup = BeautifulSoup(page, \"html.parser\")\n","        twitter_url_regex = 'twitter.com/([^\\/?]+)'\n","\n","        twitter_url_origin_list = list(set([re.findall(twitter_url_regex, item['href'])[0]\n","            for item in soup.find_all(\n","                href=re.compile(twitter_url_regex))]))\n","        twitter_url_origin_list = [t_id.lower() for t_id in twitter_url_origin_list if t_id.lower()!='githubstatus' and len(t_id)>1]\n","        if self.print_true:\n","            print(soup.find_all(href=re.compile(twitter_url_regex)))\n","        \n","        # if there are no candidates for twitter account url, return None\n","        if len(twitter_url_origin_list) == 0:\n","            return None\n","        # rank twitter IDs acc to similarity with FirstnameLastname\n","        twitter_url_origin_list = self._rank_by_similarity(twitter_url_origin_list, name=name, name_from_gs=name_from_gs)\n","        if self.print_true:\n","            print(f'[DEBUG] Find a set of twitter ids on the provided homepage:\\n{twitter_url_origin_list}')\n","        # only return the highest rank twitter account id\n","        return twitter_url_origin_list\n","\n","    def _search_google_helper(self, google_url: str):\n","        self.driver.implicitly_wait(5)  \n","        self.driver.get(google_url)\n","        time.sleep(8)\n","\n","        page = self.driver.page_source\n","        soup = BeautifulSoup(page, \"html.parser\")\n","        result_list = []\n","        result_block = soup.find_all('div', attrs={'class': 'g'})\n","        for result in result_block:\n","            # Find link, title, description\n","            link = result.find('a', href=True)\n","            title = result.find('h3')\n","            description_box = result.find(\n","                'div', {'style': '-webkit-line-clamp:2'})\n","            if link and title and description_box:\n","                result_list.append(get_search_result(\n","                    href=link['href'], title=title.text, description=description_box.text))\n","        if self.print_true:\n","            print(result_list)\n","        time.sleep(8)\n","        return result_list\n","        \n","\n","    def filter_result(self, result_list, term, web_source):\n","        \"\"\"\n","        web_source: google, twitter\n","        \"\"\"\n","        # sort twitter ids by occurrence frequency\n","        if web_source == 'google':\n","            twitter_id_dict = defaultdict(int)\n","            for result in result_list:\n","                if 'twitter.com/' in result['href']:\n","                    twitter_id_dict[re.findall('twitter.com/([^\\/?]+)', result['href'])[0]] += 1\n","        twitter_id_dict = dict(sorted(twitter_id_dict.items(), key=lambda item: item[1], reverse=True))\n","        # then, sort twitter ids by str similarity?\n","        # TODO: enter into twitter page to check profile information\n","        # Step 1: twitter profile vs google scholar profile\n","        # Step 2: twitter tweets: check whether google scholar domains are in twitter tweets\n","        # Step 3: twitter profile image (ask Yvonne about the performance)\n","        if len(twitter_id_dict) == 0:\n","            return None\n","        else:\n","            return twitter_id_dict"]},{"cell_type":"markdown","metadata":{"id":"YzGtaaqfJ22P"},"source":["# Test"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":76767,"status":"ok","timestamp":1678876319327,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"ZneOBag1VtSX","outputId":"625cdb57-c092-4dd3-98cc-7e05f674e827"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-7-8b5ec295fe04>:27: DeprecationWarning: headless property is deprecated, instead use add_argument('--headless') or add_argument('--headless=new')\n","  options.headless = True\n","<ipython-input-7-8b5ec295fe04>:35: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n","  self.driver = webdriver.Chrome(driver_path, options=options)\n","<ipython-input-7-8b5ec295fe04>:27: DeprecationWarning: headless property is deprecated, instead use add_argument('--headless') or add_argument('--headless=new')\n","  options.headless = True\n","<ipython-input-7-8b5ec295fe04>:35: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n","  self.driver = webdriver.Chrome(driver_path, options=options)\n"]},{"output_type":"stream","name":"stdout","text":["Successful Authentication\n"]}],"source":["ts = TwitterSearch(driver_path, api_keys, domain_keywords, print_true=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ftjMVDE2_bw"},"outputs":[],"source":["with open(\"/content/drive/My Drive/tweets-dataset/gs_scholars_matched_with_twitter_accounts_500.tsv\", \"r\") as fr, open('/content/drive/My Drive/tweets-dataset/gs_scholars_candidate_twitter_accounts_v2_all_changes.tsv','a') as fw:\n","  reader = csv.reader(fr, delimiter=\"\\t\")\n","  fw.write('\\n')\n","  num_times = 0\n","  for i, line in tqdm(enumerate(reader)):\n","    index = line[0]\n","    if i==0 or int(index)<=491:\n","      # line.extend(['predicted twitter ID','candidates'])\n","      # fw.write(\"\\t\".join(line))\n","      # fw.write('\\n')\n","      continue\n","    name = line[1]\n","    org = line[2]\n","    gs_url = line[3]\n","    twitter_url = line[4]\n","    if i>0:\n","      print(f\"Searching for {name}\")\n","      # get twitter IDs\n","      twitter_id, candidate_ids = ts.search_scholar_twitter(name)\n","      num_times += 1\n","      if twitter_id is not None:\n","        # save list of twitter IDs in the CSV\n","        line.append(twitter_id)\n","      else:\n","        line.append(\"N/A\")\n","        time.sleep(random.randint(2, 4))\n","        print(i)\n","      if candidate_ids is not None:\n","        if type(candidate_ids) == dict:\n","          line.extend(list(candidate_ids.keys()))\n","        else:\n","          line.extend(candidate_ids)\n","    fw.write(\"\\t\".join(line))\n","    fw.write('\\n')\n","    if num_times >= 12:\n","      num_times = 0\n","      del ts\n","      ts = TwitterSearch(driver_path, api_keys, domain_keywords, print_true=True)\n","      time.sleep(random.randint(4, 8))"]},{"cell_type":"markdown","metadata":{"id":"btLvFvxaWgm_"},"source":["## TODO\n"]},{"cell_type":"markdown","metadata":{"id":"fEARD8CoYMXq"},"source":["## Debugging"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xa2HRfwWKwLk"},"outputs":[],"source":["name = term = \"Inam Ullah Khan\"\n","# William R. Gray-Roncal, Ph.D.\n","# Vivek Joshi\n","# Jun Cheng, PhD"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-jpo_TtuAnVO"},"outputs":[],"source":["ids = ts.search_scholar_twitter(name)\n","ids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rysy-TABNESO"},"outputs":[],"source":["gs_scholar = ts.scholar_search.get_scholar(query=term, simple=True, top_n=1, print_true=False)[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1678733725632,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"Wj5xcDGqOhJM","outputId":"03e1050d-343e-452c-c003-3278806478e4"},"outputs":[{"data":{"text/plain":["{'academic': True,\n"," 'academic_age': 7.0,\n"," 'academic_lifespan': 5.0,\n"," 'cit_sum_before_year': {'2017': 4,\n","  '2018': 31,\n","  '2019': 93,\n","  '2020': 185,\n","  '2021': 337},\n"," 'citation_table': ['395', '6'],\n"," 'domain_labels': ['network system security',\n","  'optimization techniques',\n","  'ant colony optimization',\n","  'underwater networks',\n","  'artificial intelligence'],\n"," 'gender': '-',\n"," 'gs_sid': 'NN-PHcIAAAAJ',\n"," 'organization': 'School of Engineering and Applied Sciences (SEAS), Isra University, Islamabad Campus',\n"," 'organization_code': None,\n"," 'paper_num': 33,\n"," 'cites': {'years': ['2017', '2018', '2019', '2020', '2021', '2022'],\n","  'cites': ['4', '27', '62', '92', '152', '58']},\n"," 'coauthors': [['https://scholar.google.com/citations?user=0xDhDEMAAAAJ&hl=en',\n","   'Muhammad Asghar Khan',\n","   'Assistant Professor, Electrical Engineering Department , Hamdard University Islamabad',\n","   'Verified email at hamdard.edu.pk'],\n","  ['https://scholar.google.com/citations?user=83HXv3QAAAAJ&hl=en',\n","   'Muhammad Abul Hassan',\n","   'Abasyn University Pakistan',\n","   '']],\n"," 'name': 'Inam Ullah Khan',\n"," 'url': 'https://scholar.google.com/citations?hl=en&user=NN-PHcIAAAAJ',\n"," 'top_num_coauthors': 2.0,\n"," 'num_domain': 5.0,\n"," 'top_topic_diversity': 4.0,\n"," 'num_coauthors_all_wwo_ai': 56,\n"," 'organization_top': 0.0,\n"," 'email': 'No verified email - <a href=\"http://www.iuk.pk/index.html\" rel=\"nofollow\" class=\"gsc_prf_ila\">Homepage</a>'}"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["gs_scholar"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1678733725632,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"lOr0jTr00tVR","outputId":"ffd767ae-838e-46a4-d206-e2ff8b4e3b3c"},"outputs":[{"data":{"text/plain":["('Inam Ullah Khan',\n"," 'http://www.iuk.pk/index.html',\n"," None,\n"," 'School of Engineering and Applied Sciences (SEAS), Isra University, Islamabad Campus')"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["scholar_search_result = [gs_scholar]\n","name = \" \".join([n for n in scholar_search_result[0][\"name\"].split(\" \") if len(n.replace(\".\",\"\"))>1])\n","homepage_url = scholar_search_result[0]['homepage_url'] if 'homepage_url' in scholar_search_result[0] else None\n","email = scholar_search_result[0][\"email_info\"] if \"email_info\" in scholar_search_result[0] else None\n","if 'email' in scholar_search_result[0]:\n","    if homepage_url is None and re.search(\"href=\\\"([^\\s]*)\\\"\", scholar_search_result[0]['email']) is not None:\n","        homepage_url = re.search(\"href=\\\"([^\\s]*)\\\"\", scholar_search_result[0]['email']).group(0).split(\"href=\")[-1].replace(\"\\\"\", \"\")\n","    if email is None and re.search(\"Verified email at ([^\\s]*)\", scholar_search_result[0]['email']) is not None:\n","        email = re.search(\"Verified email at ([^\\s]*)\", scholar_search_result[0]['email']).group(0).split(\"Verified email at \")[-1]\n","org = scholar_search_result[0][\"organization\"] if \"organization\" in scholar_search_result[0] else None\n","name, homepage_url, email, org"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8320,"status":"ok","timestamp":1678733733940,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"osb6QiqFli9Y","outputId":"85873c5e-1b31-4f97-9e02-abcac676847e"},"outputs":[{"name":"stdout","output_type":"stream","text":["[DEBUG] WebDriverException while getting homepage: http://www.iuk.pk/index.html\n","Message: unknown error: net::ERR_NAME_NOT_RESOLVED\n","  (Session info: headless chrome=90.0.4430.212)\n","Stacktrace:\n","#0 0x55bf2fab07f9 <unknown>\n","#1 0x55bf2fa503b3 <unknown>\n","#2 0x55bf2f798016 <unknown>\n","#3 0x55bf2f795d4f <unknown>\n","#4 0x55bf2f7831f9 <unknown>\n","#5 0x55bf2f784045 <unknown>\n","#6 0x55bf2f783541 <unknown>\n","#7 0x55bf2f782aba <unknown>\n","#8 0x55bf2f781ae7 <unknown>\n","#9 0x55bf2f781f51 <unknown>\n","#10 0x55bf2f79a0d7 <unknown>\n","#11 0x55bf2f800d27 <unknown>\n","#12 0x55bf2f7efdc2 <unknown>\n","#13 0x55bf2f8009e1 <unknown>\n","#14 0x55bf2f7efc93 <unknown>\n","#15 0x55bf2f7c1ce4 <unknown>\n","#16 0x55bf2f7c34d2 <unknown>\n","#17 0x55bf2fa7c542 <unknown>\n","#18 0x55bf2fa8bce7 <unknown>\n","#19 0x55bf2fa8b9e4 <unknown>\n","#20 0x55bf2fa9013a <unknown>\n","#21 0x55bf2fa8c5b9 <unknown>\n","#22 0x55bf2fa71e00 <unknown>\n","#23 0x55bf2faa35d2 <unknown>\n","#24 0x55bf2faa3778 <unknown>\n","#25 0x55bf2fabba1f <unknown>\n","#26 0x7f0eda152609 start_thread\n","#27 0x7f0ed9366133 clone\n","\n","[]\n","None\n"]}],"source":["if homepage_url is not None:\n","  twitter_ids = ts._search_twitter_from_homepage(homepage_url, name=term, name_from_gs=name)\n","  print(twitter_ids)\n","  if twitter_ids is not None and len(twitter_ids)>0:\n","      print(twitter_ids[0][0], list((dict(twitter_ids)).keys())[:10])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49586,"status":"ok","timestamp":1678733783509,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"nLlHZ3l6n08X","outputId":"3e2477f9-c7ae-48b6-fc84-3eab79b76678"},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'href': 'https://www.sandmanbooks.com/book/9783031057519', 'title': 'Big Data Analytics and Computational Intelligence for ...', 'description': 'Sep 2, 2022 — Dr. Inam Ullah Khan was a Lecturer at different universities in Pakistan ... Sandman Books Twitter Account Sandman Books Neil Gaiman Tik Tok.'}, {'href': 'https://et.linkedin.com/posts/jalilpiran_dear-network-we-are-accepting-applications-activity-6840569561448775680-l5ce', 'title': 'Md. Jalil Piran, PhD, SMIEEE on LinkedIn: Dear Network', 'description': 'Twitter. Mahmoud Abbasi. Marie Skłodowska-Curie Researcher at Universidad de ... of Engineering and Applied Sciences, ISRA University, Islamabad Campus.'}, {'href': 'https://www.researchgate.net/profile/Kiran-Perveen-2/publication/363175726_A_Survey_of_Deep_Learning_Methods_for_Fruit_and_Vegetable_Detection_and_Yield_Estimation/links/631761d01ddd4470213e44c2/A-Survey-of-Deep-Learning-Methods-for-Fruit-and-Vegetable-Detection-and-Yield-Estimation.pdf', 'title': 'Big Data Analytics and Computational ... - ResearchGate', 'description': \"Applications, Intrusion Detection, Social media such as twitter and facebook [4]. In Medicine it's used to discover effective medical therapies for diverse\\xa0...\"}, {'href': \"https://www.nediconics.com/documents/Final-ICONICS'16-Proceedings-Website.pdf\", 'title': 'International Conference on Innovations in Computer Science ...', 'description': 'Dr. Safeeullah Soomro ... Karachi Campus of National University of Computer & Emerging Sciences ... \"Twitter for teaching: Can social media be used to.'}, {'href': 'http://tomwsanchez.com/page/2/', 'title': 'Professor, Virginia Tech Urban Affairs and Planning | Page 2', 'description': 'May 9, 2021 — This paper presents a descriptive analysis of Twitter use by urban planning faculty, reporting characteristics of users, the topics posted,\\xa0...'}, {'href': 'https://www.science.gov/topicpages/t/tanzanian+medical+students', 'title': 'tanzanian medical students: Topics by Science.gov', 'description': 'The Kilimanjaro Christian Medical University (KCMU) College and the Medical ... Facebook (100%, 63/63), YouTube (43%, 27/63), Twitter (31%, 20/63),\\xa0...'}, {'href': 'https://faculty.uol.edu.pk/publications?profileid=11809&profilename=Isra%20Sarwar', 'title': 'View Publication - Faculty / Staff - The University of Lahore', 'description': '148, Fasih Ullah Khan Durrani, Experimental testbed evaluation of cell level ... using Twitter Profile Pictures, Faculty, Department of Computer Sciences\\xa0...'}, {'href': 'https://www.scribd.com/document/398865482/Agenda-of-23rd-Meeting-of-Academic-Council-pdf', 'title': 'Agenda of 23rd Meeting of Academic Council PDF - Scribd', 'description': 'by Faculty of Allied Health Sciences, Islamabad Campus ... Dr. Nazar Ullah Raja Member ... Twitter have become forums used by international leaders to'}, {'href': 'https://islamabadpost.com.pk/wp-content/uploads/2023/01/PDF-03-01-2022.pdf', 'title': 'No mercy for outlaws, resolves NSC - Islamabad Post', 'description': 'Jan 3, 2023 — ISLAMABAD: Prime Minister Shehbaz Sharif chairs a meeting of National Security ... cial media platforms i.e., Meta (Facebook), twitter,.'}, {'href': 'https://numl.edu.pk/offices/publications/1568719337Faculty%20CVs-24-4-2019-Final.pdf', 'title': 'faculty cvs - NUML', 'description': 'Feb 4, 2018 — Department of Computer Sciences (Peshawar Campus) . ... \"Saving lives using social media: Analysis of the role of twitter for personal blood.'}]\n","[{'href': 'https://twitter.com/inamukniazi?lang=en', 'title': 'Inam Ullah Khan Niazi (@inamukniazi) / Twitter', 'description': 'Official Twitter of Inamullah Khan Niazi, Former MNA & MPA Mianwali. Ex MPA Dariya Khan Distt Bhakhar #VoteKoIzzatDo. Pakistan pmln.org Joined July 2013.'}, {'href': 'https://twitter.com/inammsm', 'title': 'InAm uLLaH kHaN (@inammsm) / Twitter', 'description': \"#BitGold is a cash flow creating asset, available at a fantastic entry opportunity and it's highly memeable. twitter.com/gokhshteinmedi…\"}, {'href': 'https://twitter.com/hiktweets', 'title': 'Dr. Hisham Inamullah Khan (@hiktweets) / Twitter', 'description': 'Dr. Hisham Inamullah Khan. @hiktweets. MPA PK-92 LM/PTI, ExHealth Minister, ExSocial Welfare Minister KP, Mem.Marwat Bhettani Qoumi Itehad, Chair.'}, {'href': 'https://twitter.com/Inamullah27K', 'title': 'inamullah khan (@Inamullah27K) / Twitter', 'description': \"Evry ordinary Muslim is with Palestine cause and will remain so inshallah this is THE news for Israel and it's all supports. 2. inamullah khan.\"}, {'href': 'https://twitter.com/Inamullahkhank7', 'title': 'Inam ullah khan king (@Inamullahkhank7) / Twitter', 'description': 'Inam ullah khan king. @Inamullahkhank7. as a doctor student. Joined August 2019. 82 Following · 0 Followers · Tweets · Tweets & replies.'}, {'href': 'https://twitter.com/iukhan976?lang=en', 'title': 'Inam Ullah Khan (@iukhan976) / Twitter', 'description': 'A SafeMars Tokens Giveaway 20M   Life Changing Opportunity Here Must Follow & Tweet   No Exceptions! @\\xa0...'}, {'href': 'https://twitter.com/Inamkhattakpak', 'title': 'Inamullah Khattak (@Inamkhattakpak) / Twitter', 'description': \"Inamullah Khattak Retweeted ... to our agriculture sector and become a part of Pakistan's development twitter.com/inamkhattakpak… ... Aftab Khan Sherpao.\"}, {'href': 'https://twitter.com/hiktweets/status/1350331935189585920?lang=en', 'title': 'Dr. Hisham Inamullah Khan on Twitter: \"In a historic move the ...', 'description': 'Dr. Hisham Inamullah Khan · @hiktweets. In a historic move the domestic violence against women bill was passed by Provincial Assembly of KP.'}, {'href': 'https://twitter.com/inamullah64?lang=en', 'title': 'Inamullah Khan (@inamullah64) / Twitter', 'description': \"Inamullah Khan's Tweets. Inamullah Khan Retweeted · Harsh Goenka · @hvgoenka. ·. Jan 30. A laid off techie….this is so funny.\"}, {'href': 'https://mobile.twitter.com/voiceof_swat', 'title': 'INAM UlLAH KHAN (@VoicEof_SWAT) / Twitter', 'description': 'Follow. Click to Follow VoicEof_SWAT. INAM UlLAH KHAN. @VoicEof_SWAT. Edu: B.A Age: 20 years. SWAT KPK twitter.com/KPK_SWAT Joined December 2012.'}]\n","[{'href': 'https://twitter.com/inamartist?lang=en', 'title': 'Inam Elahi (@inamartist) / Twitter', 'description': 'Please enable JavaScript or switch to a supported browser to continue using twitter.com. You can see a list of supported browsers in our Help Center.'}, {'href': 'https://twitter.com/inam_ashraf75?lang=en', 'title': 'Inam Ashraf (@inam_ashraf75) / Twitter', 'description': 'Oil Price Drops on Oversupply http://oil-price.net/en/articles/oil-price-drops-on-oversupply.php#.VDpt0zhaE9w.twitter… 1. Inam Ashraf · @inam_ashraf75.'}, {'href': 'https://twitter.com/inam1400', 'title': 'Inam Bari (@inam1400) / Twitter', 'description': 'Inam Bari Retweeted ... Pakistan CAA has outsourced the pilot licencing to UK CAA. ... https://twitter.com/cnnbrk/status/1099320737721184256?s=19…'}, {'href': 'https://mobile.twitter.com/inam19531152', 'title': 'Inam khan (@Inam19531152) / Twitter', 'description': 'Please enable JavaScript or switch to a supported browser to continue using twitter.com. You can see a list of supported browsers in our Help Center.'}, {'href': 'https://twitter.com/frazinam', 'title': 'Fraz Inam (@frazinam) / Twitter', 'description': 'Please enable JavaScript or switch to a supported browser to continue using twitter.com. You can see a list of supported browsers in our Help Center.'}, {'href': 'https://twitter.com/tamreezinam?lang=en', 'title': 'Tamreez Inam (@tamreezinam) / Twitter', 'description': \"Tomorrow I will be speaking in my first ever Twitter Space. For our festival theme 'Around the World', I have chosen to talk about South Asian fiction.\"}, {'href': 'https://twitter.com/TRU_Lab', 'title': 'Aseem Inam (@TRU_Lab) / Twitter', 'description': 'Please enable JavaScript or switch to a supported browser to continue using twitter.com. You can see a list of supported browsers in our Help Center.'}, {'href': 'https://suffolk.edu/redirect.aspx?url=//pergsperirdoco.tk/113suffolkeduJrx651', 'title': 'Untitled', 'description': 'Katy high school football twitter, Batute din piept de pui, ... Kadek suarsana, Thati bione 2013, Voodoo chronpro, Html reduce line spacing,\\xa0...'}, {'href': 'https://cheerunion.org/tracker/index.html?t=ad&pool_id=2&ad_id=5&url=//clartypralongduce.ml/113cheerunionorgAiY651', 'title': 'Untitled', 'description': 'Proposal network management, Khaitan fans images, #Kinder schoko bons uk? ... Badham farm twitter, Gracenica pljusak, Cometa ison chile, Torrens proceeding\\xa0...'}]\n"]}],"source":["# Google Search and get IDs ranked according to frequency in search {username:rank}\n","branch_type_gs_search, twitter_ids_gs_dict = ts.google_search_twitter_id(name, email, org, homepage_url)\n","# twitter_ids_gs_dict = dict(sorted(twitter_ids_gs_dict.items(), key=lambda item: item[1], reverse=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1678733783509,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"lw2epxET7Z4b","outputId":"c27666e1-7a65-43fb-a036-3f811f43a3bd"},"outputs":[{"data":{"text/plain":["('name, first_name + trimmed_homepage, ',\n"," {'hiktweets': 2,\n","  'inamukniazi': 1,\n","  'inammsm': 1,\n","  'Inamullah27K': 1,\n","  'Inamullahkhank7': 1,\n","  'iukhan976': 1,\n","  'Inamkhattakpak': 1,\n","  'inamullah64': 1,\n","  'voiceof_swat': 1,\n","  'inamartist': 1,\n","  'inam_ashraf75': 1,\n","  'inam1400': 1,\n","  'inam19531152': 1,\n","  'frazinam': 1,\n","  'tamreezinam': 1,\n","  'TRU_Lab': 1})"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["branch_type_gs_search, twitter_ids_gs_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1678733783510,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"0wiWhCdZxbpc","outputId":"371c8181-750e-482c-e4f9-0f4d060b8bc0"},"outputs":[{"data":{"text/plain":["{'iukniazii': 1,\n"," 'inampk0': 1,\n"," 'inammsm': 1,\n"," 'kroshai': 1,\n"," 'Iam_InamKhan': 1,\n"," 'inamulahkhan45': 1,\n"," 'Inam_Ul_Khan': 1,\n"," 'InamDaudzaii': 1,\n"," 'WCDOKP': 1,\n"," 'inamullahkhan78': 1,\n"," 'InamUll87721615': 1,\n"," 'inambinfaiz': 1,\n"," 'inamukniazi': 1,\n"," 'Inamull71870669': 1,\n"," 'HashamInam': 1,\n"," 'InamUll57629493': 1,\n"," 'l5Gifjg3MMLLq2Z': 1,\n"," 'InamUll95042275': 1,\n"," 'InamUll59164932': 1,\n"," 'InamUllah9763': 1}"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["ts.twitter_user_search.search_users_through_name(name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w-0GkzRuMSCm"},"outputs":[],"source":["twitter_ids_dict = combine_dicts_for_max_value(ts.twitter_user_search.search_users_through_name(name), twitter_ids_gs_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1678733784296,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"fjd6UzorMh_O","outputId":"c2460dfc-546c-4957-aa20-955bdd92b812"},"outputs":[{"data":{"text/plain":["{'iukniazii': 1,\n"," 'inampk0': 1,\n"," 'inammsm': 1,\n"," 'kroshai': 1,\n"," 'Iam_InamKhan': 1,\n"," 'inamulahkhan45': 1,\n"," 'Inam_Ul_Khan': 1,\n"," 'InamDaudzaii': 1,\n"," 'WCDOKP': 1,\n"," 'inamullahkhan78': 1,\n"," 'InamUll87721615': 1,\n"," 'inambinfaiz': 1,\n"," 'inamukniazi': 1,\n"," 'Inamull71870669': 1,\n"," 'HashamInam': 1,\n"," 'InamUll57629493': 1,\n"," 'l5Gifjg3MMLLq2Z': 1,\n"," 'InamUll95042275': 1,\n"," 'InamUll59164932': 1,\n"," 'InamUllah9763': 1,\n"," 'hiktweets': 2,\n"," 'Inamullah27K': 1,\n"," 'Inamullahkhank7': 1,\n"," 'iukhan976': 1,\n"," 'Inamkhattakpak': 1,\n"," 'inamullah64': 1,\n"," 'voiceof_swat': 1,\n"," 'inamartist': 1,\n"," 'inam_ashraf75': 1,\n"," 'inam1400': 1,\n"," 'inam19531152': 1,\n"," 'frazinam': 1,\n"," 'tamreezinam': 1,\n"," 'TRU_Lab': 1}"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["twitter_ids_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q-ySajbSVPC-"},"outputs":[],"source":["usernames = twitter_ids_dict.keys()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":582},"executionInfo":{"elapsed":2628,"status":"error","timestamp":1678734006176,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"MPyU3CREVNgD","outputId":"99c7e47a-0a7e-423a-d5a9-d38ff168bed2"},"outputs":[{"name":"stdout","output_type":"stream","text":["http://www.iuk.pk/index.html False\n","{'urls': [{'url': 'https://t.co/rH0SGG8HGg', 'expanded_url': 'http://www.inampakistan.com', 'display_url': 'inampakistan.com', 'indices': [0, 23]}]}\n","http://www.iuk.pk/index.html False\n","{'urls': [{'url': 'http://t.co/dMvKWbhA4L', 'expanded_url': 'http://about.me/kroshai', 'display_url': 'about.me/kroshai', 'indices': [0, 22]}]}\n","http://www.iuk.pk/index.html False\n","{'urls': [{'url': 'https://t.co/mRqBpNDp34', 'expanded_url': 'http://www.ppp.org.pk', 'display_url': 'ppp.org.pk', 'indices': [0, 23]}]}\n","http://www.iuk.pk/index.html True\n","{'urls': [{'url': 'https://t.co/PslZAfJR3e', 'expanded_url': None, 'indices': [0, 23]}]}\n"]},{"ename":"TypeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-40-89e532441f7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhomepage_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'urls'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'expanded_url'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mhomepage_url\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuzzysearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_near_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhomepage_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'urls'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'expanded_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                 \u001b[0mranked_users\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'screen_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0memail\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuzzysearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_near_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'urls'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'expanded_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memail\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/fuzzysearch/__init__.py\u001b[0m in \u001b[0;36mfind_near_matches\u001b[0;34m(subsequence, sequence, max_substitutions, max_insertions, max_deletions, max_l_dist)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0msearch_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoose_search_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubsequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msearch_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/fuzzysearch/levenshtein.py\u001b[0m in \u001b[0;36mconsolidate_matches\u001b[0;34m(cls, matches)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconsolidate_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconsolidate_overlapping_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/fuzzysearch/common.py\u001b[0m in \u001b[0;36mconsolidate_overlapping_matches\u001b[0;34m(matches)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconsolidate_overlapping_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;34m\"\"\"Replace overlapping matches with a single, \"best\" match.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m     \u001b[0mbest_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_best_match_in_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_matches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/fuzzysearch/common.py\u001b[0m in \u001b[0;36mgroup_matches\u001b[0;34m(matches)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgroup_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0moverlapping_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_match_in_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverlapping_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/fuzzysearch/levenshtein.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(cls, subsequence, sequence, search_params)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         for match in find_near_matches_levenshtein(subsequence, sequence,\n\u001b[0m\u001b[1;32m    156\u001b[0m                                                    search_params.max_l_dist):\n\u001b[1;32m    157\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/fuzzysearch/levenshtein_ngram.py\u001b[0m in \u001b[0;36mfind_near_matches_levenshtein_ngrams\u001b[0;34m(subsequence, sequence, max_l_dist)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_near_matches_levenshtein_ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubsequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0msubseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubsequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mngram_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubseq_len\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_l_dist\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"]}],"source":["ranked_users = {}\n","for username in usernames:\n","    try:\n","        user = ts.twitter_user_search.api.get_user(username)\n","    except:\n","        print(f'[DEBUG] user {username} not found on Twitter')\n","    ranked_users[user._json['screen_name']] = 1\n","    user_desc = ts.twitter_user_search.get_user_description(user)\n","    homepage_match = False\n","    email_match = False\n","    org_match = False\n","    if org is not None and user_desc is not None and len(fuzzysearch.find_near_matches(org, user_desc, max_l_dist=1))>0:\n","        ranked_users[user._json['screen_name']] += 1\n","    if 'entities' in user._json and 'url' in user._json['entities'] and 'urls' in user._json['entities']['url'] and len(user._json['entities']['url']['urls'])>0:\n","        if 'display_url' in user._json['entities']['url']['urls'][0]:\n","            if homepage_url is not None and len(fuzzysearch.find_near_matches(user._json['entities']['url']['urls'][0]['display_url'], homepage_url, max_l_dist=1))>0:\n","                homepage_match = True\n","                ranked_users[user._json['screen_name']] += 2\n","            if email is not None and len(fuzzysearch.find_near_matches(user._json['entities']['url']['urls'][0]['display_url'], email, max_l_dist=1))>0:\n","                email_match = True\n","                ranked_users[user._json['screen_name']] += 1\n","        if 'expanded_url' in user._json['entities']['url']['urls'][0]:\n","            print(homepage_url, user._json['entities']['url']['urls'][0]['expanded_url'] is None)\n","            print(user._json['entities']['url'])\n","            if homepage_url is not None and len(fuzzysearch.find_near_matches(homepage_url, user._json['entities']['url']['urls'][0]['expanded_url'], max_l_dist=1))>0:\n","                ranked_users[user._json['screen_name']] += 2\n","            if email is not None and len(fuzzysearch.find_near_matches(user._json['entities']['url']['urls'][0]['expanded_url'], email, max_l_dist=1))>0:\n","                ranked_users[user._json['screen_name']] += 1\n","# TODO - match twitter bio with GS research interest fields\n","ranked_users = dict(sorted(ranked_users.items(), key=lambda item: item[1], reverse=True))\n","print(ranked_users)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":324},"executionInfo":{"elapsed":1670,"status":"error","timestamp":1678733785956,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"Q68yfoB6sGSF","outputId":"dc45bb73-188f-46ad-98ea-9a2e599a295b"},"outputs":[{"ename":"TypeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-33-8a5f0e86cdaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtwitter_ids_ranked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtwitter_user_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank_users_through_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwitter_ids_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memail\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhomepage_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-10-add9268ff991>\u001b[0m in \u001b[0;36mrank_users_through_info\u001b[0;34m(self, usernames, name, email, org, homepage_url)\u001b[0m\n\u001b[1;32m     59\u001b[0m                         \u001b[0mranked_users\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'screen_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'expanded_url'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'urls'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mhomepage_url\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuzzysearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_near_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhomepage_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'urls'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'expanded_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                         \u001b[0mranked_users\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'screen_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0memail\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuzzysearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_near_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'urls'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'expanded_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memail\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l_dist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/fuzzysearch/__init__.py\u001b[0m in \u001b[0;36mfind_near_matches\u001b[0;34m(subsequence, sequence, max_substitutions, max_insertions, max_deletions, max_l_dist)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0msearch_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchoose_search_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearch_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearch_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubsequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msearch_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/fuzzysearch/levenshtein.py\u001b[0m in \u001b[0;36mconsolidate_matches\u001b[0;34m(cls, matches)\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconsolidate_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconsolidate_overlapping_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/fuzzysearch/common.py\u001b[0m in \u001b[0;36mconsolidate_overlapping_matches\u001b[0;34m(matches)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mconsolidate_overlapping_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;34m\"\"\"Replace overlapping matches with a single, \"best\" match.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m     \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m     \u001b[0mbest_matches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_best_match_in_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_matches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/fuzzysearch/common.py\u001b[0m in \u001b[0;36mgroup_matches\u001b[0;34m(matches)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgroup_matches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0moverlapping_groups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_match_in_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moverlapping_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/fuzzysearch/levenshtein.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(cls, subsequence, sequence, search_params)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         for match in find_near_matches_levenshtein(subsequence, sequence,\n\u001b[0m\u001b[1;32m    156\u001b[0m                                                    search_params.max_l_dist):\n\u001b[1;32m    157\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mmatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/fuzzysearch/levenshtein_ngram.py\u001b[0m in \u001b[0;36mfind_near_matches_levenshtein_ngrams\u001b[0;34m(subsequence, sequence, max_l_dist)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_near_matches_levenshtein_ngrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubsequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_l_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0msubseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubsequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mngram_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubseq_len\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_l_dist\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"]}],"source":["twitter_ids_ranked = ts.twitter_user_search.rank_users_through_info(twitter_ids_dict.keys(), name, email, org, homepage_url)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SNaOTkItsIyS"},"outputs":[],"source":["twitter_ids_ranked"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1678645364719,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"uQV1NfYR7j8b","outputId":"03107822-fac9-4960-8c38-dc065e9f5138"},"outputs":[{"data":{"text/plain":["(False,\n"," 'XUEY85',\n"," ['XUEY85',\n","  'ResearchXu',\n","  'xu3_yi',\n","  'RooockLife',\n","  'hongyi_Xs',\n","  'hongyiXu2',\n","  'carrothsu',\n","  'hongyi_xu',\n","  'xu_hongyi',\n","  'SayaLoki'])"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["# filter ranked IDs to get candidates with highest rank \n","highest_rank = twitter_ids_ranked[list(twitter_ids_ranked.keys())[0]]\n","highest_rank_candidates = [t_id for t_id in twitter_ids_ranked.keys() if twitter_ids_ranked[t_id]>=highest_rank]\n","len(highest_rank_candidates) == 1 and ts._rank_by_similarity(highest_rank_candidates[0], name=term, name_from_gs=name) >= 0.35, highest_rank_candidates[0], list(twitter_ids_ranked.keys())[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1678645508229,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"9KBFrk-4bpLI","outputId":"1ed547cc-1bd1-4496-8999-b11648dff7ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["ResearchXu ['ResearchXu', 'XUEY85', 'researchxu', 'xugroupucsd']\n"]}],"source":["# if there are more candidates with highest rank, use frequency to filter candidates and string matching for ranking\n","if len(twitter_ids_gs_dict) > 0:\n","    highest_rank = twitter_ids_gs_dict[list(twitter_ids_gs_dict.keys())[0]]\n","    highest_rank_candidates = [t_id for t_id in twitter_ids_gs_dict.keys() if twitter_ids_gs_dict[t_id]>=highest_rank]\n","    candidate_list = {}\n","    if len(twitter_ids_gs_dict) != len(highest_rank_candidates):\n","      # rank through name matching needed\n","      for item in highest_rank_candidates:\n","          rank = ts._rank_by_similarity(item, name=term, name_from_gs=name)\n","          if rank >= 0.2:\n","              candidate_list[item] = rank\n","      if len(candidate_list.keys()) > 0:\n","          sorted_candidate_list = sorted(candidate_list.items(), key=lambda item: item[1], reverse=True)\n","          print(sorted_candidate_list[0][0], list(twitter_ids_gs_dict.keys())[:10])\n","    else:\n","      print(\"all have same rank in twitter_ids_gs_dict\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1910,"status":"ok","timestamp":1678645366619,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"n_u1wTAS_eoI","outputId":"d614ded8-209c-4baa-ab18-4faad88d9c02"},"outputs":[{"data":{"text/plain":["(False,\n"," 'XUEY85',\n"," ['XUEY85',\n","  'ResearchXu',\n","  'xu3_yi',\n","  'RooockLife',\n","  'hongyi_Xs',\n","  'hongyiXu2',\n","  'carrothsu',\n","  'hongyi_xu',\n","  'xu_hongyi',\n","  'SayaLoki'])"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["# rank using latest tweets \n","twitter_ids_ranked_by_tweets = ts.twitter_user_search.rank_users_through_tweets(twitter_ids_ranked)\n","# filter ranked IDs to get candidates with highest rank \n","highest_rank = twitter_ids_ranked_by_tweets[list(twitter_ids_ranked_by_tweets.keys())[0]]\n","highest_rank_candidates = [t_id for t_id in twitter_ids_ranked_by_tweets.keys() if twitter_ids_ranked_by_tweets[t_id]>=highest_rank]\n","len(highest_rank_candidates) == 1 and ts._rank_by_similarity(highest_rank_candidates[0], name=term, name_from_gs=name) >= 0.35, highest_rank_candidates[0], highest_rank_candidates[:10]"]},{"cell_type":"markdown","metadata":{"id":"M1rUJCdW8Vzg"},"source":["# Error Analysis"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"ap9Zdz3LPqbt","executionInfo":{"status":"ok","timestamp":1678877847526,"user_tz":-330,"elapsed":920,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}}},"outputs":[],"source":["from pandas.core.internals.managers import ensure_block_shape\n","gs_link_prefix = \"https://scholar.google.com/citations?user=\" \n","twitter_link_prefix = \"https://twitter.com/\"\n","\n","twitter_id_in_top_10 = 0\n","twitter_id_in_top_1 = 0\n","twitter_id_with_no_candidates = 0\n","twitter_id_not_in_candidates = 0\n","null_id_with_no_candidates = 0\n","null_id_with_candidates = 0\n","\n","indices_not_found = []\n","\n","with open(\"/content/drive/My Drive/tweets-dataset/gs_scholars_candidate_twitter_accounts_v2_all_changes.tsv\", \"r\") as f:\n","  reader = csv.reader(f, delimiter=\"\\t\")\n","  for i, line in enumerate(reader):\n","    if i==0:\n","      continue\n","    index = line[0]\n","    name = line[1]\n","    org = line[2]\n","    gs_url = line[3]\n","    twitter_url = line[5]\n","    top_1 = line[6].lower() if line[6] != \"N/A\" and len(line[6]) > 0 else None\n","    top_10 = [id.lower() for id in line[7:] if len(id)>0]\n","    \n","    if twitter_url==\"N/A\":\n","      if len(top_10) == 0:\n","        null_id_with_no_candidates += 1\n","      else:\n","        null_id_with_candidates += 1\n","    else:\n","      twitter_id = twitter_url.split(\"twitter.com/\")[-1]\n","      if len(top_10) == 0:\n","        twitter_id_with_no_candidates += 1\n","      else:\n","        if twitter_id.lower() in top_10:\n","          twitter_id_in_top_10 += 1\n","          if twitter_id.lower() == top_1:\n","            twitter_id_in_top_1 += 1\n","        else:\n","          twitter_id_not_in_candidates += 1"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1678877847526,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"dF0OB4lo8uS8","outputId":"85eb61df-1d0e-4703-e317-53ba7092c3d4"},"outputs":[{"output_type":"stream","name":"stdout","text":["twitter_id_in_top_10 :          119\n","twitter_id_in_top_1 :           101\n","twitter_id_with_no_candidates : 28\n","twitter_id_not_in_candidates :  21\n","null_id_with_no_candidates :    150\n","null_id_with_candidates :       183\n"]}],"source":["print(f'twitter_id_in_top_10 :          {twitter_id_in_top_10}')\n","print(f'twitter_id_in_top_1 :           {twitter_id_in_top_1}')\n","print(f'twitter_id_with_no_candidates : {twitter_id_with_no_candidates}')\n","print(f'twitter_id_not_in_candidates :  {twitter_id_not_in_candidates}')\n","print(f'null_id_with_no_candidates :    {null_id_with_no_candidates}')\n","print(f'null_id_with_candidates :       {null_id_with_candidates}')"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1678877847526,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"-rRxR4Qi8vjv","outputId":"741e453d-944a-438f-c495-40c9d3ec8098"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.3684210526315789"]},"metadata":{},"execution_count":16}],"source":["precision_at_10 = twitter_id_in_top_10 / (twitter_id_in_top_10 + twitter_id_not_in_candidates + null_id_with_candidates)\n","precision_at_10"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1678877847527,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"WJMs_kwE8w1T","outputId":"97067400-d6b2-42a7-b6da-5db0a1ce3d42"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7083333333333334"]},"metadata":{},"execution_count":17}],"source":["recall_at_10 = twitter_id_in_top_10 / (twitter_id_in_top_10 + twitter_id_with_no_candidates + twitter_id_not_in_candidates)\n","recall_at_10"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":811,"status":"ok","timestamp":1678877848332,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"_A3W0g_v8x3W","outputId":"be4fac54-cee3-488b-86a6-9ea442462c8a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4847250509164969"]},"metadata":{},"execution_count":18}],"source":["f_at_10 = (2*precision_at_10*recall_at_10) / (precision_at_10 + recall_at_10)\n","f_at_10"]},{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1678877848332,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"fsl_YKmM8zSM","outputId":"18ae3f73-963e-4aa4-cbaa-5b612fc5ae41"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.3126934984520124"]},"metadata":{},"execution_count":19}],"source":["precision_at_1 = twitter_id_in_top_1 / (twitter_id_in_top_10 + twitter_id_not_in_candidates + null_id_with_candidates)\n","precision_at_1"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1678877848332,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"3jBuG2_980dT","outputId":"bb37a1aa-2ab7-4734-e653-3b4885f439a8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6011904761904762"]},"metadata":{},"execution_count":20}],"source":["recall_at_1 = twitter_id_in_top_1 / (twitter_id_in_top_10 + twitter_id_with_no_candidates + twitter_id_not_in_candidates)\n","recall_at_1"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1678877848332,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"n8je5w_x81Pi","outputId":"87cef99d-92b4-411e-bd52-bcd4a31a67c4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.4114052953156823"]},"metadata":{},"execution_count":21}],"source":["f_at_1 = (2*precision_at_1*recall_at_1) / (precision_at_1 + recall_at_1)\n","f_at_1"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"26CgNDnO82Mn","executionInfo":{"status":"ok","timestamp":1678877848332,"user_tz":-330,"elapsed":6,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}}},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["VlhwyVP3QbSU","CK0fRZjArvz4","sgM6xDo9P47w","rt7MW4cCP-Jo","43Ozad0mQCKZ","UpEts2ATBZ2v","YzGtaaqfJ22P","fEARD8CoYMXq","M1rUJCdW8Vzg"],"provenance":[{"file_id":"1hyPc9u8C9DA3WpLwOxTrJkVmuA5sYbLu","timestamp":1676829237557},{"file_id":"1auhNPM8A6flAc5ai9GipgdsF6Xp2rRWQ","timestamp":1673365201119}],"toc_visible":true},"interpreter":{"hash":"ffc4551cdfa24de1e4d6ff6a879c16b8d7cadd8b756628488549bbbf21e2c19d"},"kernelspec":{"display_name":"Python 3.7.13 ('res')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"}},"nbformat":4,"nbformat_minor":0}