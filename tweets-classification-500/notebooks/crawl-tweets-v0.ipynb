{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["OOT602g9K-FV","9Dj36TFyEpiO","M1oTbbJbEioL","k2aatnsdGZK-"],"mount_file_id":"1hpkD0-G485h635Rp6HI2E0DB-LdhGueV","authorship_tag":"ABX9TyPMQMlyzFtv4IiBzreHu2Jd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#Imports"],"metadata":{"id":"rp0YHYglwWAR"}},{"cell_type":"code","source":["!pip install twitter\n","!pip install tweepy\n","!pip install jsonlines\n","!pip install deepface\n","!pip install icecream"],"metadata":{"id":"8gErjR9Swyot"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tweepy\n","import re\n","from twitter import *\n","import requests\n","import json\n","import csv\n","import jsonlines\n","import time\n","from tqdm import tqdm\n","import numpy as np\n","import pandas as pd"],"metadata":{"id":"kOrd1_vVwqF9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Setup"],"metadata":{"id":"8KZpwrjWFtYo"}},{"cell_type":"code","source":["# API keys\n","api_key = \"\"\n","api_secrets = \"\"\n","bearer_token = \"\"\n","access_token = \"\"\n","access_secret = \"\"\n"," \n","# Authenticate to Twitter\n","auth = tweepy.OAuthHandler(api_key,api_secrets)\n","auth.set_access_token(access_token,access_secret)\n"," \n","api = tweepy.API(auth)\n"," \n","try:\n","    api.verify_credentials()\n","    print('Successful Authentication')\n","except:\n","    print('Failed authentication')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WMZXh37pwUeO","executionInfo":{"status":"ok","timestamp":1673353481527,"user_tz":-330,"elapsed":8,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}},"outputId":"bd011c4d-fb2e-4330-e50e-adc2ae762358"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Successful Authentication\n"]}]},{"cell_type":"code","source":["path = \"/content/drive/My Drive/tweets-dataset/\""],"metadata":{"id":"rrHl1auQFAeu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#change_GS_data.py"],"metadata":{"id":"gwmu09VgJUgd"}},{"cell_type":"code","source":["gs_scholars_np = np.load(path + \"gs_scholars_new.npy\", allow_pickle=True)\n","# for i in range(10):\n","#   print(gs_scholars[i]['name'], gs_scholars[i]['url'][len(\"https://scholar.google.com/citations?hl=en&user=\"):])\n","print(gs_scholars_np[0].keys())\n","meta_info = [[],[],[]] # name, url, urls\n","for i in gs_scholars_np:\n","    meta_info[0].append(i['name'])\n","    meta_info[1].append(i['url'].split(\"user=\")[-1])\n","    text = \"\"\n","    urls = []\n","    meta_info[2].append(urls)\n","\n","print(meta_info[0][0:10])\n","print(meta_info[1][0:10])\n","print(meta_info[2][0:10])\n","a = np.array(meta_info)\n","np.save(path + \"meta_info.npy\", meta_info)"],"metadata":{"id":"n1Z_EbYJJIjs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Eval data"],"metadata":{"id":"OOT602g9K-FV"}},{"cell_type":"code","source":["def get_gs_id(url):\n","  if url != url:\n","    return url\n","  else:\n","    return url[len(\"https://scholar.google.com/citations?hl=en&user=\"):]\n","    \n","def get_t_id(url):\n","  if url != url:\n","    return url\n","  else:\n","    return url[len(\"https://twitter.com/\"):]\n","    \n","gsid_2_tid_eval = pd.read_csv(path + \"gs_scholars_matched_with_twitter_accounts_500.csv\")[:500]\n","gsid_2_tid_eval.pop(gsid_2_tid_eval.columns[0])\n","gsid_2_tid_eval['gs_id'] = gsid_2_tid_eval['url'].apply(lambda x : get_gs_id(x))\n","gsid_2_tid_eval['t_id'] =  gsid_2_tid_eval['url of their twitter'].apply(lambda x: get_t_id(x))\n","gsid_2_tid_eval.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"tbyXwTObrN2n","executionInfo":{"status":"ok","timestamp":1673354161610,"user_tz":-330,"elapsed":335,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}},"outputId":"69ae99c8-da84-4849-f17e-2eac34820041"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                     name                                      organization  \\\n","0         Sebastian Gerke                                    Ree Technology   \n","1                Yang Liu  Computer Science, harbin institute of technology   \n","2             Son N. Tran                            University of Tasmania   \n","3  Massimiliano Ciaramita                                            Google   \n","4        Marco Baity-Jesi                                             Eawag   \n","\n","                                                 url  \\\n","0  https://scholar.google.com/citations?hl=en&use...   \n","1  https://scholar.google.com/citations?hl=en&use...   \n","2  https://scholar.google.com/citations?hl=en&use...   \n","3  https://scholar.google.com/citations?hl=en&use...   \n","4  https://scholar.google.com/citations?hl=en&use...   \n","\n","              url of their twitter         gs_id         t_id  \n","0     https://twitter.com/sebgerke  DeP0anAAAAAJ     sebgerke  \n","1                              NaN  3IC_ZtkAAAAJ          NaN  \n","2  https://twitter.com/sondinhtran  h82eCK8AAAAJ  sondinhtran  \n","3                              NaN  PwsKWfUAAAAJ          NaN  \n","4                              NaN  4S1Ajs8AAAAJ          NaN  "],"text/html":["\n","  <div id=\"df-47aeff05-1b20-40ab-a819-e8478852cf65\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>organization</th>\n","      <th>url</th>\n","      <th>url of their twitter</th>\n","      <th>gs_id</th>\n","      <th>t_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sebastian Gerke</td>\n","      <td>Ree Technology</td>\n","      <td>https://scholar.google.com/citations?hl=en&amp;use...</td>\n","      <td>https://twitter.com/sebgerke</td>\n","      <td>DeP0anAAAAAJ</td>\n","      <td>sebgerke</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Yang Liu</td>\n","      <td>Computer Science, harbin institute of technology</td>\n","      <td>https://scholar.google.com/citations?hl=en&amp;use...</td>\n","      <td>NaN</td>\n","      <td>3IC_ZtkAAAAJ</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Son N. Tran</td>\n","      <td>University of Tasmania</td>\n","      <td>https://scholar.google.com/citations?hl=en&amp;use...</td>\n","      <td>https://twitter.com/sondinhtran</td>\n","      <td>h82eCK8AAAAJ</td>\n","      <td>sondinhtran</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Massimiliano Ciaramita</td>\n","      <td>Google</td>\n","      <td>https://scholar.google.com/citations?hl=en&amp;use...</td>\n","      <td>NaN</td>\n","      <td>PwsKWfUAAAAJ</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Marco Baity-Jesi</td>\n","      <td>Eawag</td>\n","      <td>https://scholar.google.com/citations?hl=en&amp;use...</td>\n","      <td>NaN</td>\n","      <td>4S1Ajs8AAAAJ</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-47aeff05-1b20-40ab-a819-e8478852cf65')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-47aeff05-1b20-40ab-a819-e8478852cf65 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-47aeff05-1b20-40ab-a819-e8478852cf65');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["# save_twitter_metainfo.py"],"metadata":{"id":"6aJm0E7VF1cx"}},{"cell_type":"code","source":["# current algorithm\n","# Step 1 -  search for GS ID name on twitter to get candidates. \n","# Sort candidates on descending number of followers. \n","# Write candidates in sorted order and GS ID in a file - ./intermediate_data.\n","import re\n","import tweepy\n","from twitter import *\n","import requests\n","import json\n","import csv\n","import jsonlines\n","import time\n","from tqdm import tqdm\n","\n","\n","def save_user_info(q, save_name, GS_ID, api):\n","    all_candidate = []\n","    # search the query\n","    users = api.search_users(q)\n","\n","    for user in users:\n","        if user.followers_count > 10 and user.friends_count > 10:\n","            dic = user._json\n","            dic[\"GS_ID\"] = GS_ID\n","            dic[\"GS_Name\"] = q\n","            all_candidate.append(dic)\n","\n","    newlist = sorted(all_candidate, key=lambda d: d['followers_count'], reverse=True)\n","    # print(newlist)\n","    with jsonlines.open(path + \"intermediate_data/\" + save_name + \".jsonl\", 'a') as writer:\n","        # csv_writer = csv.writer(writer)\n","        for each in newlist:\n","            # print(each)\n","            # csv_writer.writerow(each)\n","            writer.write(each)\n","\n","    with jsonlines.open(path + \"intermediate_data/\" + save_name + \".csv\", 'a') as writer:\n","        writer.write(GS_ID)"],"metadata":{"id":"LPVxDwjazQkK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(10):\n","    save_user_info(gsid_2_tid_eval['name'][i], \"search_info\", gsid_2_tid_eval['gs_id'][i], api)\n","    # try:\n","    #     save_user_info(total_info[1][item], \"search_info\", total_info[0][item], api)\n","    # except:\n","    #     time.sleep(20)\n","    #     api, api2 = api2, api\n","    # time.sleep(2)"],"metadata":{"id":"jlGFAucVtp1f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#sketch_match.py"],"metadata":{"id":"9Dj36TFyEpiO"}},{"cell_type":"code","source":["from sys import meta_path\n","import tweepy\n","import csv\n","import requests\n","import os\n","from deepface import DeepFace\n","from twitter import *\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.feature_extraction.text import CountVectorizer\n","from nltk.tokenize import RegexpTokenizer\n","from tqdm import tqdm\n","from icecream import ic\n","import re\n","import jsonlines\n","import json\n","\n","def download_profile_image(name, img_link):\n","    img_link = img_link.replace(\"normal\", \"400x400\")\n","    if \"default_profile_images\" in img_link:\n","        return False\n","    if img_link:\n","        response = requests.get(img_link)\n","        file = open(path + \"temp_pictures/\" + name + '.jpg', \"wb\")\n","        file.write(response.content)\n","        return True\n","    return False\n","\n","def compare_two_images(img1, img2):\n","    try:\n","        result = DeepFace.verify(img1, img2)\n","        if result[\"distance\"] < 0.25:\n","            return True\n","        return False\n","    except:\n","        return False\n","\n","def whether_same_image(img1, img2):\n","    # https://deepai.org/machine-learning-model/image-similarity\n","    r = requests.post(\n","        \"https://api.deepai.org/api/image-similarity\",\n","        files={\n","            'image1': open(img1, 'rb'),\n","            'image2': open(img2, 'rb'),\n","        },\n","        headers={'api-key': '8d00045d-7b5b-4b84-969f-f96d79a4a94c'}\n","    )\n","    if r.json()[\"output\"][\"distance\"] == 0:\n","        return True\n","    return False\n","\n","def match_by_meta_description(str1, str2):\n","    corpus = [str1, str2]\n","    token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n","    count_vectorizer = CountVectorizer(ngram_range=(1, 1), tokenizer=token.tokenize)\n","    vec = count_vectorizer.fit_transform(corpus)\n","    score = cosine_similarity(vec, vec)[0][1]\n","    if score > 0.399:\n","        return True\n","    return False\n","\n","def get_keywords():\n","    field_names = ['natural language processing', 'computer vision', 'machine learning', 'artificial intelligence',\n","                   'deep learning', 'reinforcement learning', 'CV', 'NLP', 'AI', 'RL', 'GAN']\n","    role_names = ['researcher', 'research scientist', 'professor', 'Prof ', 'computational linguist']\n","    org_names = ['university']\n","    misc_names = ['#NLProc']\n","    additional_field_names = []\n","    for field in field_names:\n","        abbrev = \"\"\n","        for word in field.split():\n","            abbrev += word[0]\n","\n","        additional_field_names += [field.replace(' ', '_'), field.replace(' ', ''), abbrev]\n","    field_names += additional_field_names\n","    keywords = field_names + role_names + org_names + misc_names\n","    return keywords\n","\n","\n","def if_keyword_match(text, keywords):\n","    text_lower = text.lower().split()\n","    return any(i in text_lower for i in keywords)\n","\n","\n","def match_by_homepage_link(str1, str2):\n","    corpus = [str1, str2]\n","    token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n","    count_vectorizer = CountVectorizer(ngram_range=(1, 1), tokenizer=token.tokenize, stop_words='english')\n","    vec = count_vectorizer.fit_transform(corpus)\n","    score = cosine_similarity(vec, vec)[0][1]\n","    if score > 0.9:\n","        return True\n","    return False\n","\n","\n","def match_by_tweets_arXiv(twitter_name):\n","    if os.path.exists(path + \"/saved_user_tweets/\" + twitter_name + \".csv\"):\n","        with open(path + \"/saved_user_tweets/\" + twitter_name + \".csv\", \"r\") as csv_file:\n","            csv_reader = csv.reader(csv_file)\n","            for each_line in csv_reader:\n","                text = each_line[3]\n","                urls = re.findall('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-&?=%.]+', text)\n","                for each in urls:\n","                    try:\n","                        r = requests.get(each)\n","                        text = text.replace(each, r.url)\n","                    except:\n","                        continue\n","                if \"arXiv\" in text:\n","                    return True\n","        return False\n","    return False\n","\n","\n","keywords = get_keywords()\n","\n","def match_GS_with_Twitter(file_name, output_file, GS_image_folder):\n","    with open(output_file, 'w') as csv_write:\n","        # csv_reader = csv.reader(csv_file)\n","        import numpy as np\n","        meta_GS_info = np.load(file_name, allow_pickle=True)\n","        csv_writer = csv.writer(csv_write)\n","        for row in meta_GS_info:\n","            real_name = row['name']\n","            if \"/\" in real_name:\n","                continue\n","            # print(real_name)\n","            # search_result = twitter.users.search(q = real_name)\n","            with jsonlines.open(path + \"saved_user_info/\" + real_name + \".jsonl\", 'r') as reader:\n","                found_match = False\n","                for each_candidate in reader:\n","                    candidate_screen_name = each_candidate[\"screen_name\"]\n","                    Twitter_discription = each_candidate[\"description\"]\n","                    if each_candidate[\"entities\"][\"description\"][\"urls\"]:\n","                        description_link = each_candidate[\"entities\"][\"description\"][\"urls\"][0][\"display_url\"]\n","                        Twitter_discription = re.sub(\n","                            r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)',\n","                            description_link, Twitter_discription)\n","                    GS_meta_info = row['extra_info'][0]\n","                    similiar_description = match_by_meta_description(Twitter_discription, GS_meta_info)\n","                    if similiar_description:\n","                        csv_writer.writerow([real_name, candidate_screen_name, \"match by meta description\"])\n","                        found_match = True\n","                    if if_keyword_match(Twitter_discription, keywords):\n","                        csv_writer.writerow([real_name, candidate_screen_name, \"match by keywords in meta description\"])\n","                        found_match = True\n","                if not found_match:\n","                    csv_writer.writerow([real_name, 'Not Found'])\n","\n","\n","def match_single_author(GS_info, twiteer_meta_info):\n","    # return a Boolean, whether it can be matched\n","    candidate_screen_name = twiteer_meta_info[\"screen_name\"]\n","    twitter_discription = twiteer_meta_info[\"description\"]\n","    if twiteer_meta_info[\"entities\"][\"description\"][\"urls\"]:\n","        description_link = twiteer_meta_info[\"entities\"][\"description\"][\"urls\"][0][\"display_url\"]\n","        twitter_discription = re.sub(\n","            r'https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()@:%_\\+.~#?&//=]*)',\n","            description_link, twitter_discription)\n","    personal_webpage = GS_info.get(\"personal_webpage\")\n","    try:\n","        candidate_homepage_link = twiteer_meta_info['entities']['url']['urls'][0]['expanded_url']\n","    except KeyError:\n","        candidate_homepage_link = None\n","    if personal_webpage and candidate_homepage_link:\n","        for i in personal_webpage:\n","            if match_by_homepage_link(candidate_homepage_link, i):\n","                return 1\n","    GS_meta_info = \"\"\n","    for i in GS_info['extra_info']:\n","        GS_meta_info+=i\n","    similiar_description = match_by_meta_description(twitter_discription, GS_meta_info)\n","    if if_keyword_match(twitter_discription, keywords):\n","        return 2\n","    if similiar_description:\n","        return 3\n","    return 0\n","\n","# if __name__ == '__main__':\n","#     import numpy as np\n","#     meta_GS_info = np.load(path + \"/data_from_GS/sample_info.npy\", allow_pickle=True)\n","#     ans = 0\n","#     sum = 0\n","#     for row in meta_GS_info:\n","#         real_name = row['name']\n","#         with jsonlines.open(\"./saved_user_info/\" + real_name + \".jsonl\", 'r') as reader:\n","#             found_match = False\n","#             for each_candidate in reader:\n","#                 if match_single_author(row, each_candidate):\n","#                     ans+=1\n","#         sum += 1\n","#     print(ans)\n","#     print(sum)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NhHvbDH8Dr_y","executionInfo":{"status":"ok","timestamp":1673361032835,"user_tz":-330,"elapsed":16245,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}},"outputId":"8b16916e-b42e-4d7e-f6f8-8b572b4fce1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Directory  /root /.deepface created\n","Directory  /root /.deepface/weights created\n"]}]},{"cell_type":"markdown","source":["#save_tweets.py "],"metadata":{"id":"M1oTbbJbEioL"}},{"cell_type":"code","source":["import re\n","import tweepy\n","from twitter import *\n","import requests\n","import json\n","import csv\n","import jsonlines\n","import time\n","from tqdm import tqdm\n","import time\n","\n","def get_tweets_simple(twitter_name, api):\n","    try:\n","        tweets = api.user_timeline(screen_name=twitter_name, count=10, tweet_mode=\"extended\")\n","        tweets_list = []\n","        for tweet in tweets:\n","            json_object = tweet._json\n","            tweet_info = {\"tweet_id\": json_object[\"id\"], \"num_of_likes\": json_object[\"favorite_count\"],\n","                          \"text\": json_object[\"full_text\"]}\n","            # get full text from retweets\n","            if \"retweeted_status\" in json_object:\n","                tweet_info = {\"tweet_id\": json_object[\"id\"], \"num_of_likes\": json_object[\"favorite_count\"],\n","                              \"text\": json_object[\"retweeted_status\"][\"full_text\"]}\n","            # change the form of link in the tweets\n","            urls = re.findall('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-&?=%.]+', tweet_info[\"text\"])\n","            for each in urls:\n","                try:\n","                    r = requests.get(each)\n","                    tweet_info[\"text\"] = tweet_info[\"text\"].replace(each, r.url)\n","                except:\n","                    continue\n","            tweets_list.append(tweet_info)\n","        return tweets_list\n","    except:\n","        return None\n","\n","\n","def get_tweets_simple_qnqq(twitter_name, api):\n","    try:\n","        tweets = api.user_timeline(screen_name=twitter_name, count=10, tweet_mode=\"extended\")\n","        tweets_list = []\n","        for tweet in tweets:\n","            json_object = tweet._json\n","            tweets_list.append(json_object)\n","        return tweets_list\n","    except:\n","        return None\n","\n","def get_tweets_qnqq(twitter_name, api):\n","    try:\n","        tweets = api.user_timeline(screen_name=twitter_name, count=100, tweet_mode=\"extended\")\n","        tweets_list = []\n","        for tweet in tweets:\n","            json_object = tweet._json\n","            tweets_list.append(json_object)\n","        return tweets_list\n","    except:\n","        return None\n","\n","def get_tweets(twitter_name, api):\n","    current_time = time.time()\n","    try:\n","        tweets = api.user_timeline(screen_name=twitter_name, count=300, tweet_mode=\"extended\")\n","        print(f\"time crawing one tweets is {time.time()-current_time}\")\n","        current_time = time.time()\n","        tweets_list = []\n","        for tweet in tweets:\n","            json_object = tweet._json\n","            tweet_info = {\"tweet_id\": json_object[\"id\"], \"num_of_likes\": json_object[\"favorite_count\"],\n","                          \"text\": json_object[\"full_text\"]}\n","            # get full text from retweets\n","            if \"retweeted_status\" in json_object:\n","                tweet_info = {\"tweet_id\": json_object[\"id\"], \"num_of_likes\": json_object[\"favorite_count\"],\n","                              \"text\": json_object[\"retweeted_status\"][\"full_text\"]}\n","            # change the form of link in the tweets\n","            urls = re.findall('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-&?=%.]+', tweet_info[\"text\"])\n","            for each in urls:\n","                try:\n","                    r = requests.get(each)\n","                    tweet_info[\"text\"] = tweet_info[\"text\"].replace(each, r.url)\n","                except:\n","                    continue\n","            tweets_list.append(tweet_info)\n","        print(f\"time processing one tweets is {time.time()-current_time}\")\n","        return tweets_list\n","    except:\n","        return None\n","\n","def get_full_tweets(twitter_name, api):\n","    pass\n","\n","# if __name__ == \"__main__\":\n","#     import numpy as np\n","#     total_info = np.load(\"./data_from_GS/base_info_clean.npy\", allow_pickle=True)\n","#     searched_dict = {}\n","#     for i in tqdm(range(100)):\n","#         get_tweets(\"aarnilmari\", api)\n","#         time.sleep(1)"],"metadata":{"id":"ZdwrO8_6ENy9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#match_and_save.py"],"metadata":{"id":"k2aatnsdGZK-"}},{"cell_type":"code","source":["# Step 2 - Use match_and_save.py to make a sketch match by the type 1, 2, 3 match and save those users tweets\n","# type 1: matched by personal website\n","# type 2: matched by keyword\n","# type3: matched by similar description with the information in GS\n","\n","import re\n","import tweepy\n","from twitter import *\n","import requests\n","import json\n","import csv\n","import jsonlines\n","import time\n","from tqdm import tqdm\n","import numpy as np\n","from icecream import ic\n","\n","GS_info_path = path + \"/gs_scholars_new.npy\"\n","supplement_GS_info_path = path + \"/data_from_GS/GS_info_beta.npy\"\n","search_info_path = path + \"intermediate_data/search_info.jsonl\"\n","tweets_save_path = path + \"final_output/tweets_data_website_matching.jsonl\"\n","GS_save_path = path + \"/final_output/GS_data_website_matching.jsonl\"\n","\n","tweets_dict = {}\n","with jsonlines.open(tweets_save_path, 'r') as reader:\n","    for i in reader:\n","        tweets_dict[i[\"screen_name\"]] = 1\n","print(\"Finish load screen_name\")\n","\n","GS_info = np.load(GS_info_path, allow_pickle=True)\n","GS_info_dict = {}\n","for i in range(len(GS_info[0])):\n","    id = GS_info[1][i]\n","    GS_info_dict[id] = {\n","        \"name\":GS_info[0][i],\n","        \"id\":GS_info[1][i],\n","        \"personal_webpage\":GS_info[2][i],\n","    }\n","supplement_GS_info = np.load(supplement_GS_info_path, allow_pickle=True)\n","for i in supplement_GS_info:\n","    id = i.get('url').split(\"user=\")[-1]\n","    if (GS_info_dict.get(id)==None):\n","        GS_info_dict[id] = i\n","\n","current_time = 0\n","import time\n","last_time = time.time()\n","with jsonlines.open(search_info_path, 'r') as reader:\n","    now_id = \"\"\n","    candidate_list = []\n","    for each_candidate in reader:\n","        if each_candidate[\"GS_ID\"] != now_id:\n","            GS_author_dict = {}\n","            GS_author_dict[\"id\"] = now_id\n","            GS_author_dict[\"screen_name_list\"] = []\n","            GS_author_dict[\"matched_type\"] = []\n","\n","            if (len(candidate_list)!=0):\n","                matched_list = []\n","                if (GS_info_dict.get(now_id)!=None):\n","                    GS_info_now = GS_info_dict[now_id]\n","                    for i in candidate_list:\n","                        if match_single_author(GS_info_now, i)>0:\n","                            matched_list.append([i, match_single_author(GS_info_now, i)])\n","                if (len(matched_list) == 0):\n","                    GS_author_dict[\"is_matched\"] = False\n","                    for i in candidate_list:\n","                        screeen_name = i[\"screen_name\"]\n","                        if (tweets_dict.get(screeen_name) != None):\n","                            continue\n","                        GS_author_dict[\"screen_name_list\"].append(screeen_name)\n","                        GS_author_dict[\"matched_type\"].append(0)\n","                        tweets_list = get_tweets_simple_qnqq(screeen_name, api)\n","                        if (tweets_list == None):\n","                            api = apis.change_api()\n","                        else:\n","                            current_time += 1\n","                            if current_time % 100 == 0:\n","                                print(f\"time usage: {time.time() - last_time}\")\n","                                last_time = time.time()\n","                            with jsonlines.open(tweets_save_path, 'a') as writer:\n","                                writer.write({\"screen_name\":screeen_name,\"tweets\":tweets_list})\n","                            del tweets_list\n","                            tweets_dict[screeen_name] = 1\n","                else:\n","                    GS_author_dict[\"is_matched\"] = True\n","                    for i in matched_list:\n","                        screeen_name = i[0][\"screen_name\"]\n","                        if (tweets_dict.get(screeen_name) != None):\n","                            continue\n","                        GS_author_dict[\"screen_name_list\"].append(screeen_name)\n","                        GS_author_dict[\"matched_type\"].append(i[1])\n","                        tweets_list = get_tweets_qnqq(screeen_name, api)\n","                        if (tweets_list == None):\n","                            api = apis.change_api()\n","                        else:\n","                            current_time += 1\n","                            if current_time % 100 == 0:\n","                                print(f\"time usage: {time.time() - last_time}\")\n","                                last_time = time.time()\n","                            with jsonlines.open(tweets_save_path, 'a') as writer:\n","                                writer.write({\"screen_name\":screeen_name,\"tweets\":tweets_list})\n","                            del tweets_list\n","                            tweets_dict[screeen_name] = 1\n","\n","            with jsonlines.open(GS_save_path, 'a') as writer:\n","                writer.write(GS_author_dict)\n","\n","            del GS_author_dict\n","            del candidate_list\n","            candidate_list = [each_candidate]\n","            now_id = each_candidate[\"GS_ID\"]\n","        else:\n","            candidate_list.append(each_candidate)"],"metadata":{"id":"TEEIC-3osAb9"},"execution_count":null,"outputs":[]}]}