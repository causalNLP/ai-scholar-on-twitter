{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"tdCiCGpaOrxc"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"PEWY6iMCOrxf"},"source":["# google scholar search"]},{"cell_type":"markdown","metadata":{"id":"VlhwyVP3QbSU"},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":119565,"status":"ok","timestamp":1676810321109,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"X4OwlpQLgPhH","outputId":"f20dbf97-0f73-40ec-e4fb-e3c2f8bb432a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49743,"status":"ok","timestamp":1676810370848,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"GMEIh6fFkiLk","outputId":"3228e201-0584-433a-a820-cdcce21852b2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Executing: /tmp/apt-key-gpghome.Ntv4MLRgSX/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n","gpg: key DCC9EFBF77E11517: public key \"Debian Stable Release Key (10/buster) <debian-release@lists.debian.org>\" imported\n","gpg: Total number processed: 1\n","gpg:               imported: 1\n","Executing: /tmp/apt-key-gpghome.sh7hgi4kqt/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n","gpg: key DC30D7C23CBBABEE: public key \"Debian Archive Automatic Signing Key (10/buster) <ftpmaster@debian.org>\" imported\n","gpg: Total number processed: 1\n","gpg:               imported: 1\n","Executing: /tmp/apt-key-gpghome.p5IzuGrKFS/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n","gpg: key 4DFAB270CAA96DFA: public key \"Debian Security Archive Automatic Signing Key (10/buster) <ftpmaster@debian.org>\" imported\n","gpg: Total number processed: 1\n","gpg:               imported: 1\n","Warning: apt-key output should not be parsed (stdout is not a terminal)\n","Warning: apt-key output should not be parsed (stdout is not a terminal)\n","Warning: apt-key output should not be parsed (stdout is not a terminal)\n","Get:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n","Get:2 http://deb.debian.org/debian buster InRelease [122 kB]\n","Get:3 http://deb.debian.org/debian buster-updates InRelease [56.6 kB]\n","Get:4 http://deb.debian.org/debian-security buster/updates InRelease [34.8 kB]\n","Get:5 http://deb.debian.org/debian buster/main amd64 Packages [10.7 MB]\n","Get:6 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n","Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n","Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n","Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n","Get:10 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB]\n","Get:11 http://deb.debian.org/debian buster-updates/main amd64 Packages [9,745 B]\n","Get:12 http://deb.debian.org/debian-security buster/updates/main amd64 Packages [550 kB]\n","Hit:13 http://archive.ubuntu.com/ubuntu focal InRelease\n","Get:15 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n","Hit:16 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n","Get:17 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n","Hit:18 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n","Get:19 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2,970 kB]\n","Hit:20 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n","Hit:21 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n","Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,386 kB]\n","Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,131 kB]\n","Fetched 18.4 MB in 6s (2,865 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-510\n","Use 'apt autoremove' to remove it.\n","The following additional packages will be installed:\n","  chromium-common chromium-sandbox libevent-2.1-6 libicu63 libimobiledevice6\n","  libjpeg62-turbo libplist3 libre2-5 libu2f-udev libusbmuxd6 libvpx5\n","  libxxf86dga1 upower usbmuxd x11-utils\n","Suggested packages:\n","  chromium-l10n chromium-shell libusbmuxd-tools mesa-utils\n","The following NEW packages will be installed:\n","  chromium chromium-common chromium-driver chromium-sandbox libevent-2.1-6\n","  libicu63 libimobiledevice6 libjpeg62-turbo libplist3 libre2-5 libu2f-udev\n","  libusbmuxd6 libvpx5 libxxf86dga1 upower usbmuxd x11-utils\n","0 upgraded, 17 newly installed, 0 to remove and 22 not upgraded.\n","Need to get 74.6 MB of archives.\n","After this operation, 256 MB of additional disk space will be used.\n","Get:1 http://deb.debian.org/debian buster/main amd64 libevent-2.1-6 amd64 2.1.8-stable-4 [177 kB]\n","Get:2 http://deb.debian.org/debian buster/main amd64 libicu63 amd64 63.1-6+deb10u3 [8,293 kB]\n","Get:3 http://deb.debian.org/debian buster/main amd64 libjpeg62-turbo amd64 1:1.5.2-2+deb10u1 [133 kB]\n","Get:4 http://deb.debian.org/debian buster/main amd64 libvpx5 amd64 1.7.0-3+deb10u1 [800 kB]\n","Get:5 http://deb.debian.org/debian buster/main amd64 chromium-common amd64 90.0.4430.212-1~deb10u1 [1,423 kB]\n","Get:6 http://deb.debian.org/debian buster/main amd64 chromium amd64 90.0.4430.212-1~deb10u1 [58.3 MB]\n","Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 libre2-5 amd64 20200101+dfsg-1build1 [162 kB]\n","Get:8 http://deb.debian.org/debian buster/main amd64 chromium-driver amd64 90.0.4430.212-1~deb10u1 [4,703 kB]\n","Get:9 http://deb.debian.org/debian buster/main amd64 chromium-sandbox amd64 90.0.4430.212-1~deb10u1 [146 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu focal/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu1 [12.0 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 x11-utils amd64 7.7+5 [199 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 libplist3 amd64 2.1.0-4build2 [31.6 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu focal/main amd64 libusbmuxd6 amd64 2.0.1-2 [19.1 kB]\n","Get:14 http://archive.ubuntu.com/ubuntu focal/main amd64 libimobiledevice6 amd64 1.2.1~git20191129.9f79242-1build1 [65.2 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu focal/main amd64 libu2f-udev all 1.1.10-1 [6,108 B]\n","Get:16 http://archive.ubuntu.com/ubuntu focal/main amd64 upower amd64 0.99.11-1build2 [104 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu focal/main amd64 usbmuxd amd64 1.1.1~git20191130.9af2b12-1 [38.4 kB]\n","Fetched 74.6 MB in 2s (46.1 MB/s)\n","Selecting previously unselected package libevent-2.1-6:amd64.\n","(Reading database ... 128126 files and directories currently installed.)\n","Preparing to unpack .../00-libevent-2.1-6_2.1.8-stable-4_amd64.deb ...\n","Unpacking libevent-2.1-6:amd64 (2.1.8-stable-4) ...\n","Selecting previously unselected package libicu63:amd64.\n","Preparing to unpack .../01-libicu63_63.1-6+deb10u3_amd64.deb ...\n","Unpacking libicu63:amd64 (63.1-6+deb10u3) ...\n","Selecting previously unselected package libjpeg62-turbo:amd64.\n","Preparing to unpack .../02-libjpeg62-turbo_1%3a1.5.2-2+deb10u1_amd64.deb ...\n","Unpacking libjpeg62-turbo:amd64 (1:1.5.2-2+deb10u1) ...\n","Selecting previously unselected package libre2-5:amd64.\n","Preparing to unpack .../03-libre2-5_20200101+dfsg-1build1_amd64.deb ...\n","Unpacking libre2-5:amd64 (20200101+dfsg-1build1) ...\n","Selecting previously unselected package libvpx5:amd64.\n","Preparing to unpack .../04-libvpx5_1.7.0-3+deb10u1_amd64.deb ...\n","Unpacking libvpx5:amd64 (1.7.0-3+deb10u1) ...\n","Selecting previously unselected package libxxf86dga1:amd64.\n","Preparing to unpack .../05-libxxf86dga1_2%3a1.1.5-0ubuntu1_amd64.deb ...\n","Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu1) ...\n","Selecting previously unselected package x11-utils.\n","Preparing to unpack .../06-x11-utils_7.7+5_amd64.deb ...\n","Unpacking x11-utils (7.7+5) ...\n","Selecting previously unselected package chromium-common.\n","Preparing to unpack .../07-chromium-common_90.0.4430.212-1~deb10u1_amd64.deb ...\n","Unpacking chromium-common (90.0.4430.212-1~deb10u1) ...\n","Selecting previously unselected package chromium.\n","Preparing to unpack .../08-chromium_90.0.4430.212-1~deb10u1_amd64.deb ...\n","Unpacking chromium (90.0.4430.212-1~deb10u1) ...\n","Selecting previously unselected package chromium-driver.\n","Preparing to unpack .../09-chromium-driver_90.0.4430.212-1~deb10u1_amd64.deb ...\n","Unpacking chromium-driver (90.0.4430.212-1~deb10u1) ...\n","Selecting previously unselected package chromium-sandbox.\n","Preparing to unpack .../10-chromium-sandbox_90.0.4430.212-1~deb10u1_amd64.deb ...\n","Unpacking chromium-sandbox (90.0.4430.212-1~deb10u1) ...\n","Selecting previously unselected package libplist3:amd64.\n","Preparing to unpack .../11-libplist3_2.1.0-4build2_amd64.deb ...\n","Unpacking libplist3:amd64 (2.1.0-4build2) ...\n","Selecting previously unselected package libusbmuxd6:amd64.\n","Preparing to unpack .../12-libusbmuxd6_2.0.1-2_amd64.deb ...\n","Unpacking libusbmuxd6:amd64 (2.0.1-2) ...\n","Selecting previously unselected package libimobiledevice6:amd64.\n","Preparing to unpack .../13-libimobiledevice6_1.2.1~git20191129.9f79242-1build1_amd64.deb ...\n","Unpacking libimobiledevice6:amd64 (1.2.1~git20191129.9f79242-1build1) ...\n","Selecting previously unselected package libu2f-udev.\n","Preparing to unpack .../14-libu2f-udev_1.1.10-1_all.deb ...\n","Unpacking libu2f-udev (1.1.10-1) ...\n","Selecting previously unselected package upower.\n","Preparing to unpack .../15-upower_0.99.11-1build2_amd64.deb ...\n","Unpacking upower (0.99.11-1build2) ...\n","Selecting previously unselected package usbmuxd.\n","Preparing to unpack .../16-usbmuxd_1.1.1~git20191130.9af2b12-1_amd64.deb ...\n","Unpacking usbmuxd (1.1.1~git20191130.9af2b12-1) ...\n","Setting up libplist3:amd64 (2.1.0-4build2) ...\n","Setting up libu2f-udev (1.1.10-1) ...\n","Failed to send reload request: No such file or directory\n","Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu1) ...\n","Setting up chromium-sandbox (90.0.4430.212-1~deb10u1) ...\n","Setting up libicu63:amd64 (63.1-6+deb10u3) ...\n","Setting up libjpeg62-turbo:amd64 (1:1.5.2-2+deb10u1) ...\n","Setting up libevent-2.1-6:amd64 (2.1.8-stable-4) ...\n","Setting up libusbmuxd6:amd64 (2.0.1-2) ...\n","Setting up x11-utils (7.7+5) ...\n","Setting up libre2-5:amd64 (20200101+dfsg-1build1) ...\n","Setting up chromium-common (90.0.4430.212-1~deb10u1) ...\n","Setting up libimobiledevice6:amd64 (1.2.1~git20191129.9f79242-1build1) ...\n","Setting up libvpx5:amd64 (1.7.0-3+deb10u1) ...\n","Setting up upower (0.99.11-1build2) ...\n","Setting up usbmuxd (1.1.1~git20191130.9af2b12-1) ...\n","Warning: The home dir /var/lib/usbmux you specified can't be accessed: No such file or directory\n","Adding system user `usbmux' (UID 107) ...\n","Adding new user `usbmux' (UID 107) with group `plugdev' ...\n","Not creating home directory `/var/lib/usbmux'.\n","Setting up chromium (90.0.4430.212-1~deb10u1) ...\n","update-alternatives: using /usr/bin/chromium to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n","update-alternatives: using /usr/bin/chromium to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n","Setting up chromium-driver (90.0.4430.212-1~deb10u1) ...\n","Processing triggers for man-db (2.9.1-1) ...\n","Processing triggers for dbus (1.12.16-2ubuntu2.3) ...\n","Processing triggers for mime-support (3.64ubuntu1) ...\n","Processing triggers for hicolor-icon-theme (0.17-2) ...\n","Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting selenium\n","  Downloading selenium-4.8.2-py3-none-any.whl (6.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.8/dist-packages (from selenium) (2022.12.7)\n","Collecting urllib3[socks]~=1.26\n","  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting trio-websocket~=0.9\n","  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n","Collecting trio~=0.17\n","  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 KB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting outcome\n","  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n","Collecting sniffio\n","  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n","Collecting exceptiongroup>=1.0.0rc9\n","  Downloading exceptiongroup-1.1.0-py3-none-any.whl (14 kB)\n","Collecting async-generator>=1.9\n","  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium) (2.4.0)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium) (22.2.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium) (2.10)\n","Collecting wsproto>=0.14\n","  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n","Collecting h11<1,>=0.9.0\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: urllib3, sniffio, outcome, h11, exceptiongroup, async-generator, wsproto, trio, trio-websocket, selenium\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed async-generator-1.10 exceptiongroup-1.1.0 h11-0.14.0 outcome-1.2.0 selenium-4.8.2 sniffio-1.3.0 trio-0.22.0 trio-websocket-0.9.2 urllib3-1.26.14 wsproto-1.2.0\n"]},{"data":{"text/plain":[]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["%%shell\n","# Ubuntu no longer distributes chromium-browser outside of snap\n","#\n","# Proposed solution: https://askubuntu.com/questions/1204571/how-to-install-chromium-without-snap\n","\n","# Add debian buster\n","cat > /etc/apt/sources.list.d/debian.list <<'EOF'\n","deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main\n","deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main\n","deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main\n","EOF\n","\n","# Add keys\n","apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n","apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n","apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n","\n","apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg\n","apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg\n","apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg\n","\n","# Prefer debian repo for chromium* packages only\n","# Note the double-blank lines between entries\n","cat > /etc/apt/preferences.d/chromium.pref << 'EOF'\n","Package: *\n","Pin: release a=eoan\n","Pin-Priority: 500\n","\n","\n","Package: *\n","Pin: origin \"deb.debian.org\"\n","Pin-Priority: 300\n","\n","\n","Package: chromium*\n","Pin: origin \"deb.debian.org\"\n","Pin-Priority: 700\n","EOF\n","\n","# Install chromium and chromium-driver\n","apt-get update\n","apt-get install chromium chromium-driver\n","\n","# Install selenium\n","pip install selenium"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uqc0S4N-ksyb"},"outputs":[],"source":["# from selenium import webdriver\n","# from selenium.webdriver.chrome.options import Options\n","\n","# url = \"https://github.com/googlecolab/colabtools/issues/3347\" \n","# options = Options()\n","# options.add_argument(\"--headless\")\n","# options.add_argument(\"--no-sandbox\")\n","\n","# # options.headless = True\n","\n","# driver = webdriver.Chrome(\"/usr/bin/chromedriver\", options=options)\n","\n","# driver.get(url)\n","# print(driver.title)\n","# driver.quit()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6683,"status":"ok","timestamp":1676810377524,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"Ta260ht0TFGX","outputId":"faf1e92b-1399-47b5-ec03-468a13e674e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting jsonlines\n","  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from jsonlines) (22.2.0)\n","Installing collected packages: jsonlines\n","Successfully installed jsonlines-3.1.0\n"]}],"source":["!pip install jsonlines\n","# !pip install selenium\n","# !pip install webdriver-manager\n","# !pip install webdriver-manager==3.8.5\n","# !apt-get update\n","# !apt-get install chromium\n","# !apt install chromium-chromedriver\n","# !cp /usr/lib/chromium-browser/chromedriver /usr/bin"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DljCawwaOrxf"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import pickle\n","import json\n","import jsonlines\n","import csv"]},{"cell_type":"markdown","metadata":{"id":"CK0fRZjArvz4"},"source":["###Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vRap3bc7UrUx"},"outputs":[],"source":["def generate_or_keyword_list(query_dict: dict):\n","    \"\"\"Generate necessary keyword lists to help selecting final candidates.\"\"\"\n","    or_keyword_list = []\n","    or_keyword_dict = {}\n","    or_keyword_dict['gs_sid'] = ''\n","    domain_labels = []\n","    if 'expertise' in query_dict['profile']['content']:\n","        for keyword in query_dict['profile']['content']['expertise']:\n","            for key in keyword['keywords']:\n","                key = key.strip().lower()\n","                domain_labels.append(key)\n","    or_keyword_dict['domain_labels'] = domain_labels\n","\n","    coauthors = []\n","    if 'relations' in query_dict['profile']['content'] and len(query_dict['profile']['content']['relations']) > 0:\n","        for relation in query_dict['profile']['content']['relations']:\n","            coauthors.append(relation['name'])\n","    or_keyword_dict['coauthors'] = coauthors\n","\n","    if 'history' in query_dict['profile']['content'] and len(query_dict['profile']['content']['history']) > 0:\n","        tmp_dict = query_dict['profile']['content']['history'][0]\n","        if 'position' in tmp_dict:\n","            or_keyword_dict['position'] = tmp_dict['position']\n","        if 'institution' in tmp_dict:\n","            if 'domain' in tmp_dict['institution']:\n","                or_keyword_dict['email_suffix'] = tmp_dict['institution']['domain']\n","            if 'name' in tmp_dict['institution']:\n","                or_keyword_dict['organization'] = tmp_dict['institution']['name']\n","\n","    or_keyword_list.append(or_keyword_dict)\n","\n","    return or_keyword_list\n","\n","from difflib import SequenceMatcher\n","\n","def get_str_similarity(a: str, b: str) -> float:\n","    \"\"\"Calculate the similarity of two strings and return a similarity ratio.\"\"\"\n","    return SequenceMatcher(None, a, b).ratio()"]},{"cell_type":"markdown","metadata":{"id":"sgM6xDo9P47w"},"source":["### Scholar78kSearch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iFhE-0YePq3U"},"outputs":[],"source":["import pandas as pd\n","import os\n","import re\n","import numpy as np\n","from typing import Union, List\n","\n","\n","class Scholar78kSearch():\n","    def __init__(self):\n","        self.get_78kdata()\n","        self.simple = False\n","        self.verbose = False\n","        self.print_true = False\n","\n","    def get_78kdata(self, source='gdrive'):\n","        \"\"\"Download and load the 78k dataset data.\n","        \n","        Parameters\n","        ----------\n","        source : default is 'gdrive'.\n","        \"\"\"\n","        # path_name = 'gs_scholars_all.npy'\n","        path_name = 'gs_scholars_new.npy'\n","        if source == 'gdrive':\n","            # import gdown\n","            # if not os.path.exists('source'):\n","            #     os.mkdir('source')\n","            # if not os.path.exists(f'source/{path_name}'):\n","            #     gdown.download(\n","            #         'https://self.drive.google.com/uc?id=1NTvn_HiGX3Lr0FtTw5ot3UxcdeNLsv7h',\n","            #         f'source/{path_name}'\n","            #     )\n","            # self.df = pd.DataFrame.from_records(np.load(f'source/{path_name}', allow_pickle=True))\n","            self.df = pd.DataFrame.from_records(np.load(\"/content/drive/My Drive/tweets-dataset/gs_scholars_new.npy\", allow_pickle=True))\n","        else:\n","            raise NotImplementedError\n","    \n","    def search_name(self, name: Union[str, list], query_dict: dict = None) -> List[dict]:\n","        \"\"\"Search scholar candidates given name in the 78k AI scholar dataset.\n","        \n","        Parameters\n","        ----------\n","        name : name of the scholar.\n","        query_dict : if this is given, the method will run <self._search_name_others_helper()>\n","\n","        Returns\n","        -------\n","        df_row_list : a list of response dictionaries.\n","        \n","        \"\"\"\n","        if type(name) is list:\n","            name_list = [name[0], name[-1]]\n","            name = f'{name[0]} {name[-1]}' \n","        elif type(name) is str:\n","            name_list = re.sub('[0-9_\\.\\(\\)\\[\\],]', '', name).split(' ')\n","        else:\n","            raise TypeError(f'Argument \"name\" passed to Scholar78kSearch.search_name has the wrong type.')\n","        df_row = self._search_name_only_helper(name, name_list)\n","        if df_row.shape[0] > 0 and query_dict is not None:\n","            df_row = self._search_name_others_helper(df_row, query_dict)\n","        if self.print_true:\n","            print(f'[Info] Found {df_row.shape[0]} scholars are in 78k data.')\n","            print(f'[Debug] Names: {df_row[\"name\"]}')\n","        if self.verbose:\n","            print(df_row)\n","        return self._deal_with_simple(df_row)\n","        # return df_row\n","\n","    def _deal_with_simple(self, df_row):\n","        if self.simple:\n","            df_row = df_row.loc[:, df_row.columns != 'papers']\n","        df_row = df_row.drop(['co_authors_all'], axis=1)\n","        return df_row.to_dict(orient='records')\n","\n","    def _search_name_only_helper(self, name, name_list):\n","        \"\"\"Helper function of search_name\n","\n","        Returns\n","        -------\n","        Boolean : found or not.\n","        DataFrame : if find else None.\n","        \"\"\"\n","        # find the scholar in our dataset\n","        name_df = self.df.loc[self.df['name'] == name].copy()\n","        name_list_df = self.df.loc[self.df['name'].str.contains(pat = f'^{name_list[0].capitalize()} .*{name_list[-1].capitalize()}', regex=True, case=False)].copy()\n","        return pd.concat([name_df, name_list_df]).drop_duplicates(subset=['url']).reset_index(drop=True)\n","\n","    def _search_name_others_helper(self, df_row, query_dict):\n","        # TODO: add a better filter more than by name\n","        return df_row"]},{"cell_type":"markdown","metadata":{"id":"rt7MW4cCP-Jo"},"source":["### GoogleSearch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kEFrgb4FPupy"},"outputs":[],"source":["import re\n","import time\n","from typing import Union\n","# from selenium import webdriver\n","# from selenium.webdriver.chrome.options import ChromiumOptions\n","from selenium.webdriver.chromium.webdriver import ChromiumDriver\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.remote.errorhandler import NoSuchElementException\n","# from webdriver_manager.chrome import ChromeDriverManager\n","\n","from selenium import webdriver\n","from selenium.webdriver.chrome.options import Options\n","\n","# url = \"https://github.com/googlecolab/colabtools/issues/3347\" \n","\n","class GoogleSearch():\n","    \"\"\"Base class for performing web search on Google using REST API.\"\"\"\n","    def __init__(self, driver_path):\n","        self.setup_webdriver(driver_path)\n","    \n","    def setup_webdriver(self, driver_path):\n","        \"\"\"Setup the webdriver object.\"\"\"\n","        options = Options()\n","        options.add_argument(\"--headless\")\n","        options.add_argument(\"--no-sandbox\")\n","        options.headless = True\n","        # options = ChromiumOptions()\n","        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n","        options.add_experimental_option('useAutomationExtension', False)\n","        # options.add_argument('--headless')\n","        # options.add_argument('--no-sandbox')\n","        options.add_argument('--disable-dev-shm-usage')\n","\n","        # self.driver = webdriver.Chrome(driver_path, options=options)\n","        # # self.driver = webdriver.Chrome(ChromeDriverManager().install())\n","\n","        self.driver = webdriver.Chrome(driver_path, options=options)\n","\n","        self.driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n","            \"source\": \"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\"\n","        })\n","\n","        \n","\n","\n","class ScholarGsSearch(GoogleSearch):\n","    \"\"\"Class that handling searching on Google Scholar webpage using REST GET API.\"\"\"\n","    def __init__(self, driver_path):\n","        super().__init__(driver_path)\n","        self._authsearch = 'https://scholar.google.com/citations?hl=en&view_op=search_authors&mauthors={0}'\n","        self._gsidsearch = 'https://scholar.google.com/citations?hl=en&user={0}'\n","        self.print_true = False\n","        # print(\"ScholarGsSearch initiated\")\n","    \n","    def change_name(self, name):\n","        new_name = name[1:].split('_')\n","        new_name[-1] = re.sub(r'[0-9]+', '', new_name[-1])\n","        new_name = ' '.join(new_name)\n","        return new_name\n","\n","    def search_gsid(self, gs_sid: str, simple: bool = True):\n","        \"\"\"Search scholar on Google Scholar based given gs_sid.\n","        \n","        Parameters\n","        ----------\n","        gs_sid : google scholar sid\n","        simple : whether return simple information without paper list.\n","\n","        Returns\n","        -------\n","        scholar_dict_list : a list of dicts of responses.\n","        \n","        \"\"\"\n","        url = self._gsidsearch.format(gs_sid)\n","        self.search_gs_url(url, simple=simple)\n","        \n","    def search_gs_url(self, url: str, simple: bool = True):\n","        self.driver.get(url)\n","        scholar_dict = self._search_gsid_helper(self.driver, url, simple=simple)\n","        time.sleep(5)\n","        if scholar_dict is not None:\n","            \n","            return [scholar_dict]\n","        else:\n","            if self.print_true:\n","                print('[Info] No scholars found given gs_sid in search_gs.')\n","            return []\n","\n","        \n","    def _search_gsid_helper(self, driver: ChromiumDriver, url: str, simple: bool = True):\n","        \"\"\"Helper function for search_gsid.\"\"\"\n","\n","        def get_single_author(element):\n","            li=[]\n","            li.append(element.find_elements(By.TAG_NAME, \"a\")[0].get_attribute('href'))\n","            li.append(element.find_elements(By.TAG_NAME, \"a\")[0].get_attribute('textContent'))\n","            for i in element.find_elements(By.CLASS_NAME, \"gsc_rsb_a_ext\"):\n","                li.append(i.get_attribute('textContent'))\n","            return li\n","\n","        html_first_class = driver.find_elements(By.CLASS_NAME, \"gsc_g_hist_wrp\")\n","        if (len(html_first_class)==0):\n","            if self.print_true:\n","                print(\"[Info] len(html_first_class)==0\")\n","            return None\n","        idx_list = html_first_class[0].find_elements(By.CLASS_NAME, \"gsc_md_hist_b\")[0]\n","        years =  [i.get_attribute('textContent') for i in idx_list.find_elements(By.CLASS_NAME, \"gsc_g_t\")]\n","        cites =  [i.get_attribute('innerHTML') for i in idx_list.find_elements(By.CLASS_NAME, \"gsc_g_al\")]\n","        rsb = driver.find_elements(By.CLASS_NAME, \"gsc_rsb\")[0]\n","        Citations_table=[i.get_attribute('textContent') for i in  rsb.find_elements(By.CLASS_NAME, \"gsc_rsb_std\")]\n","        Co_authors = rsb.find_elements(By.CLASS_NAME, \"gsc_rsb_a\")\n","        if len(Co_authors) == 0:\n","            Co_authors = None\n","        else:\n","            Co_authors = [get_single_author(i) for i in rsb.find_element(By.CLASS_NAME, \"gsc_rsb_a\").find_elements(By.CLASS_NAME, \"gsc_rsb_a_desc\")]\n","\n","        Researcher = {\"url\": url}\n","        gs_sid = None\n","        if 'user=' in url:\n","            tmp_gs_sid = url.split('user=', 1)[1]\n","            if len(tmp_gs_sid) >= 12:\n","                gs_sid = tmp_gs_sid[:12]\n","        # gs_sid\n","        Researcher['gs_sid'] = gs_sid\n","        # coauthors that are listed at the lower right of the profile page\n","        Researcher[\"coauthors\"] = Co_authors\n","        # citation table\n","        Researcher[\"citation_table\"] = [Citations_table[0], Citations_table[2]]\n","        # time series citations\n","        Researcher[\"cites\"] = {\"years\":years, \"cites\":cites}\n","        # name\n","        nameList = driver.find_elements(By.ID, \"gsc_prf_in\")\n","        if (len(nameList) != 1):\n","            if self.print_true:\n","                print(\"len(nameList)!=1\")\n","            return None\n","        Researcher[\"name\"] = nameList[0].text\n","        # organization\n","        infoList = driver.find_elements(By.CLASS_NAME, 'gsc_prf_il')\n","        Researcher['organization'] = infoList[0].get_attribute('textContent')\n","        # homepage\n","        homepage_url = infoList[1].find_elements(By.TAG_NAME, 'a')\n","        if len(homepage_url) == 0:\n","            Researcher['homepage_url'] = None\n","        else:\n","            Researcher['homepage_url'] = homepage_url[0].get_attribute('href')\n","        # email address\n","        email_str_match = re.search(r'[\\w-]+\\.[\\w.-]+', infoList[1].text)\n","        if email_str_match is not None:\n","            Researcher['email_info'] = email_str_match.group(0)\n","        # domain labels\n","        Researcher['domain_labels'] = [i.get_attribute('textContent').strip().lower() for i in infoList[2].find_elements(By.CLASS_NAME, 'gsc_prf_inta')]\n","        # if not simple, get paper lists\n","        if not simple:\n","            button = driver.find_elements(By.CLASS_NAME, 'gs_btnPD')\n","            if (len(button) != 1):\n","                if self.print_true:\n","                    print(\"len(button)!=1\")\n","                return None\n","            while (button[0].is_enabled()):\n","                while (button[0].is_enabled()):\n","                    while (button[0].is_enabled()):\n","                        button[0].click()\n","                        time.sleep(5)\n","                    time.sleep(1)\n","                time.sleep(2)\n","            papers = []\n","            items = driver.find_elements(By.CLASS_NAME, 'gsc_a_tr')\n","            for i in items:\n","                item = i.find_element(By.CLASS_NAME, 'gsc_a_at')\n","                url = item.get_attribute(\"href\")\n","                paper_info=[j.text for j in i.find_elements(By.CLASS_NAME, 'gs_gray')]\n","                cite = i.find_element(By.CLASS_NAME, 'gsc_a_ac')\n","                year = i.find_element(By.CLASS_NAME, 'gsc_a_y').find_element(By.CLASS_NAME, \"gsc_a_h\").text\n","                papers.append([url, item.text, \n","                                paper_info,\n","                            cite.text, cite.get_attribute(\"href\"),\n","                            year])\n","            Researcher[\"papers\"] = papers\n","\n","        def generate_single_coauthor(element):\n","            coauthor_dict = {\n","                \"name\":element.find_elements(By.CLASS_NAME, 'gs_ai_name')[0].get_attribute('textContent'),\n","                \"url\":element.find_elements(By.CLASS_NAME, 'gs_ai_pho')[0].get_attribute('href'),\n","                \"description\":element.get_attribute('innerHTML'),\n","            }\n","            return coauthor_dict\n","        extra_coauthors = driver.find_elements(By.CLASS_NAME, \"gsc_ucoar\")\n","        Researcher['extra_co_authors'] = [generate_single_coauthor(i) for i in extra_coauthors]\n","        return Researcher\n","\n","    def search_name(self, name: Union[str, list], query_dict: dict = None, top_n=3, simple=True):\n","        \"\"\"Search on Google Scholar webpage given name.\n","        \n","        Parameters\n","        ----------\n","        name : name of the scholar.\n","        query_dict : a dict containing information of the scholar.\n","        top_n : select <top_n> candidates.\n","        simple : whether return simple information without paper list.\n","\n","        Returns\n","        -------\n","        resp : list of candidate scholars, empty if no candidates are found.\n","\n","        \"\"\"\n","        if type(name) is list:\n","            # current case\n","            name_list = [name[0], name[-1]]\n","            name = f'{name[0]} {name[-1]}' \n","        elif type(name) is str:\n","            name_list = name.split(' ')\n","        else:\n","            raise TypeError('Argument \"name\" passed to ScholarGsSearch.search_name has the wrong type.')\n","        url_fragment = f'{name} '\n","        if query_dict is not None:\n","            # first try (name, email_suffix, position, organization) as url\n","            keyword_list = generate_or_keyword_list(query_dict)[0]\n","            url_fragment_new = url_fragment\n","            # if 'email_suffix' in keyword_list:\n","            #     url_fragment_new = url_fragment_new + keyword_list['email_suffix'] + ' '\n","            # if 'position' in keyword_list:\n","            #     url_fragment_new = url_fragment_new + keyword_list['position'] + ' '\n","            # if 'organization' in keyword_list:\n","            #     url_fragment_new = url_fragment_new + keyword_list['organization'] + ' '\n","\n","            # url = self._authsearch.format(url_fragment_new)\n","            # self.driver.get(url)\n","            # time.sleep(5)\n","            # scholar_list = self._search_name_helper(self.driver, name_list)\n","            # if len(scholar_list) > 0:\n","            #     if wo_full:\n","            #         return scholar_list\n","            #     else:\n","            #         return self._search_name_list_expand(scholar_list, simple=simple)\n","            \n","            # second try (name, email_suffix)\n","            if 'email_suffix' in keyword_list:\n","                url_fragment_new = url_fragment + keyword_list['email_suffix'] # + ' '\n","            url = self._authsearch.format(url_fragment_new)\n","            self.driver.get(url)\n","            time.sleep(5)\n","            scholar_list = self._search_name_helper(self.driver, name_list)\n","            # return scholar_list\n","            if len(scholar_list) > 0:\n","                if self.print_true:\n","                    print(f'[Info] Find {len(scholar_list)} scholars using query without gs_sid in step 1.')\n","                # return self._search_name_list_expand(scholar_list, simple=simple)\n","                return scholar_list\n","        \n","            # third try (name, position)\n","            if 'position' in keyword_list:\n","                url_fragment_new = url_fragment + keyword_list['position'] # + ' '\n","            url = self._authsearch.format(url_fragment_new)\n","            self.driver.get(url)\n","            time.sleep(5)\n","            scholar_list = self._search_name_helper(self.driver, name_list)\n","            # return scholar_list\n","            if len(scholar_list) > 0:\n","                if self.print_true:\n","                    print(f'[Info] Find {len(scholar_list)} scholars using query without gs_sid in step 2.')\n","                # return self._search_name_list_expand(scholar_list, simple=simple)\n","                return scholar_list\n","\n","            # fourth try (name, organization)\n","            if 'organization' in keyword_list:\n","                url_fragment_new = url_fragment + keyword_list['organization'] # + ' '\n","            url = self._authsearch.format(url_fragment_new)\n","            self.driver.get(url)\n","            time.sleep(5)\n","            scholar_list = self._search_name_helper(self.driver, name_list)\n","            # return scholar_list\n","            if len(scholar_list) > 0:\n","                if self.print_true:\n","                    print(f'[Info] Find {len(scholar_list)} scholars using query without gs_sid in step 3.')\n","                # return self._search_name_list_expand(scholar_list, simple=simple)\n","                return scholar_list\n","\n","        # finally, only search (name: firstname and lastname). If only one response returns, mark it as candidate\n","        url = self._authsearch.format(url_fragment)\n","        self.driver.get(url)\n","        time.sleep(5)\n","        scholar_list = self._search_name_helper(self.driver, name_list)\n","        if len(scholar_list) > 0:\n","        # if len(scholar_list) > 0 and len(scholar_list) <= top_n:\n","            if self.print_true:\n","                print(f'[Info] Find {len(scholar_list)} scholars using query without gs_sid in step 4.')\n","            # return self._search_name_list_expand(scholar_list, simple=simple)\n","            return scholar_list\n","        \n","        return []\n","\n","    def _search_name_helper(self, driver, name_list):\n","        \"\"\"Helper function of <self.search_name()>.\"\"\"\n","        # iterate over searched list, find dicts that contains the name (including)\n","        useful_info_list = driver.find_elements(By.CLASS_NAME, 'gs_ai_t')\n","        useful_info_ext_list = []\n","        if len(useful_info_list) != 0:\n","            for scholar_webdriver in useful_info_list:\n","                name = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_name').get_attribute('textContent').strip()\n","                # check whether name is correct\n","                not_a_candidate = False\n","                for name_fragment in name_list:\n","                    if name_fragment.lower() not in name.lower():\n","                        not_a_candidate = True\n","                        break\n","                if not_a_candidate:\n","                    continue\n","                \n","                # grab all the other information\n","                pos_org = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_aff').get_attribute('textContent').strip()\n","                email_str = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_eml').get_attribute('textContent').strip()\n","                cite = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_cby').get_attribute('textContent').strip()\n","                url = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_name').find_element(By.TAG_NAME, 'a').get_attribute('href').strip()\n","                domain_labels = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_int').find_elements(By.CLASS_NAME, 'gs_ai_ont_int')\n","                for idx, domain in enumerate(domain_labels):\n","                    domain_labels[idx] = domain.get_attribute('textContent').strip().lower()\n","\n","                # continue processing\n","                gs_sid = None\n","                if 'user=' in url:\n","                    tmp_gs_sid = url.split('user=', 1)[1]\n","                    if len(tmp_gs_sid) >= 12:\n","                        gs_sid = tmp_gs_sid[:12]\n","\n","                if email_str is not None and email_str != '':\n","                    match = re.search(r'[\\w-]+\\.[\\w.-]+', email_str)\n","                    email_str = match.group(0)\n","\n","                cites = [int(s) for s in cite.split() if s.isdigit()]\n","                useful_info_ext_list.append({\n","                    'name': name,\n","                    'pos_org': pos_org,\n","                    'email': email_str,\n","                    'cite': cites[0] if len(cites)>0 else None,\n","                    'url': url,\n","                    'gs_sid': gs_sid,\n","                    'domain_labels': domain_labels\n","                })\n","        return useful_info_ext_list\n","        \n","    def _search_name_list_expand(self, scholar_list, simple=True):\n","        \"\"\"Expand the name_list to full_name_list.\"\"\"\n","        new_scholar_list = []\n","        for scholar in scholar_list:\n","            if 'gs_sid' in scholar:\n","                url = self._gsidsearch.format(scholar['gs_sid'])\n","                self.driver.get(url)\n","                scholar_dict = self._search_gsid_helper(self.driver, url, simple=simple)\n","                if scholar_dict is not None:\n","                    new_scholar_list.append(scholar_dict)\n","                time.sleep(5)\n","        return new_scholar_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-F3mp0N7QIro"},"outputs":[],"source":["from difflib import SequenceMatcher\n","\n","def get_str_similarity(a: str, b: str) -> float:\n","    \"\"\"Calculate the similarity of two strings and return a similarity ratio.\"\"\"\n","    return SequenceMatcher(None, a, b).ratio()"]},{"cell_type":"markdown","metadata":{"id":"43Ozad0mQCKZ"},"source":["### ScholarSearch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DVB-Cz04PNOv"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import pickle\n","import json\n","import typing\n","from typing import List, Union\n","import os\n","import re\n","import time\n","import sys\n","import requests\n","from bs4 import BeautifulSoup\n","\n","\n","class ScholarSearch():\n","    \"\"\"A class that handles searching over Google Scholar profiles and the 78k AI scholar dataset.\"\"\"\n","    def __init__(self):\n","        # attributes\n","        self.similarity_ratio = 0.8\n","        # self.driver_path = '../chromedriver'\n","        self.driver_path = '/usr/bin/chromedriver'\n","    \n","    def setup(self):\n","        # self.get_profiles(['review_data/area_chair_id_to_profile.json', 'review_data/reviewer_id_to_profile.json'])\n","        # self.get_profiles(None)\n","        self.search_78k = Scholar78kSearch()\n","        # print(\"Scholar78kSearch initiated\" )\n","        self.search_gs = ScholarGsSearch(self.driver_path)\n","\n","    def reset(self):\n","        pass\n","\n","    def get_profiles(self, filepath_list: List[str] = None) -> None:\n","        \"\"\"In case that you want to get responses of a list of scholars, \n","        the method is implemented for you to load (could be multiple) json data files.\n","\n","        Parameters\n","        ----------\n","        filepath_list : list of json data filepaths to load.\n","\n","        \"\"\"\n","        if filepath_list is None:\n","            return\n","        # set of json data dicts\n","        self.profile = {}\n","        for filepath in filepath_list:\n","            with open(filepath) as file:\n","                profile = json.load(file)\n","                self.profile.update(profile)\n","        # number of unique json data dicts in total\n","        # print(f'Number of unique json data dicts in total: {len(self.profile)}')\n","\n","    def get_scholar(\n","        self,\n","        query: Union[str, dict],\n","        field: List[str] = None,\n","        simple: bool = True,\n","        top_n: int = 3,\n","        print_true: bool = True) -> List[dict]:\n","        \"\"\"Get up to <top_n> relevant candidate scholars by searching over Google Scholar profiles and the 78k AI scholar dataset.\n","        \n","        Parameters\n","        ----------\n","        query : a query containing the known scholar information.\n","        field : a list of fields wants to return. If not given, by default full information will be returned.\n","        simple : whether return simple information without paper list. This works only if the argument <field> is not specified.\n","        top_n : return at most <top_n> scholars if the result is not considered as determined.\n","        print_true : print info / debug info of the search process.\n","\n","        Returns\n","        -------\n","        resp : list of candidate scholars, empty if no candidates are found.\n","\n","        \"\"\"\n","\n","        self.search_78k.simple = simple\n","        self.search_78k.print_true = print_true\n","        self.search_gs.print_true = print_true\n","        self.print_true = print_true\n","        self.reset()\n","\n","        scholar_cnt = 0\n","        if type(query) is dict:\n","            # query is dict\n","            resp = self.search_dict(query, simple=simple, top_n=top_n)\n","        elif type(query) is str:\n","            # query is str\n","            resp = self.search_name(query, simple=simple, top_n=top_n)                \n","        else:\n","            raise TypeError(f'[Error] The argument \"query\" must be str or dict, not {type(query)}.')\n","\n","        \n","        # select specific features\n","        if field is not None:\n","            resp_final = []\n","            for resp_item in resp:\n","                resp_dict = {}\n","                for field_item in field:\n","                    if field_item not in resp_item:\n","                        raise KeyError(f'The key {field_item} is not in the response dictionary')\n","                    \n","                    resp_dict[field_item] = resp_item[field_item]\n","                resp_dict['gs_sid'] = resp_item['gs_sid']\n","                resp_dict['url'] = resp_item['url']\n","                resp_dict['citation_table'] = resp_item['citation_table']\n","                resp_final.append(resp_dict)\n","            if print_true:\n","                scholar_cnt = len(resp_final)\n","                if scholar_cnt == 1:\n","                    print(f'[Info] In total 1 scholar is found:')\n","                else:\n","                    print(f'[Info] In total {scholar_cnt} scholars are found:')\n","                resp_str = json.dumps(resp_final, indent=2)\n","                print(resp_str)\n","            return resp_final\n","        else:\n","            if print_true:\n","                scholar_cnt = len(resp)\n","                if scholar_cnt == 1:\n","                    print(f'[Info] In total 1 scholar is found:')\n","                else:\n","                    print(f'[Info] In total {scholar_cnt} scholars are found:')\n","                resp_str = json.dumps(resp, indent=2)\n","                print(resp_str)\n","            return resp\n","    \n","    def search_name(self, name: str, simple: bool = True, top_n: int = 3, from_dict: bool = False, query_dict: dict = None) -> List[dict]:\n","        \"\"\"Search gs profile given name or OpenReview id.\n","        \n","        Parameters\n","        ----------\n","        name : the name of the scholar ([first_name last_name]).\n","        simple : whether return simple information without paper list. This works only if the argument <field> is not specified.\n","        top_n : return at most <top_n> scholars if the result is not considered as determined.\n","        from_dict : default = False. Should be true only if using <get_scholar()> class method.\n","        query_dict : default = None. Should be a dict only if using <get_scholar()> class method.\n","\n","        Returns\n","        -------\n","        resp : list of candidate scholars, empty if no candidates are found.\n","        \"\"\"\n","\n","        self.search_78k.simple = simple\n","        name = name.strip()\n","        dict = None\n","        real_name = True\n","        # OpenReview id\n","        if ' ' not in name and name[0] == '~':\n","            # search over chair id\n","            if name in self.profile:\n","                dict = self.profile[name]\n","            # crawl http api response\n","            if dict is not None and not from_dict:\n","                # name\n","                real_name = False\n","                resp = self.search_dict(dict, simple=simple, top_n=top_n)\n","            else:\n","                # get real name\n","                or_name = name # string\n","                name = name[1:].split('_')\n","                name[-1] = re.sub(r'[0-9]+', '', name[-1]) # list\n","                # name = ' '.join(name) # e.g., Rachel K. E. Bellamy\n","        else:\n","            or_name = name.split(' ') # list\n","            # name string\n","        if real_name:\n","            if from_dict:\n","                print('Not find by gs_sid, search from_dict')\n","                # it inputs a real name (firstname, lastname)\n","                resp = self.search_78k.search_name(name, query_dict)\n","                resp_gs = self.search_gs.search_name(name, query_dict, top_n=top_n, simple=simple)\n","                resp = self.select_final_cands(resp, top_n, query_dict=query_dict, resp_gs_prop={'resp_gs': resp_gs})\n","            else:\n","                # or_resp = self.get_or_scholars(or_name)\n","                # TODO: resp_gs for only searching name is not implemented\n","                # resp = self.select_final_cands(resp, or_resp, top_n, simple=simple)\n","                resp = self.search_78k.search_name(name)\n","                resp_gs = self.search_gs.search_name(name, query_dict=None, top_n=top_n, simple=simple)\n","                resp = self.select_final_cands(resp, top_n, query_dict=None, resp_gs_prop={'resp_gs': resp_gs})\n","        return resp\n","    \n","\n","    def get_or_scholars(self, or_name: Union[str, list]):\n","        \"\"\"Get OpenReview candidate scholars list by name through http api response.\"\"\"\n","        # format the name list to get OpenReview rest api response\n","        if type(or_name) is list:\n","            or_name_list = []\n","            if len(or_name) >= 2:\n","                id_list = []\n","                for idx, name_part in enumerate(or_name):\n","                    if idx == 0 or idx == len(or_name) - 1:\n","                        id_list.append(name_part.capitalize())\n","                    else:\n","                        if len(name_part) > 1:\n","                            id_list.append(f'{name_part[0].upper()}.') # middle name in abbreviate form\n","                        else:\n","                            id_list.append(name_part.upper())\n","                if len(id_list) == 2:\n","                    or_name_list.append(f'~{id_list[0]}_{id_list[-1]}')\n","                elif len(id_list) > 2:\n","                    or_name_list.append(f'~{id_list[0]}_{id_list[-1]}')\n","                    tmp_str = '_'.join(id_list)\n","                    or_name_list.append(f'~{tmp_str}')\n","            else:\n","                raise ValueError('Argument \"or_name\" passed to get_or_scholars is not a valid name list.')\n","        elif type(or_name) is str:\n","            or_name_list = [or_name]\n","        else:\n","            raise TypeError(f'Argument \"or_name\" passed to get_or_scholars has the wrong type.')\n","        del or_name\n","\n","        # get request response\n","        go_ahead = True\n","        resp_list = []\n","        for name in or_name_list:\n","            if name[-1].isnumeric():\n","                name_cur = name\n","                go_ahead = False\n","                name_cur_cnt = 1\n","            else:\n","                name_cur_cnt = 1\n","                name_cur = f'{name}{name_cur_cnt}'\n","\n","            # set accumulative count\n","            acc_cnt = 0\n","            while acc_cnt <= 1:\n","                response = requests.get(f'https://openreview.net/profile?id={name_cur}')\n","                time.sleep(1)\n","\n","                if not response.ok:\n","                    acc_cnt += 1\n","                else:\n","                    soup = BeautifulSoup(response.content.decode('utf-8'), 'html.parser')\n","                    resp_list.append(json.loads(soup.find_all('script', id=\"__NEXT_DATA__\")[0].string))\n","                name_cur_cnt += 1\n","                name_cur = f'{name}{name_cur_cnt}'\n","                if not go_ahead:\n","                    break\n","        if self.print_true:\n","            if len(resp_list) != 1:\n","                print(f'[Info] Found {len(resp_list)} scholars using OpenReview REST API.')\n","            else:\n","                print(f'[Info] Found 1 scholar using OpenReview REST API.')\n","        return resp_list \n","        # NOTE: the dict in this list is in a different format than the dict from OpenReview dataset.\n","\n","    def select_final_cands(self, resp: List[dict], top_n: int, query_dict: dict = None, resp_gs_prop: dict = None, simple: bool = True) -> List[dict]:\n","        \"\"\"Select final candidates according to the response from OpenReview and 78k data.\n","        \n","        Parameters\n","        ----------\n","        resp : response from 78k dataset.\n","        or_resp : prepare the necessary key-value pairs to help filtering.\n","        top_n : return at most <top_n> scholars if the result is not considered as determined.\n","        query_dict : default = None. Should be a dict only if using <get_scholar()> class method.\n","        resp_gs_prop : dict containing the response from Google Scholar webpage.\n","        simple : whether return simple information without paper list. This works only if the argument <field> is not specified.\n","\n","        Returns\n","        -------\n","        resp : list of candidate scholars, empty if no candidates are found.\n","        \n","        \"\"\"\n","        # get useful data from or_resp\n","        if query_dict is not None:\n","            or_keyword_list = generate_or_keyword_list(query_dict)\n","\n","        # merge resp with resp_gs\n","        if resp_gs_prop is not None:\n","            resp_gs = resp_gs_prop['resp_gs']\n","            # if there are one candidate from google scholar pages, we throw out resp from 78k data.\n","            if len(resp_gs) == 1:\n","                resp = []\n","            # iterate over resp_gs\n","            for resp_gs_item in resp_gs:\n","                find_flag = False\n","                # gs_sid\n","                for resp_item in resp:\n","                    if resp_gs_item['gs_sid'] == resp_item['gs_sid']:\n","                        find_flag = True\n","                        break\n","                if find_flag:\n","                    continue\n","                # construct new prep\n","                # generate full dict\n","                self.search_gs.driver.get(resp_gs_item['url'])\n","                time.sleep(5)\n","                if query_dict is not None or (query_dict is None and len(resp) <= top_n):\n","                    resp_gs_full_item = self.search_gs._search_gsid_helper(self.search_gs.driver, resp_gs_item['url'], simple=simple)\n","                    if resp_gs_full_item is not None:\n","                        resp.append(resp_gs_full_item)\n","        \n","        if query_dict is None:\n","            return resp[:top_n]\n","\n","        # calculate rankings\n","        rank = {}\n","        for idx_cand, cand in enumerate(resp):\n","            rank[idx_cand] = []\n","            gs_sid_flag = 0\n","            cnt_true = [0] * len(or_keyword_list) \n","            cnt_all = 0\n","            cnt_true_rel = [0] * len(or_keyword_list) \n","            cnt_all_rel = 0\n","            for idx_or_scholar, or_scholar in enumerate(or_keyword_list):\n","                # gs_sid\n","                if 'gs_sid' in cand:\n","                    if cand['gs_sid'] == or_scholar['gs_sid']: \n","                        gs_sid_flag = 1\n","\n","                # domain_labels\n","                if cand['domain_labels'] is not None:\n","                    for cand_domain_tag in cand['domain_labels']:\n","                        cnt_all += 1\n","                        for or_domain_tag in or_scholar['domain_labels']:\n","                            if get_str_similarity(cand_domain_tag, or_domain_tag) >= self.similarity_ratio:\n","                                cnt_true[idx_or_scholar] += 1\n","                \n","                \n","                # relations\n","                cnt_all_rel = 0\n","                # print(cand)\n","                if cand['coauthors'] is not None:\n","                    for cand_coauth in cand['coauthors']:\n","                        cnt_all_rel += 1\n","                        for or_coauth in or_scholar['coauthors']:\n","                            if get_str_similarity(or_coauth, cand_coauth[1]) >= self.similarity_ratio:\n","                                cnt_true_rel[idx_or_scholar] += 1\n","                \n","            # get the rank list\n","            # gs_sid\n","            if gs_sid_flag:\n","                rank[idx_cand].append(1)\n","            else:\n","                rank[idx_cand].append(0)\n","            \n","            # domain_labels\n","            for i in range(len(cnt_true)):\n","                if cnt_all == 0:\n","                    cnt_true[i] = 0\n","                else:\n","                    cnt_true[i] = cnt_true[i] / cnt_all\n","            rank[idx_cand].append(max(cnt_true))\n","\n","            # relations\n","            for i in range(len(cnt_true_rel)):\n","                if cnt_all_rel == 0:\n","                    cnt_true_rel[i] = 0\n","                else:\n","                    cnt_true_rel[i] = cnt_true_rel[i] / cnt_all_rel\n","            rank[idx_cand].append(max(cnt_true_rel))\n","        \n","        # select final candidate\n","        final_idx = []\n","        for rank_idx in rank:\n","            if rank[rank_idx][0] == 1:\n","                final_idx.append(rank_idx)\n","        \n","        # TODO: or we can set weights to (relations, domain_tags) to rank the scholar candidates\n","        if len(final_idx) < top_n:\n","            domain_tag_rank = []\n","            relation_rank = []\n","            for rank_idx in sorted(rank.keys()):\n","                # print(rank_idx)\n","                domain_tag_rank.append(rank[rank_idx][1])\n","                relation_rank.append(rank[rank_idx][2])\n","            # print(domain_tag_rank, relation_rank)\n","            domain_tag_idxes = np.argsort(domain_tag_rank)[::-1]\n","            relation_idxes = np.argsort(relation_rank)[::-1]\n","            for idx in relation_idxes:\n","                if relation_rank[idx] == 0:\n","                    break\n","                if len(final_idx) < top_n:\n","                    if idx not in final_idx:\n","                        final_idx.append(idx)\n","                else:\n","                    break\n","            for idx in domain_tag_idxes:\n","                if domain_tag_rank[idx] == 0:\n","                    break\n","                if len(final_idx) < top_n:\n","                    if idx not in final_idx:\n","                        final_idx.append(idx)\n","                else:\n","                    break\n","            if len(final_idx) == 0 and len(rank.keys()) > 0:\n","                    for rank_idx in sorted(rank.keys()):\n","                        if len(final_idx) >= top_n:\n","                            break\n","                        else:\n","                            final_idx.append(rank_idx)\n","        # print(resp)\n","        # print(or_keyword_list)\n","        # print(rank)\n","        # print(final_idx)\n","        resp = [resp[i] for i in final_idx]\n","        return resp\n","\n","    def search_dict(self, query_dict: dict, simple: bool = True, top_n: int = 3):\n","        \"\"\"Search candidates given a dictionary.\n","        \n","        Parameters\n","        ----------\n","        query_dict : default = None. Should be a dict only if using <get_scholar()> class method.\n","        simple : whether return simple information without paper list. This works only if the argument <field> is not specified.\n","        top_n : return at most <top_n> scholars if the result is not considered as determined.\n","\n","        Returns\n","        -------\n","        resp : list of candidate scholars, empty if no candidates are found.\n","\n","        \"\"\"\n","        self.search_78k.simple = simple\n","        # gs_sid\n","        if 'gscholar' in query_dict['profile']['content'] and 'user=' in query_dict['profile']['content']['gscholar']:\n","            tmp_gs_sid = query_dict['profile']['content']['gscholar'].split('user=', 1)[1]\n","            if len(tmp_gs_sid) >= 12:\n","                gs_sid = tmp_gs_sid[:12]\n","                name_df = self.search_78k.df.loc[self.search_78k.df['gs_sid'] == gs_sid].copy()\n","                if name_df.shape[0] != 0:\n","                    print(f'[Info] Found a scholar using 78k gs_sid')\n","                    return self.search_78k._deal_with_simple(name_df)\n","                else:\n","                    print(f'[Info] Found a scholar using query dict gs_sid')\n","                    resp = self.search_gs.search_gsid(gs_sid, simple=simple)\n","                    if len(resp) > 0:\n","                        return resp\n","                    \n","        \n","        # search_name\n","        return self.search_name(query_dict['profile']['id'], simple=simple, top_n=top_n, from_dict=True, query_dict=query_dict)"]},{"cell_type":"markdown","metadata":{"id":"0GF_xG8UVOK6"},"source":["### TwitterSearch - edited\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"237sT5XKVK7Z"},"outputs":[],"source":["import re\n","import time\n","from typing import Union\n","from collections import defaultdict\n","from selenium import webdriver\n","from selenium.webdriver.chromium.webdriver import ChromiumDriver\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.remote.errorhandler import WebDriverException\n","from bs4 import BeautifulSoup\n","\n","url_search_dict = {\n","    'google': 'https://www.google.com/search?q={0}'\n","}\n","\n","\n","def get_search_result(**kwargs):\n","    return kwargs\n","\n","\n","class TwitterSearch(GoogleSearch):\n","    \"\"\"Class that handling searching on Google search bar using REST API.\"\"\"\n","\n","    def __init__(self, driver_path):\n","        super().__init__(driver_path)\n","        # print(\"GoogleSearch initiated\")\n","        self._urlsearch = url_search_dict['google']\n","        self.print_true = False #NOTE: should be set by users\n","        # initialize scholar_search object\n","        self.scholar_search = ScholarSearch()\n","        # print(\"ScholarSearch initiated\")\n","        self.scholar_search.setup()\n","        # print(\"setup done\")\n","\n","    def search_scholar(self, str_type: str, term: str):\n","        \"\"\"\n","        NOTE: now the only allowed str_type is name\n","        \"\"\"\n","        if str_type == 'name':\n","            scholar_search_result = self.scholar_search.get_scholar(query=term, simple=True, top_n=1, print_true=False)\n","            branch_type = \"\"\n","            if len(scholar_search_result) == 0:\n","                # print(\"directly searching\")\n","                # directly search\n","                url_fragment = self._urlsearch.format(f'{term} \"twitter\"')\n","                result = self._search_google_helper(url_fragment)\n","                twitter_ids = self.filter_result(result, term, web_source='google')\n","                # print(twitter_ids)\n","                branch_type += 'directly search,'\n","            else:\n","                # twitter_ids = None\n","                twitter_ids = []\n","                len_twitter_ids = 0\n","                # try directly get twitter account through homepage\n","                # print(\"searching homepage\")\n","                if 'homepage_url' in scholar_search_result[0] and scholar_search_result[0]['homepage_url'] is not None:\n","                    new_ids = self._search_twitter_from_homepage(scholar_search_result[0]['homepage_url'], name=term, name_from_gs=scholar_search_result[0][\"name\"])\n","                    if new_ids:\n","                      twitter_ids.extend(new_ids)\n","                # print(twitter_ids)\n","                if len(twitter_ids)>len_twitter_ids:\n","                    branch_type += 'homepage,'\n","                    len_twitter_ids = len(twitter_ids)\n","\n","                # then try (google_name email_suffix \"twitter\")\n","                # print(\"searching name + email\")\n","                if \"email_info\" in scholar_search_result[0] and scholar_search_result[0][\"email_info\"] != '':# and twitter_ids is None:\n","                    url_fragment = self._urlsearch.format(f'{scholar_search_result[0][\"name\"]} {scholar_search_result[0][\"email_info\"]} \"twitter\"')\n","                    result = self._search_google_helper(url_fragment)\n","                    new_ids = self.filter_result(result, term, web_source='google')\n","                    if new_ids:\n","                      twitter_ids.extend(new_ids)\n","                # print(twitter_ids)\n","                if len(twitter_ids)>len_twitter_ids:\n","                    branch_type += 'name + email,'\n","                    len_twitter_ids = len(twitter_ids)\n","\n","                # then try (google_name organization \"twitter\")\n","                # print(\"searching name + organization\")\n","                if \"organization\" in scholar_search_result[0] and scholar_search_result[0][\"organization\"] != '':# and twitter_ids is None:\n","                    url_fragment = self._urlsearch.format(f'{scholar_search_result[0][\"name\"]} {scholar_search_result[0][\"organization\"]} \"twitter\"')\n","                    result = self._search_google_helper(url_fragment)\n","                    new_ids = self.filter_result(result, term, web_source='google')\n","                    if new_ids:\n","                      twitter_ids.extend(new_ids)\n","                # print(twitter_ids)\n","                if len(twitter_ids)>len_twitter_ids:\n","                    branch_type += 'name + organization,'\n","                    len_twitter_ids = len(twitter_ids)\n","\n","                # then try (google_name \"twitter\")\n","                # print(\"searching name \\\"twitter\\\"\")\n","                # if len(twitter_ids):\n","                # TODO - preprocess google_name to include only tokens which have length greater than one\n","                # gs_name = re.sub('[0-9_\\.,]', '', scholar_search_result[0][\"name\"])\n","                url_fragment = self._urlsearch.format(f'{scholar_search_result[0][\"name\"]} \"twitter\"')\n","                result = self._search_google_helper(url_fragment)\n","                new_ids = self.filter_result(result, term, web_source='google')\n","                if new_ids:\n","                      twitter_ids.extend(new_ids)\n","                # print(twitter_ids)\n","                if len(twitter_ids)>len_twitter_ids:\n","                    branch_type += 'name'\n","                    len_twitter_ids = len(twitter_ids)\n","            twitter_ids = list(set(twitter_ids))\n","\n","            # if self.print_true:\n","            print(f'[INFO] branch_type: {branch_type}')\n","            print(f'[INFO] twitter_ids: {twitter_ids}')\n","\n","            if twitter_ids is None or len(twitter_ids) == 0:\n","                return []\n","\n","            # print(\"ranking by similarity\")\n","            twitter_ids = self._rank_by_similarity(twitter_ids, name=term, name_from_gs=scholar_search_result[0][\"name\"])\n","            # print(twitter_ids)\n","            return twitter_ids[:10] if len(twitter_ids)>=10 else twitter_ids\n","            # # elif type(twitter_ids) == dict:\n","            # twitter_ids_list = list(twitter_ids.keys())\n","            # # print(\"twitter_ids_list\", twitter_ids_list)\n","            # highest_occurrence = twitter_ids[twitter_ids_list[0]]\n","            # # print(\"highest_occurrence\", highest_occurrence)\n","            # candidate_list = []\n","            # for item in twitter_ids.items():\n","            #     # print(\"item[1] == highest_occurrence ?\", item, item[0], item[1], highest_occurrence)\n","            #     if item[1] == highest_occurrence:\n","            #         similarity_rank = self._rank_by_similarity(item[0], term, scholar_search_result[0][\"name\"])\n","            #         # print(\"similarity_rank\", item[0], term, scholar_search_result[0][\"name\"], similarity_rank)\n","            #         if similarity_rank >= 0.15:\n","            #         # if self._rank_by_similarity(item[0], term, scholar_search_result[0][\"name\"]) >= 0.15:\n","            #             candidate_list.append(item[0])\n","            # # print(\"candidate_list\", candidate_list)\n","            # if len(candidate_list) > 0:\n","            #     return self._rank_by_similarity(candidate_list, term, scholar_search_result[0]['name'])[0]\n","            # return None\n","            # else:\n","            #     return twitter_ids[0]\n","\n","            # TODO: match profile images of google_scholar/homepage with twitter profile images\n","            # result = self._search_name_helper(term)\n","            # return self.filter_result(result, term, web_source='google')\n","        elif str_type == 'gs_url':\n","            raise NotImplementedError\n","        else:\n","            raise NotImplementedError\n","\n","    def _rank_by_similarity(self, twitter_url_origin_list: Union[list, str], name: str=None, name_from_gs: str=None):\n","        # process name and name_from_gs\n","        if name is not None:\n","            name = re.sub('[0-9_\\., ]', '', name.lower())\n","        if name_from_gs is not None:\n","            name_from_gs = re.sub('[0-9_\\., ]', '', name_from_gs.lower())\n","\n","        if type(twitter_url_origin_list) == list:\n","            \n","            # else\n","            twitter_url_list = [re.sub('[0-9_\\., ]', '', item) for item in twitter_url_origin_list]\n","            twitter_url_map_dict = {re.sub('[0-9_\\., ]', '', item): item for item in twitter_url_origin_list}\n","            # rank twitter_url_origin_list\n","            if name is not None and name_from_gs is not None:\n","                twitter_url_list = sorted(twitter_url_list, key=lambda x: max(get_str_similarity(x, name), get_str_similarity(x, name_from_gs)), reverse=True)\n","            elif name is not None:\n","                twitter_url_list = sorted(twitter_url_list, key=lambda x: get_str_similarity(x, name), reverse=True)\n","            elif name_from_gs is not None:\n","                twitter_url_list = sorted(twitter_url_list, key=lambda x: get_str_similarity(x, name_from_gs), reverse=True)\n","            else:\n","                # do not consider this branch at the moment\n","                pass\n","            twitter_url_origin_list = [twitter_url_map_dict[item] for item in twitter_url_list]\n","            # print(\"twitter_url_origin_list\", twitter_url_origin_list)\n","            return twitter_url_origin_list\n","        else:\n","            twitter_url_origin_str = twitter_url_origin_list\n","            twitter_url_str = re.sub('[0-9_\\., ]', '', twitter_url_origin_str)\n","            rank = 0\n","            if name is not None and name_from_gs is not None:\n","                rank = max(get_str_similarity(twitter_url_str, name), get_str_similarity(twitter_url_origin_list, name_from_gs))\n","            elif name is not None:\n","                rank = get_str_similarity(twitter_url_str, name)\n","            elif name_from_gs is not None:\n","                rank = get_str_similarity(twitter_url_str, name_from_gs)\n","            \n","            return rank\n","\n","    def _search_twitter_from_homepage(self, homepage_url: str, name: str=None, name_from_gs: str=None):\n","        # get content of scholar homepage using chromedriver\n","        try:\n","            self.driver.implicitly_wait(5)  \n","            self.driver.get(homepage_url)\n","        except WebDriverException as e:\n","            if self.print_true:\n","                print('[DEBUG] WebDriverException while getting homepage: %s' % homepage_url)\n","                print(e)\n","        time.sleep(8)\n","\n","        page = self.driver.page_source\n","        soup = BeautifulSoup(page, \"html.parser\")\n","        twitter_url_origin_list = list(set([re.findall('twitter.com/([^\\/?]+)', item['href'])[0]\n","            for item in soup.find_all(\n","                href=re.compile('twitter.com/([^\\/?]+)'))]))\n","        print(soup.find_all(\n","                href=re.compile('twitter.com/([^\\/?]+)')))\n","        # if there are no candidates for twitter account url, return None\n","        if len(twitter_url_origin_list) == 0:\n","            # return soup\n","            return None\n","\n","        twitter_url_origin_list = self._rank_by_similarity(twitter_url_origin_list, name=name, name_from_gs=name_from_gs)\n","\n","        if self.print_true:\n","            print(f'[DEBUG] Find a set of twitter ids on the provided homepage:\\n{twitter_url_origin_list}')\n","        \n","        # only return the highest rank twitter account id\n","        return twitter_url_origin_list\n","\n","    def _search_google_helper(self, google_url: str):\n","        self.driver.implicitly_wait(5)  \n","        self.driver.get(google_url)\n","        time.sleep(8)\n","\n","        page = self.driver.page_source\n","        soup = BeautifulSoup(page, \"html.parser\")\n","\n","        result_list = []\n","        result_block = soup.find_all('div', attrs={'class': 'g'})\n","        for result in result_block:\n","            # Find link, title, description\n","            link = result.find('a', href=True)\n","            title = result.find('h3')\n","            description_box = result.find(\n","                'div', {'style': '-webkit-line-clamp:2'})\n","            if link and title and description_box:\n","                result_list.append(get_search_result(\n","                    href=link['href'], title=title.text, description=description_box.text))\n","        if self.print_true:\n","            print(result_list)\n","        time.sleep(5)\n","        return result_list\n","        \n","\n","    def filter_result(self, result_list, term, web_source):\n","        \"\"\"\n","        web_source: google, twitter\n","        \"\"\"\n","        # sort twitter ids by occurrence frequency\n","        if web_source == 'google':\n","            twitter_id_dict = defaultdict(int)\n","            for result in result_list:\n","                if 'twitter.com/' in result['href']:\n","                    twitter_id_dict[re.findall('twitter.com/([^\\/?]+)', result['href'])[0]] += 1\n","        twitter_id_dict = dict(sorted(twitter_id_dict.items(), key=lambda item: item[1], reverse=True))\n","        # then, sort twitter ids by str similarity?\n","        # TODO\n","        # TODO: enter into twitter page to check profile information\n","        # Step 1: twitter profile vs google scholar profile\n","        # Step 2: twitter tweets: check whether google scholar domains are in twitter tweets\n","        # Step 3: twitter profile image (ask Yvonne about the performance)\n","        \n","        if len(twitter_id_dict) == 0:\n","            return None\n","        else:\n","            return twitter_id_dict\n","    \n","    def search_scholar_batch(self, name_list: list):\n","        # self.result_list = []\n","        with jsonlines.open(\"/content/drive/My Drive/tweets-dataset/gs_id2twitter_id_500k.csv\", 'a') as writer:\n","          for i, name in enumerate(name_list):\n","            t_id_name = self.search_scholar('name', name)\n","            writer.write((i, name, t_id_name))\n","            # self.result_list.append(t_id_name)\n","        # return self.result_list\n","\n","    def get_scholar_twitter(self, str_type: str, term: str, only_one=True):\n","        \"\"\"\n","        Final function that search a scholar's twitter account\n","        # TODO\n","        \"\"\"\n","        raise NotImplementedError\n","        result = self.search_scholar(str_type=str_type, term=term)\n","\n","        # first, google web search: name \"twitter\", get a list of top results, and check whatever name matches exactly\n","        # if matches, then get the twitter account id, use tweepy API search of the id to get the user profile and do further check\n","\n","        # if no matches, then search by name directly using Tweepy\n","\n","        # if there are candidates, do type 1, 2, 3 check of the result\n","        #\n","\n","        # '''\n","        #     The current code and the data is on the folder /cluster/project/sachan/zhiheng/twiteer at Euler server, because of the security reason, I save the twitter key as this structure, and use get_auth.py to load the key in the file. If you need more APIs, please contact me\n","        #     {\n","        #     \"API_key\" : \"ilH6jnBJdh9HQdsufmygvUwMB\",\n","        #     \"API_secret_key\" : \"LqErCdWfdP6BWf3LH3Q0RrJAXHoFvmweBUNtI1WljJ2A8SMelW\"\n","        #     }\n","        #     The current algorithm has the follow steps\n","        #     Step1: Find all twitter’s screen name by simply search the GS_name on twitter save_twitter_metainfo.py\n","        #     The problem now is that simply search the GS_name have a low recall rate, which is seen as the current bottleneck, about 52% of valid user loose in this step (see the below information)\n","        #     Step2: Use match_and_save.py to make a sketch match by the type 1, 2, 3 match and save those users tweets\n","        #     type 1: matched by personal website\n","        #     type 2: matched by keyword\n","        #     type3: matched by similar description with the information in GS\n","        #     Step3: Process the tweets (not important in current step)\n","        #     For the current 400 datapoint, there are 136 valid twitter accounts. I can match 20 of them by personal website, 36 of them by using type 1,2,3 match(with FN=20), and only 66 of them appeared in our search by users name (for example, if I simply search \"Mohammad Moradi\", I can not find the correspondent user moradideli by https://twitter.com/search?q=Mohammad%20Moradi&src=typed_query&f=user).\n","        #     8:45\n","        #     Here is some useful info about how a person annotator find the ground truth twitter user:\n","        #     8:46\n","        #     I followed the instruction in this doc by searching the name + Twitter in the Google first, and click top results to see if there is any match. If none, I will go search the name in Twitter and also browse through the top results. Sometimes I will also search their LinkedIn page to get their most up-to-date information. (the current institute in Google Scholar is not as accurate as their LinkedIn, and LinkedIn has a full history of where they worked. Moreover, they tend to put their photos in LinkedIn)\n","        # '''\n","\n","\n","# literature:\n","\n","# https://direct.mit.edu/qss/article/1/2/771/96149/Large-scale-identification-and-characterization-of"]},{"cell_type":"markdown","metadata":{"id":"6mK1zSXBe6NK"},"source":["### TwitterSearch-original"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"75S61xAce8dw"},"outputs":[],"source":["# import re\n","# import time\n","# from typing import Union\n","# from collections import defaultdict\n","# from selenium import webdriver\n","# from selenium.webdriver.chromium.webdriver import ChromiumDriver\n","# from selenium.webdriver.common.by import By\n","# from selenium.webdriver.remote.errorhandler import WebDriverException\n","# from bs4 import BeautifulSoup\n","\n","# url_search_dict = {\n","#     'google': 'https://www.google.com/search?q={0}'\n","# }\n","\n","\n","# def get_search_result(**kwargs):\n","#     return kwargs\n","\n","\n","# class TwitterSearch(GoogleSearch):\n","#     \"\"\"Class that handling searching on Google search bar using REST API.\"\"\"\n","\n","#     def __init__(self, driver_path):\n","#         super().__init__(driver_path)\n","#         # print(\"GoogleSearch initiated\")\n","#         self._urlsearch = url_search_dict['google']\n","#         self.print_true = False #NOTE: should be set by users\n","#         # initialize scholar_search object\n","#         self.scholar_search = ScholarSearch()\n","#         self.scholar_search.setup()\n","\n","#     def search_scholar(self, scholar_search_result, term: str):\n","#         \"\"\"\n","#         NOTE: now the only allowed str_type is name\n","#         \"\"\"\n","#         # twitter_ids = None\n","#         twitter_ids = []\n","#         len_twitter_ids = 0\n","#         # try directly get twitter account through homepage\n","#         # print(\"searching homepage\")\n","#         if scholar_search_result['homepage_url'] is not None:\n","#             new_ids = self._search_twitter_from_homepage(['homepage_url'], name=term, name_from_gs=scholar_search_result[\"name\"])\n","#             if new_ids:\n","#               twitter_ids.extend(new_ids)\n","#         # print(twitter_ids)\n","#         if len(twitter_ids)>len_twitter_ids:\n","#             branch_type += 'homepage,'\n","#             len_twitter_ids = len(twitter_ids)\n","\n","#         # then try (google_name email_suffix \"twitter\")\n","#         # print(\"searching name + email\")\n","#         if scholar_search_result[\"email_info\"] != '':# and twitter_ids is None:\n","#             url_fragment = self._urlsearch.format(f'{scholar_search_result[\"name\"]} {scholar_search_result[\"email_info\"]} \"twitter\"')\n","#             result = self._search_google_helper(url_fragment)\n","#             new_ids = self.filter_result(result, term, web_source='google')\n","#             if new_ids:\n","#               twitter_ids.extend(new_ids)\n","#         # print(twitter_ids)\n","#         if len(twitter_ids)>len_twitter_ids:\n","#             branch_type += 'name + email,'\n","#             len_twitter_ids = len(twitter_ids)\n","\n","#         # then try (google_name organization \"twitter\")\n","#         # print(\"searching name + organization\")\n","#         if scholar_search_result[\"organization\"] != '':# and twitter_ids is None:\n","#             url_fragment = self._urlsearch.format(f'{scholar_search_result[\"name\"]} {scholar_search_result[\"organization\"]} \"twitter\"')\n","#             result = self._search_google_helper(url_fragment)\n","#             new_ids = self.filter_result(result, term, web_source='google')\n","#             if new_ids:\n","#               twitter_ids.extend(new_ids)\n","#         # print(twitter_ids)\n","#         if len(twitter_ids)>len_twitter_ids:\n","#             branch_type += 'name + organization,'\n","#             len_twitter_ids = len(twitter_ids)\n","\n","#         # then try (google_name \"twitter\")\n","#         # print(\"searching name \\\"twitter\\\"\")\n","#         # if len(twitter_ids):\n","#         # TODO - preprocess google_name to include only tokens which have length greater than one\n","#         # gs_name = re.sub('[0-9_\\.,]', '', scholar_search_result[0][\"name\"])\n","#         url_fragment = self._urlsearch.format(f'{scholar_search_result[\"name\"]} \"twitter\"')\n","#         result = self._search_google_helper(url_fragment)\n","#         new_ids = self.filter_result(result, term, web_source='google')\n","#         if new_ids:\n","#               twitter_ids.extend(new_ids)\n","#         # print(twitter_ids)\n","#         if len(twitter_ids)>len_twitter_ids:\n","#             branch_type += 'name'\n","#             len_twitter_ids = len(twitter_ids)\n","#         twitter_ids = list(set(twitter_ids))\n","\n","#         # if self.print_true:\n","#         print(f'[INFO] branch_type: {branch_type}')\n","#         print(f'[INFO] twitter_ids: {twitter_ids}')\n","\n","#         if twitter_ids is None or len(twitter_ids) == 0:\n","#             return []\n","\n","#         twitter_ids = self._rank_by_similarity(twitter_ids, name=term, name_from_gs=scholar_search_result[0][\"name\"])\n","#         return twitter_ids[:10] if len(twitter_ids)>=10 else twitter_ids\n","\n","#     def _rank_by_similarity(self, twitter_url_origin_list: Union[list, str], name: str=None, name_from_gs: str=None):\n","#         # process name and name_from_gs\n","#         if name is not None:\n","#             name = re.sub('[0-9_\\., ]', '', name.lower())\n","#         if name_from_gs is not None:\n","#             name_from_gs = re.sub('[0-9_\\., ]', '', name_from_gs.lower())\n","\n","#         if type(twitter_url_origin_list) == list:\n","            \n","#             # else\n","#             twitter_url_list = [re.sub('[0-9_\\., ]', '', item) for item in twitter_url_origin_list]\n","#             twitter_url_map_dict = {re.sub('[0-9_\\., ]', '', item): item for item in twitter_url_origin_list}\n","#             # rank twitter_url_origin_list\n","#             if name is not None and name_from_gs is not None:\n","#                 twitter_url_list = sorted(twitter_url_list, key=lambda x: max(get_str_similarity(x, name), get_str_similarity(x, name_from_gs)), reverse=True)\n","#             elif name is not None:\n","#                 twitter_url_list = sorted(twitter_url_list, key=lambda x: get_str_similarity(x, name), reverse=True)\n","#             elif name_from_gs is not None:\n","#                 twitter_url_list = sorted(twitter_url_list, key=lambda x: get_str_similarity(x, name_from_gs), reverse=True)\n","#             else:\n","#                 # do not consider this branch at the moment\n","#                 pass\n","#             twitter_url_origin_list = [twitter_url_map_dict[item] for item in twitter_url_list]\n","#             # print(\"twitter_url_origin_list\", twitter_url_origin_list)\n","#             return twitter_url_origin_list\n","#         else:\n","#             twitter_url_origin_str = twitter_url_origin_list\n","#             twitter_url_str = re.sub('[0-9_\\., ]', '', twitter_url_origin_str)\n","#             rank = 0\n","#             if name is not None and name_from_gs is not None:\n","#                 rank = max(get_str_similarity(twitter_url_str, name), get_str_similarity(twitter_url_origin_list, name_from_gs))\n","#             elif name is not None:\n","#                 rank = get_str_similarity(twitter_url_str, name)\n","#             elif name_from_gs is not None:\n","#                 rank = get_str_similarity(twitter_url_str, name_from_gs)\n","            \n","#             return rank\n","\n","#     def _search_twitter_from_homepage(self, homepage_url: str, name: str=None, name_from_gs: str=None):\n","#         # get content of scholar homepage using chromedriver\n","#         try:\n","#             self.driver.get(homepage_url)\n","#         except WebDriverException as e:\n","#             if self.print_true:\n","#                 print('[DEBUG] WebDriverException while getting homepage: %s' % homepage_url)\n","#                 print(e)\n","#         time.sleep(3)\n","\n","#         page = self.driver.page_source\n","#         soup = BeautifulSoup(page, \"html.parser\")\n","#         twitter_url_origin_list = list(set([re.findall('twitter.com/([^\\/?]+)', item['href'])[0]\n","#             for item in soup.find_all(\n","#                 href=re.compile('twitter.com/([^\\/?]+)'))]))\n","#         print(soup.find_all(\n","#                 href=re.compile('twitter.com/([^\\/?]+)')))\n","#         # if there are no candidates for twitter account url, return None\n","#         if len(twitter_url_origin_list) == 0:\n","#             # return soup\n","#             return None\n","\n","#         twitter_url_origin_list = self._rank_by_similarity(twitter_url_origin_list, name=name, name_from_gs=name_from_gs)\n","\n","#         if self.print_true:\n","#             print(f'[DEBUG] Find a set of twitter ids on the provided homepage:\\n{twitter_url_origin_list}')\n","        \n","#         # only return the highest rank twitter account id\n","#         return twitter_url_origin_list\n","\n","#     def _search_google_helper(self, google_url: str):\n","#         self.driver.get(google_url)\n","#         time.sleep(3)\n","\n","#         page = self.driver.page_source\n","#         soup = BeautifulSoup(page, \"html.parser\")\n","\n","#         result_list = []\n","#         result_block = soup.find_all('div', attrs={'class': 'g'})\n","#         for result in result_block:\n","#             # Find link, title, description\n","#             link = result.find('a', href=True)\n","#             title = result.find('h3')\n","#             description_box = result.find(\n","#                 'div', {'style': '-webkit-line-clamp:2'})\n","#             if link and title and description_box:\n","#                 result_list.append(get_search_result(\n","#                     href=link['href'], title=title.text, description=description_box.text))\n","#         if self.print_true:\n","#             print(result_list)\n","#         time.sleep(5)\n","#         return result_list\n","        \n","\n","#     def filter_result(self, result_list, term, web_source):\n","#         \"\"\"\n","#         web_source: google, twitter\n","#         \"\"\"\n","#         # sort twitter ids by occurrence frequency\n","#         if web_source == 'google':\n","#             twitter_id_dict = defaultdict(int)\n","#             for result in result_list:\n","#                 if 'twitter.com/' in result['href']:\n","#                     twitter_id_dict[re.findall('twitter.com/([^\\/?]+)', result['href'])[0]] += 1\n","#         twitter_id_dict = dict(sorted(twitter_id_dict.items(), key=lambda item: item[1], reverse=True))\n","#         # then, sort twitter ids by str similarity?\n","#         # TODO\n","#         # TODO: enter into twitter page to check profile information\n","#         # Step 1: twitter profile vs google scholar profile\n","#         # Step 2: twitter tweets: check whether google scholar domains are in twitter tweets\n","#         # Step 3: twitter profile image (ask Yvonne about the performance)\n","        \n","#         if len(twitter_id_dict) == 0:\n","#             return None\n","#         else:\n","#             return twitter_id_dict\n","    \n","#     def search_scholar_batch(self, name_list: list):\n","#         # self.result_list = []\n","#         with jsonlines.open(\"/content/drive/My Drive/tweets-dataset/gs_id2twitter_id_500k.csv\", 'a') as writer:\n","#           for i, name in enumerate(name_list):\n","#             t_id_name = self.search_scholar('name', name)\n","#             writer.write((i, name, t_id_name))\n","#             # self.result_list.append(t_id_name)\n","#         # return self.result_list\n","\n","#     def get_scholar_twitter(self, str_type: str, term: str, only_one=True):\n","#         \"\"\"\n","#         Final function that search a scholar's twitter account\n","#         # TODO\n","#         \"\"\"\n","#         raise NotImplementedError\n","#         result = self.search_scholar(str_type=str_type, term=term)\n","\n","#         # first, google web search: name \"twitter\", get a list of top results, and check whatever name matches exactly\n","#         # if matches, then get the twitter account id, use tweepy API search of the id to get the user profile and do further check\n","\n","#         # if no matches, then search by name directly using Tweepy\n","\n","#         # if there are candidates, do type 1, 2, 3 check of the result\n","#         #\n","\n","#         # '''\n","#         #     The current code and the data is on the folder /cluster/project/sachan/zhiheng/twiteer at Euler server, because of the security reason, I save the twitter key as this structure, and use get_auth.py to load the key in the file. If you need more APIs, please contact me\n","#         #     {\n","#         #     \"API_key\" : \"ilH6jnBJdh9HQdsufmygvUwMB\",\n","#         #     \"API_secret_key\" : \"LqErCdWfdP6BWf3LH3Q0RrJAXHoFvmweBUNtI1WljJ2A8SMelW\"\n","#         #     }\n","#         #     The current algorithm has the follow steps\n","#         #     Step1: Find all twitter’s screen name by simply search the GS_name on twitter save_twitter_metainfo.py\n","#         #     The problem now is that simply search the GS_name have a low recall rate, which is seen as the current bottleneck, about 52% of valid user loose in this step (see the below information)\n","#         #     Step2: Use match_and_save.py to make a sketch match by the type 1, 2, 3 match and save those users tweets\n","#         #     type 1: matched by personal website\n","#         #     type 2: matched by keyword\n","#         #     type3: matched by similar description with the information in GS\n","#         #     Step3: Process the tweets (not important in current step)\n","#         #     For the current 400 datapoint, there are 136 valid twitter accounts. I can match 20 of them by personal website, 36 of them by using type 1,2,3 match(with FN=20), and only 66 of them appeared in our search by users name (for example, if I simply search \"Mohammad Moradi\", I can not find the correspondent user moradideli by https://twitter.com/search?q=Mohammad%20Moradi&src=typed_query&f=user).\n","#         #     8:45\n","#         #     Here is some useful info about how a person annotator find the ground truth twitter user:\n","#         #     8:46\n","#         #     I followed the instruction in this doc by searching the name + Twitter in the Google first, and click top results to see if there is any match. If none, I will go search the name in Twitter and also browse through the top results. Sometimes I will also search their LinkedIn page to get their most up-to-date information. (the current institute in Google Scholar is not as accurate as their LinkedIn, and LinkedIn has a full history of where they worked. Moreover, they tend to put their photos in LinkedIn)\n","#         # '''\n","\n","\n","# # literature:\n","\n","# # https://direct.mit.edu/qss/article/1/2/771/96149/Large-scale-identification-and-characterization-of"]},{"cell_type":"markdown","metadata":{"id":"ivicehnTyRRO"},"source":["#Get top 10 names"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vb7EKi2fZltc"},"outputs":[],"source":["# del ts"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":84354,"status":"ok","timestamp":1676810538569,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"zo7uH-sfyUBn","outputId":"bc9b9495-027b-4dc5-8361-cdec65e0dfc5"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-8-3edefa08cdc3>:26: DeprecationWarning: headless property is deprecated, instead use add_argument('--headless') or add_argument('--headless=new')\n","  options.headless = True\n","<ipython-input-8-3edefa08cdc3>:37: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n","  self.driver = webdriver.Chrome(driver_path, options=options)\n"]}],"source":["ts = TwitterSearch('/usr/bin/chromedriver')\n","# df = pd.read_csv('/content/drive/My Drive/tweets-dataset/gs_scholars_matched_with_twitter_accounts_500.csv', index_col=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ewfklpmAtSOw"},"outputs":[],"source":["# t_ids = ts.search_scholar('name', \"Paramita Mirza\")\n","# print(t_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A_n6YAgMF3F6"},"outputs":[],"source":["small_df = pd.read_csv(\"/content/drive/My Drive/tweets-dataset/gs_scholars_matched_with_twitter_accounts_500.tsv\",  sep='\\t')\n","scholars_in_1000 = set(list(small_df['name']))\n","del small_df"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"KsNdniiFW5cp","executionInfo":{"status":"error","timestamp":1676822173928,"user_tz":-330,"elapsed":144093,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}},"outputId":"6ffd50e5-2fa6-4163-f98a-c639da983588"},"outputs":[{"output_type":"stream","name":"stdout","text":["[<a href=\"https://twitter.com/srchvrs\">Twitter</a>]\n","[INFO] branch_type: homepage,name + email,name + organization,name\n","[INFO] twitter_ids: ['srchvrs']\n","[INFO] branch_type: name + organization,name\n","[INFO] twitter_ids: ['jenniferdoleac', 'xinrongtan', 'zhu_xinrong', 'Xinrongnet', 'xinrong', 'zann_lim', 'xinrongding', 'XinRongChen2', 'christin073084', 'iqssm', 'xinrongftw']\n","[<a href=\"https://twitter.com/naverlabseurope\" rel=\"noopener\" target=\"_blank\"><img alt=\"blank\" class=\"alignnone size-full wp-image-9337708\" data-wpfc-original-src=\"https://europe.naverlabs.com/wp-content/uploads/2019/01/twitter-logo-silhouette.png\" decoding=\"async\" height=\"24\" loading=\"lazy\" onload=\"Wpfcll.r(this,true);\" src=\"https://europe.naverlabs.com/wp-content/plugins/wp-fastest-cache-premium/pro/images/blank.gif\" width=\"24\"/></a>]\n","[INFO] branch_type: homepage,name\n","[INFO] twitter_ids: ['DenishabsDenis', 'denisproulx10', 'denisproulx2', 'proulxd2', 'lenaigre', 'denisproulx', 'naverlabseurope', 'DenisCProulx', 'deprarur']\n","[INFO] branch_type: name\n","[INFO] twitter_ids: ['ERRIC_UPB', 'trebedea']\n","[INFO] branch_type: name + email,name + organization,name\n","[INFO] twitter_ids: ['antske']\n","[]\n","[INFO] branch_type: name + organization,name\n","[INFO] twitter_ids: ['terpeniye', 'Olga51413930', 'kononovaolga', 'OlgaKononova2', 'cedergroup', 'olkozian', 'lofa4']\n","[INFO] branch_type: name\n","[INFO] twitter_ids: ['marthaschool']\n","[INFO] branch_type: name + organization,name\n","[INFO] twitter_ids: ['laauraplaza', 'lauraplaza89', 'RealtorLauraP', 'LauraPlazaOk', 'laura13atm', 'lali1921', 'LauraPitman', 'lauraplazac']\n","[INFO] branch_type: name + organization,name\n","[INFO] twitter_ids: ['jenilaria', 'khlee169', 'Kira_UCL', 'cyril8282', 'kwok_young', 'alanlflee', 'mimikwok6', 'rayhongkong']\n","[<a href=\"https://twitter.com/EricTopol/status/1152610437642375169\">significant challenge</a>, <a href=\"https://twitter.com/jasonafries\" style=\"padding-right:10px\">Twitter</a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['EricTopol', 'jasonafries']\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","537\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","538\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","539\n","[<a href=\"http://twitter.com/ananya__g\"> <i class=\"fa fa-twitter\" style=\"font-size:24px;color:black\"></i></a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['ananya__g']\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","541\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","542\n","543 Jia Xu already in smaller sheet\n","[<a href=\"https://twitter.com/AngusGLChen\"><i aria-hidden=\"true\" class=\"fab fa-fw fa-twitter-square\"></i> Twitter</a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['AngusGLChen']\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","545\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","546\n","[<a href=\"https://twitter.com/J_Novikova_NLP\" target=\"_blank\" title=\"Twitter\"><i class=\"fab fa-twitter-square tw-icon\"></i></a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['J_Novikova_NLP']\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","548\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","549\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","550\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","551\n","[<a href=\"https://twitter.com/DuMNCH\" rel=\"noopener\" target=\"_blank\"><i class=\"fab fa-twitter big-icon\"></i></a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['DuMNCH']\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","553\n","[<a href=\"https://twitter.com/TomKenter\" target=\"_blank\">Twitter</a>, <a href=\"https://twitter.com/manish1765\" target=\"_blank\">Manish Sharma</a>, <a href=\"https://twitter.com/manish1765\" target=\"_blank\">Manish Sharma</a>, <a href=\"https://twitter.com/TomKenter/status/928227290454134784\" target=\"_blank\">this tweet over here</a>, <a href=\"https://twitter.com/search?src=typd&amp;q=%23nn4ir\" target=\"_blank\">tweeting</a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['manish1765', 'search', 'TomKenter']\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","555\n","[<a href=\"https://twitter.com/dalstonchen\">Twitter</a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['dalstonchen']\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","557\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","558\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","559\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","560\n","[<a href=\"https://twitter.com/ivanvladimir\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"svg-inline--fa fa-twitter fa-w-16 has-text-info\" data-fa-i2svg=\"\" data-icon=\"twitter\" data-prefix=\"fab\" focusable=\"false\" role=\"img\" viewbox=\"0 0 512 512\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z\" fill=\"currentColor\"></path></svg><!-- <i class=\"fab fa-twitter has-text-info\"></i> --></a>, <a href=\"http://twitter.com/ivanvladimir\" target=\"_blank\"><svg aria-hidden=\"true\" class=\"svg-inline--fa fa-twitter-square fa-w-14\" data-fa-i2svg=\"\" data-icon=\"twitter-square\" data-prefix=\"fab\" focusable=\"false\" role=\"img\" viewbox=\"0 0 448 512\" xmlns=\"http://www.w3.org/2000/svg\"><path d=\"M400 32H48C21.5 32 0 53.5 0 80v352c0 26.5 21.5 48 48 48h352c26.5 0 48-21.5 48-48V80c0-26.5-21.5-48-48-48zm-48.9 158.8c.2 2.8.2 5.7.2 8.5 0 86.7-66 186.6-186.6 186.6-37.2 0-71.7-10.8-100.7-29.4 5.3.6 10.4.8 15.8.8 30.7 0 58.9-10.4 81.4-28-28.8-.6-53-19.5-61.3-45.5 10.1 1.5 19.2 1.5 29.6-1.2-30-6.1-52.5-32.5-52.5-64.4v-.8c8.7 4.9 18.9 7.9 29.6 8.3a65.447 65.447 0 0 1-29.2-54.6c0-12.2 3.2-23.4 8.9-33.1 32.3 39.8 80.8 65.8 135.2 68.6-9.3-44.5 24-80.6 64-80.6 18.9 0 35.9 7.9 47.9 20.7 14.8-2.8 29-8.3 41.6-15.8-4.9 15.2-15.2 28-28.8 36.1 13.2-1.4 26-5.1 37.8-10.2-8.9 13.1-20.1 24.7-32.9 34z\" fill=\"currentColor\"></path></svg><!-- <i class=\"fab fa-twitter-square\"></i> --></a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['ivanvladimir']\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","562\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","563\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","564\n","[<a href=\"https://twitter.com/xycking\" target=\"_blank\">twitter</a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['xycking']\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","566\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","567\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","568\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","569\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","570\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","571\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","572\n","[<a alt=\"Twitter\" aria-label=\"Twitter\" class=\"wsite-social-item wsite-social-twitter\" href=\"https://twitter.com/muntsa_padro\" target=\"_blank\"><span class=\"wsite-social-item-inner\"></span></a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['muntsa_padro']\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","574\n","[<a href=\"https://twitter.com/artskuleuven\">\n","<svg height=\"20\" role=\"img\" title=\"twitter\" viewbox=\"0 0 1792 1792\" width=\"20\">\n","<path d=\"M1408 610q-56 25-121 34 68-40 93-117-65 38-134 51-61-66-153-66-87 0-148.5 61.5T883 722q0 29 5 48-129-7-242-65T454 550q-29 50-29 106 0 114 91 175-47-1-100-26v2q0 75 50 133.5t123 72.5q-29 8-51 8-13 0-39-4 21 63 74.5 104t121.5 42q-116 90-261 90-26 0-50-3 148 94 322 94 112 0 210-35.5t168-95 120.5-137 75-162T1304 746q0-18-1-27 63-45 105-109zm256-194v960q0 119-84.5 203.5T1376 1664H416q-119 0-203.5-84.5T128 1376V416q0-119 84.5-203.5T416 128h960q119 0 203.5 84.5T1664 416z\"></path>\n","</svg>\n","</a>, <a href=\"https://twitter.com/artskuleuven\">\n","<svg height=\"20\" role=\"img\" title=\"twitter\" viewbox=\"0 0 1792 1792\" width=\"20\">\n","<path d=\"M1408 610q-56 25-121 34 68-40 93-117-65 38-134 51-61-66-153-66-87 0-148.5 61.5T883 722q0 29 5 48-129-7-242-65T454 550q-29 50-29 106 0 114 91 175-47-1-100-26v2q0 75 50 133.5t123 72.5q-29 8-51 8-13 0-39-4 21 63 74.5 104t121.5 42q-116 90-261 90-26 0-50-3 148 94 322 94 112 0 210-35.5t168-95 120.5-137 75-162T1304 746q0-18-1-27 63-45 105-109zm256-194v960q0 119-84.5 203.5T1376 1664H416q-119 0-203.5-84.5T128 1376V416q0-119 84.5-203.5T416 128h960q119 0 203.5 84.5T1664 416z\"></path>\n","</svg>\n","</a>, <a href=\"https://twitter.com/KU_Leuven/\">\n","<svg height=\"30\" role=\"img\" viewbox=\"0 0 1792 1792\" width=\"30\">\n","<title>twitter</title>\n","<path d=\"M1408 610q-56 25-121 34 68-40 93-117-65 38-134 51-61-66-153-66-87 0-148.5 61.5T883 722q0 29 5 48-129-7-242-65T454 550q-29 50-29 106 0 114 91 175-47-1-100-26v2q0 75 50 133.5t123 72.5q-29 8-51 8-13 0-39-4 21 63 74.5 104t121.5 42q-116 90-261 90-26 0-50-3 148 94 322 94 112 0 210-35.5t168-95 120.5-137 75-162T1304 746q0-18-1-27 63-45 105-109zm256-194v960q0 119-84.5 203.5T1376 1664H416q-119 0-203.5-84.5T128 1376V416q0-119 84.5-203.5T416 128h960q119 0 203.5 84.5T1664 416z\"></path>\n","</svg>\n","</a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['artskuleuven', 'KU_Leuven']\n","[<a href=\"https://twitter.com/jekonym\" target=\"_blank\">Twitter</a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['jekonym']\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","577\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","578\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","579\n","[<a class=\"Link--primary\" href=\"https://twitter.com/mgraffg\" rel=\"nofollow me\">@mgraffg</a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['mgraffg']\n","[<a class=\"text-dark h4 mr-2\" href=\"https://twitter.com/Prompsit\" target=\"_blank\"><i class=\"fab fa-twitter-square\"></i></a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['Prompsit']\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","582\n","[<a class=\"d-flex align-items-center bg-white border border-transparent link-outer hover-bg-key hover-border-white hover-text-white justify-content-center rounded-circle text-key sans-h4 social\" href=\"https://twitter.com/CS_TUDarmstadt\" title=\"Twitter-Kanal des Fachbereichs Informatik der TU Darmstadt\"><span aria-hidden=\"true\" class=\"fab fa-twitter\"></span> <span class=\"sr-only link-inner\">Twitter-Kanal des Fachbereichs Informatik der TU Darmstadt</span></a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['CS_TUDarmstadt']\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","584\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","585\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","586\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","587\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","588\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","589\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","590\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","591\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","592\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","593\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","594\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","595\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","596\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","597\n","[<a href=\"https://twitter.com/ufal_cuni\" style=\"color: gray\"><img height=\"22px\" src=\"/sites/default/files/hp/twitter.svg\" style=\"background-color: darkblue;position:relative;top:-1px\" width=\"22px\"/>@ufal_cuni</a>, <a href=\"https://twitter.com/lindatclariahcz\" style=\"color: gray\"><img height=\"22px\" src=\"/sites/default/files/hp/twitter.svg\" style=\"background-color: darkblue;position:relative;top:-1px\" width=\"22px\"/>@lindatclariahcz</a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['lindatclariahcz', 'ufal_cuni']\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","599\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","600\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","601\n","[<a href=\"https://twitter.com/daiqng\"><i aria-hidden=\"true\" class=\"fab fa-fw fa-twitter-square\"></i> Twitter</a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['daiqng']\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","603\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","604\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","605\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","606\n","[<a aria-label=\"Laboratoire Lattice sur Twitter\" class=\"ow-button-hover sow-social-media-button-twitter-0 sow-social-media-button\" href=\"https://twitter.com/Lab_LATTICE\" rel=\"noopener noreferrer\" target=\"_blank\" title=\"Laboratoire Lattice sur Twitter\">\n","<span>\n","<span aria-hidden=\"true\" class=\"sow-icon-fontawesome sow-fab\" data-sow-icon=\"\"></span> </span>\n","</a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['Lab_LATTICE']\n","[<a class=\"social-link\" href=\"https://twitter.com/atanasovapepa\"><i class=\"fab fa-twitter\"></i></a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['atanasovapepa']\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","609\n","[INFO] branch_type: name + organization,name\n","[INFO] twitter_ids: ['asukahan', 'WNBA', 'chaoshxxu', 'azureneverfades', 'hxu1101', 'hanxu_521', 'uxnah', 'cusc_springer', 'wwwYQ6678531067', 'wnba']\n","[INFO] branch_type: name + email,name + organization,name\n","[INFO] twitter_ids: ['Andie_Beeee', 'manuel_berning', 'manuelberning']\n","[INFO] branch_type: name + organization,name\n","[INFO] twitter_ids: ['docflo', 'florianw1986', 'mrflorianwolf01', 'fwolf_mergeflow', 'fwwolf98']\n","[<a href=\"https://twitter.com/ufal_cuni\" style=\"color: gray\"><img height=\"22px\" src=\"/sites/default/files/hp/twitter.svg\" style=\"background-color: darkblue;position:relative;top:-1px\" width=\"22px\"/>@ufal_cuni</a>, <a href=\"https://twitter.com/lindatclariahcz\" style=\"color: gray\"><img height=\"22px\" src=\"/sites/default/files/hp/twitter.svg\" style=\"background-color: darkblue;position:relative;top:-1px\" width=\"22px\"/>@lindatclariahcz</a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['lindatclariahcz', 'ufal_cuni']\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","614\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","615\n","[<a href=\"https://twitter.com/BredowInstitut\" target=\"_blank\">\n","<svg class=\"icon icon-large icon-facebook\">\n","<use xlink:href=\"#icon-twitter\"></use>\n","</svg>\n","<span>Follow us on Twitter</span>\n","</a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['BredowInstitut']\n","[<a href=\"https://twitter.com/ehsanedding\" style=\"background-color: #eee;\"><img src=\"images/twitter.png\" style=\"width:50px;\"/></a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['ehsanedding']\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","618\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","619\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","620\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","621\n","[<a href=\"https://twitter.com/suzanntee\">@suzanntee</a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['suzanntee']\n","[<link data-rh=\"true\" href=\"https://twitter.com/brianspiering\" hreflang=\"x-default\" rel=\"alternate\"/>, <link data-rh=\"true\" href=\"https://twitter.com/BrianSpiering\" rel=\"canonical\"/>, <a href=\"https://help.twitter.com/using-twitter/twitter-supported-browsers\">Help Center</a>, <a href=\"https://twitter.com/tos\">Terms of Service</a>, <a href=\"https://twitter.com/privacy\">Privacy Policy</a>, <a href=\"https://support.twitter.com/articles/20170514\">Cookie Policy</a>, <a href=\"https://legal.twitter.com/imprint.html\">Imprint</a>, <a href=\"https://business.twitter.com/en/help/troubleshooting/how-twitter-ads-work.html?ref=web-twc-ao-gbl-adsinfo&amp;utm_source=twc&amp;utm_medium=web&amp;utm_campaign=ao&amp;utm_content=adsinfo\">Ads info</a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['privacy', 'using-twitter', 'imprint.html', 'articles', 'brianspiering', 'tos', 'BrianSpiering', 'en']\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","624\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","625\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","626\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","627\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","628\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","629\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","630\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","631\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","632\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","633\n","[<a href=\"https://twitter.com/vladeidelman\" target=\"_self\"><span class=\"screen-reader-text\">Twitter</span><svg aria-hidden=\"true\" class=\"icon icon-twitter\" role=\"presentation\"> <use href=\"#icon-twitter\" xlink:href=\"#icon-twitter\"></use> </svg> </a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['vladeidelman']\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","635\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","636\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","637\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","638\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","639\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","640\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","641\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","642\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","643\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","644\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","645\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","646\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","647\n","[<a href=\"https://twitter.com/xiongwenhan\"><i class=\"fab fa-twitter-square\" style=\"font-size:24px\"></i></a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['xiongwenhan']\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","649\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","650\n","[<a aria-label=\"twitter\" href=\"https://twitter.com/jcalbornozc\" rel=\"noopener\" target=\"_blank\">\n","<i class=\"fab fa-twitter big-icon\"></i>\n","</a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['jcalbornozc']\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","652\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","653\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","654\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","655\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","656\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","657\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","658\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","659\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","660\n","[<a href=\"https://twitter.com/sistema_arText\" target=\"_blank\"><img alt=\"logo_twitter\" class=\"alignnone wp-image-2245\" height=\"18\" sizes=\"(max-width: 21px) 100vw, 21px\" src=\"http://iriadacunha.com/wp-content/uploads/2016/03/logo_twitter-150x133.png\" srcset=\"http://iriadacunha.com/wp-content/uploads/2016/03/logo_twitter-150x133.png 150w, http://iriadacunha.com/wp-content/uploads/2016/03/logo_twitter.png 162w\" width=\"21\"/></a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['sistema_arText']\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","662\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","663\n","[<a href=\"https://twitter.com/ragerri\"><i aria-hidden=\"true\" class=\"fab fa-fw fa-twitter-square\"></i> Twitter</a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['ragerri']\n","[<a href=\"https://twitter.com/jasonkessler\">@jasonkessler</a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['jasonkessler']\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","666\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","667\n","[<a class=\"icon fa-twitter\" href=\"https://twitter.com/wangly0229\" style=\"-webkit-tap-highlight-color: rgba(0, 0, 0, 0);\"><span class=\"label\">Twitter</span></a>, <a class=\"icon fa-twitter\" href=\"https://twitter.com/wangly0229\"><span class=\"label\">Twitter</span></a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['wangly0229']\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","669\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","670\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","671\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","672\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","673\n","[<a class=\"first leaf\" href=\"https://twitter.com/Berkeley_EECS\" target=\"_blank\"><span class=\"sr-only\">Berkley EECS on Twitter</span></a>]\n","[INFO] branch_type: homepage,\n","[INFO] twitter_ids: ['Berkeley_EECS']\n","[]\n","[INFO] branch_type: \n","[INFO] twitter_ids: []\n","675\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-ffbccd711d1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscholars_in_1000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;31m# get top 10 candidates for twitter IDs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m       \u001b[0mcandidate_twitter_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_scholar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m       \u001b[0mnum\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mcandidate_twitter_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-e40ad84a7750>\u001b[0m in \u001b[0;36msearch_scholar\u001b[0;34m(self, str_type, term)\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"organization\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mscholar_search_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mscholar_search_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"organization\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m# and twitter_ids is None:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                     \u001b[0murl_fragment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_urlsearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{scholar_search_result[0][\"name\"]} {scholar_search_result[0][\"organization\"]} \"twitter\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_search_google_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_fragment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m                     \u001b[0mnew_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweb_source\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'google'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnew_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-e40ad84a7750>\u001b[0m in \u001b[0;36m_search_google_helper\u001b[0;34m(self, google_url)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimplicitly_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgoogle_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_source\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# pd.DataFrame.from_records(np.load(\"/content/drive/My Drive/tweets-dataset/gs_scholars_new.npy\", allow_pickle=True))\n","num = 0\n","num_empty = 0\n","with open('/content/drive/My Drive/tweets-dataset/gs_scholars_candidate_twitter_accounts_78k.tsv','a') as fw:\n","  # fw.write(\"\\t\".join([\"index\", \"name\", \"org\", \"url\"]))\n","  # fw.write('\\n')\n","  fw.write('\\n')\n","  for index, row in ts.scholar_search.search_78k.df.iterrows():\n","    if index <= 526:\n","      continue\n","    name = ts.scholar_search.search_78k.df['name'][index]\n","    org = ts.scholar_search.search_78k.df['organization'][index]\n","    gs_url = ts.scholar_search.search_78k.df['url'][index]\n","    line = [str(index), name, org, gs_url]\n","    if name not in scholars_in_1000:\n","      # get top 10 candidates for twitter IDs\n","      candidate_twitter_ids = ts.search_scholar('name', name)\n","      num+=1\n","      if candidate_twitter_ids:\n","        # save list of twitter IDs in the CSV\n","        line.extend(candidate_twitter_ids)\n","      else:\n","        num_empty += 1\n","        if num_empty>3:\n","          time.sleep(10)\n","        print(index)\n","    else:\n","      print(f\"{index} {name} already in smaller sheet\")\n","    fw.write(\"\\t\".join(line))\n","    fw.write('\\n')\n","    time.sleep(3)"]}],"metadata":{"colab":{"collapsed_sections":["PEWY6iMCOrxf","VlhwyVP3QbSU","CK0fRZjArvz4","sgM6xDo9P47w","43Ozad0mQCKZ","6mK1zSXBe6NK"],"provenance":[{"file_id":"1hyPc9u8C9DA3WpLwOxTrJkVmuA5sYbLu","timestamp":1675927416219},{"file_id":"1auhNPM8A6flAc5ai9GipgdsF6Xp2rRWQ","timestamp":1673365201119}],"toc_visible":true},"interpreter":{"hash":"ffc4551cdfa24de1e4d6ff6a879c16b8d7cadd8b756628488549bbbf21e2c19d"},"kernelspec":{"display_name":"Python 3.7.13 ('res')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"}},"nbformat":4,"nbformat_minor":0}