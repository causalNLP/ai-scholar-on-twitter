{"cells":[{"cell_type":"markdown","metadata":{"id":"PEWY6iMCOrxf"},"source":["# google scholar search"]},{"cell_type":"markdown","metadata":{"id":"VlhwyVP3QbSU"},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X4OwlpQLgPhH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677681068163,"user_tz":-330,"elapsed":58403,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}},"outputId":"e356301a-c043-4bf7-94a7-8bdfb74ae046"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GMEIh6fFkiLk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677681139373,"user_tz":-330,"elapsed":71236,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}},"outputId":"539191b7-049d-4eaf-a895-b02b7f73ccd7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Executing: /tmp/apt-key-gpghome.mhECaHMQ2M/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n","gpg: key DCC9EFBF77E11517: public key \"Debian Stable Release Key (10/buster) <debian-release@lists.debian.org>\" imported\n","gpg: Total number processed: 1\n","gpg:               imported: 1\n","Executing: /tmp/apt-key-gpghome.I6IQ5xaSSp/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n","gpg: key DC30D7C23CBBABEE: public key \"Debian Archive Automatic Signing Key (10/buster) <ftpmaster@debian.org>\" imported\n","gpg: Total number processed: 1\n","gpg:               imported: 1\n","Executing: /tmp/apt-key-gpghome.wYvYVg65fo/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n","gpg: key 4DFAB270CAA96DFA: public key \"Debian Security Archive Automatic Signing Key (10/buster) <ftpmaster@debian.org>\" imported\n","gpg: Total number processed: 1\n","gpg:               imported: 1\n","Warning: apt-key output should not be parsed (stdout is not a terminal)\n","Warning: apt-key output should not be parsed (stdout is not a terminal)\n","Warning: apt-key output should not be parsed (stdout is not a terminal)\n","Get:1 http://deb.debian.org/debian buster InRelease [122 kB]\n","Get:2 http://deb.debian.org/debian buster-updates InRelease [56.6 kB]\n","Get:3 http://deb.debian.org/debian-security buster/updates InRelease [34.8 kB]\n","Get:4 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n","Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease\n","Get:6 http://deb.debian.org/debian buster/main amd64 Packages [10.7 MB]\n","Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n","Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1,581 B]\n","Get:9 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n","Get:10 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n","Hit:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n","Get:12 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease [18.1 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n","Get:14 http://deb.debian.org/debian buster-updates/main amd64 Packages [9,745 B]\n","Get:15 http://deb.debian.org/debian-security buster/updates/main amd64 Packages [591 kB]\n","Hit:16 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n","Hit:17 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n","Get:18 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease [24.3 kB]\n","Get:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [907 kB]\n","Hit:21 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n","Get:22 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2,972 kB]\n","Get:23 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,304 kB]\n","Get:24 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main Sources [2,394 kB]\n","Get:25 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,008 kB]\n","Get:26 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,497 kB]\n","Get:27 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal/main amd64 Packages [1,134 kB]\n","Get:28 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal/main amd64 Packages [42.9 kB]\n","Fetched 24.2 MB in 3s (7,401 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  chromium-common chromium-sandbox libevent-2.1-6 libicu63 libimobiledevice6\n","  libjpeg62-turbo libplist3 libre2-5 libu2f-udev libusbmuxd6 libvpx5\n","  libxxf86dga1 upower usbmuxd x11-utils\n","Suggested packages:\n","  chromium-l10n chromium-shell libusbmuxd-tools mesa-utils\n","The following NEW packages will be installed:\n","  chromium chromium-common chromium-driver chromium-sandbox libevent-2.1-6\n","  libicu63 libimobiledevice6 libjpeg62-turbo libplist3 libre2-5 libu2f-udev\n","  libusbmuxd6 libvpx5 libxxf86dga1 upower usbmuxd x11-utils\n","0 upgraded, 17 newly installed, 0 to remove and 52 not upgraded.\n","Need to get 74.6 MB of archives.\n","After this operation, 256 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libre2-5 amd64 20200101+dfsg-1build1 [162 kB]\n","Get:2 http://deb.debian.org/debian buster/main amd64 libevent-2.1-6 amd64 2.1.8-stable-4 [177 kB]\n","Get:3 http://deb.debian.org/debian buster/main amd64 libicu63 amd64 63.1-6+deb10u3 [8,293 kB]\n","Get:4 http://archive.ubuntu.com/ubuntu focal/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu1 [12.0 kB]\n","Get:5 http://archive.ubuntu.com/ubuntu focal/main amd64 x11-utils amd64 7.7+5 [199 kB]\n","Get:6 http://archive.ubuntu.com/ubuntu focal/main amd64 libplist3 amd64 2.1.0-4build2 [31.6 kB]\n","Get:7 http://archive.ubuntu.com/ubuntu focal/main amd64 libusbmuxd6 amd64 2.0.1-2 [19.1 kB]\n","Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 libimobiledevice6 amd64 1.2.1~git20191129.9f79242-1build1 [65.2 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu focal/main amd64 libu2f-udev all 1.1.10-1 [6,108 B]\n","Get:10 http://archive.ubuntu.com/ubuntu focal/main amd64 upower amd64 0.99.11-1build2 [104 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 usbmuxd amd64 1.1.1~git20191130.9af2b12-1 [38.4 kB]\n","Get:12 http://deb.debian.org/debian buster/main amd64 libjpeg62-turbo amd64 1:1.5.2-2+deb10u1 [133 kB]\n","Get:13 http://deb.debian.org/debian buster/main amd64 libvpx5 amd64 1.7.0-3+deb10u1 [800 kB]\n","Get:14 http://deb.debian.org/debian buster/main amd64 chromium-common amd64 90.0.4430.212-1~deb10u1 [1,423 kB]\n","Get:15 http://deb.debian.org/debian buster/main amd64 chromium amd64 90.0.4430.212-1~deb10u1 [58.3 MB]\n","Get:16 http://deb.debian.org/debian buster/main amd64 chromium-driver amd64 90.0.4430.212-1~deb10u1 [4,703 kB]\n","Get:17 http://deb.debian.org/debian buster/main amd64 chromium-sandbox amd64 90.0.4430.212-1~deb10u1 [146 kB]\n","Fetched 74.6 MB in 1s (82.3 MB/s)\n","Selecting previously unselected package libevent-2.1-6:amd64.\n","(Reading database ... 128208 files and directories currently installed.)\n","Preparing to unpack .../00-libevent-2.1-6_2.1.8-stable-4_amd64.deb ...\n","Unpacking libevent-2.1-6:amd64 (2.1.8-stable-4) ...\n","Selecting previously unselected package libicu63:amd64.\n","Preparing to unpack .../01-libicu63_63.1-6+deb10u3_amd64.deb ...\n","Unpacking libicu63:amd64 (63.1-6+deb10u3) ...\n","Selecting previously unselected package libjpeg62-turbo:amd64.\n","Preparing to unpack .../02-libjpeg62-turbo_1%3a1.5.2-2+deb10u1_amd64.deb ...\n","Unpacking libjpeg62-turbo:amd64 (1:1.5.2-2+deb10u1) ...\n","Selecting previously unselected package libre2-5:amd64.\n","Preparing to unpack .../03-libre2-5_20200101+dfsg-1build1_amd64.deb ...\n","Unpacking libre2-5:amd64 (20200101+dfsg-1build1) ...\n","Selecting previously unselected package libvpx5:amd64.\n","Preparing to unpack .../04-libvpx5_1.7.0-3+deb10u1_amd64.deb ...\n","Unpacking libvpx5:amd64 (1.7.0-3+deb10u1) ...\n","Selecting previously unselected package libxxf86dga1:amd64.\n","Preparing to unpack .../05-libxxf86dga1_2%3a1.1.5-0ubuntu1_amd64.deb ...\n","Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu1) ...\n","Selecting previously unselected package x11-utils.\n","Preparing to unpack .../06-x11-utils_7.7+5_amd64.deb ...\n","Unpacking x11-utils (7.7+5) ...\n","Selecting previously unselected package chromium-common.\n","Preparing to unpack .../07-chromium-common_90.0.4430.212-1~deb10u1_amd64.deb ...\n","Unpacking chromium-common (90.0.4430.212-1~deb10u1) ...\n","Selecting previously unselected package chromium.\n","Preparing to unpack .../08-chromium_90.0.4430.212-1~deb10u1_amd64.deb ...\n","Unpacking chromium (90.0.4430.212-1~deb10u1) ...\n","Selecting previously unselected package chromium-driver.\n","Preparing to unpack .../09-chromium-driver_90.0.4430.212-1~deb10u1_amd64.deb ...\n","Unpacking chromium-driver (90.0.4430.212-1~deb10u1) ...\n","Selecting previously unselected package chromium-sandbox.\n","Preparing to unpack .../10-chromium-sandbox_90.0.4430.212-1~deb10u1_amd64.deb ...\n","Unpacking chromium-sandbox (90.0.4430.212-1~deb10u1) ...\n","Selecting previously unselected package libplist3:amd64.\n","Preparing to unpack .../11-libplist3_2.1.0-4build2_amd64.deb ...\n","Unpacking libplist3:amd64 (2.1.0-4build2) ...\n","Selecting previously unselected package libusbmuxd6:amd64.\n","Preparing to unpack .../12-libusbmuxd6_2.0.1-2_amd64.deb ...\n","Unpacking libusbmuxd6:amd64 (2.0.1-2) ...\n","Selecting previously unselected package libimobiledevice6:amd64.\n","Preparing to unpack .../13-libimobiledevice6_1.2.1~git20191129.9f79242-1build1_amd64.deb ...\n","Unpacking libimobiledevice6:amd64 (1.2.1~git20191129.9f79242-1build1) ...\n","Selecting previously unselected package libu2f-udev.\n","Preparing to unpack .../14-libu2f-udev_1.1.10-1_all.deb ...\n","Unpacking libu2f-udev (1.1.10-1) ...\n","Selecting previously unselected package upower.\n","Preparing to unpack .../15-upower_0.99.11-1build2_amd64.deb ...\n","Unpacking upower (0.99.11-1build2) ...\n","Selecting previously unselected package usbmuxd.\n","Preparing to unpack .../16-usbmuxd_1.1.1~git20191130.9af2b12-1_amd64.deb ...\n","Unpacking usbmuxd (1.1.1~git20191130.9af2b12-1) ...\n","Setting up libplist3:amd64 (2.1.0-4build2) ...\n","Setting up libu2f-udev (1.1.10-1) ...\n","Failed to send reload request: No such file or directory\n","Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu1) ...\n","Setting up chromium-sandbox (90.0.4430.212-1~deb10u1) ...\n","Setting up libicu63:amd64 (63.1-6+deb10u3) ...\n","Setting up libjpeg62-turbo:amd64 (1:1.5.2-2+deb10u1) ...\n","Setting up libevent-2.1-6:amd64 (2.1.8-stable-4) ...\n","Setting up libusbmuxd6:amd64 (2.0.1-2) ...\n","Setting up x11-utils (7.7+5) ...\n","Setting up libre2-5:amd64 (20200101+dfsg-1build1) ...\n","Setting up chromium-common (90.0.4430.212-1~deb10u1) ...\n","Setting up libimobiledevice6:amd64 (1.2.1~git20191129.9f79242-1build1) ...\n","Setting up libvpx5:amd64 (1.7.0-3+deb10u1) ...\n","Setting up upower (0.99.11-1build2) ...\n","Setting up usbmuxd (1.1.1~git20191130.9af2b12-1) ...\n","Warning: The home dir /var/lib/usbmux you specified can't be accessed: No such file or directory\n","Adding system user `usbmux' (UID 107) ...\n","Adding new user `usbmux' (UID 107) with group `plugdev' ...\n","Not creating home directory `/var/lib/usbmux'.\n","Setting up chromium (90.0.4430.212-1~deb10u1) ...\n","update-alternatives: using /usr/bin/chromium to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n","update-alternatives: using /usr/bin/chromium to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n","Setting up chromium-driver (90.0.4430.212-1~deb10u1) ...\n","Processing triggers for man-db (2.9.1-1) ...\n","Processing triggers for dbus (1.12.16-2ubuntu2.3) ...\n","Processing triggers for mime-support (3.64ubuntu1) ...\n","Processing triggers for hicolor-icon-theme (0.17-2) ...\n","Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting selenium\n","  Downloading selenium-4.8.2-py3-none-any.whl (6.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.8/dist-packages (from selenium) (2022.12.7)\n","Collecting trio~=0.17\n","  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 KB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting trio-websocket~=0.9\n","  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n","Requirement already satisfied: urllib3[socks]~=1.26 in /usr/local/lib/python3.8/dist-packages (from selenium) (1.26.14)\n","Collecting outcome\n","  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium) (2.4.0)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium) (22.2.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium) (2.10)\n","Collecting sniffio\n","  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n","Collecting exceptiongroup>=1.0.0rc9\n","  Downloading exceptiongroup-1.1.0-py3-none-any.whl (14 kB)\n","Collecting async-generator>=1.9\n","  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n","Collecting wsproto>=0.14\n","  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n","Collecting h11<1,>=0.9.0\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: sniffio, outcome, h11, exceptiongroup, async-generator, wsproto, trio, trio-websocket, selenium\n","Successfully installed async-generator-1.10 exceptiongroup-1.1.0 h11-0.14.0 outcome-1.2.0 selenium-4.8.2 sniffio-1.3.0 trio-0.22.0 trio-websocket-0.9.2 wsproto-1.2.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting jsonlines\n","  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from jsonlines) (22.2.0)\n","Installing collected packages: jsonlines\n","Successfully installed jsonlines-3.1.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting twitter\n","  Downloading twitter-1.19.6-py2.py3-none-any.whl (50 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from twitter) (2022.12.7)\n","Installing collected packages: twitter\n","Successfully installed twitter-1.19.6\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tweepy in /usr/local/lib/python3.8/dist-packages (3.10.0)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from tweepy) (1.3.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.8/dist-packages (from tweepy) (1.15.0)\n","Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.8/dist-packages (from tweepy) (2.25.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->tweepy) (3.2.2)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]>=2.11.1->tweepy) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]>=2.11.1->tweepy) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.26.14)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]>=2.11.1->tweepy) (2022.12.7)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]>=2.11.1->tweepy) (1.7.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting fuzzysearch\n","  Downloading fuzzysearch-0.7.3.tar.gz (112 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: attrs>=19.3 in /usr/local/lib/python3.8/dist-packages (from fuzzysearch) (22.2.0)\n","Building wheels for collected packages: fuzzysearch\n","  Building wheel for fuzzysearch (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fuzzysearch: filename=fuzzysearch-0.7.3-cp38-cp38-linux_x86_64.whl size=363996 sha256=cf303d5b219771d580a637d15fa0d9fe8ebd1d516a6e41b636374daee61ee16d\n","  Stored in directory: /root/.cache/pip/wheels/fc/e6/eb/4c4b250a0d6562161dcb8667e2cb07a5e20b257fcb75e50b04\n","Successfully built fuzzysearch\n","Installing collected packages: fuzzysearch\n","Successfully installed fuzzysearch-0.7.3\n"]},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":2}],"source":["%%shell\n","# Ubuntu no longer distributes chromium-browser outside of snap\n","#\n","# Proposed solution: https://askubuntu.com/questions/1204571/how-to-install-chromium-without-snap\n","\n","# Add debian buster\n","cat > /etc/apt/sources.list.d/debian.list <<'EOF'\n","deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main\n","deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main\n","deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main\n","EOF\n","\n","# Add keys\n","apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n","apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n","apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n","\n","apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg\n","apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg\n","apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg\n","\n","# Prefer debian repo for chromium* packages only\n","# Note the double-blank lines between entries\n","cat > /etc/apt/preferences.d/chromium.pref << 'EOF'\n","Package: *\n","Pin: release a=eoan\n","Pin-Priority: 500\n","\n","\n","Package: *\n","Pin: origin \"deb.debian.org\"\n","Pin-Priority: 300\n","\n","\n","Package: chromium*\n","Pin: origin \"deb.debian.org\"\n","Pin-Priority: 700\n","EOF\n","\n","# Install chromium and chromium-driver\n","apt-get update\n","apt-get install chromium chromium-driver\n","\n","# Install selenium\n","pip install selenium\n","pip install jsonlines\n","pip install twitter\n","pip install tweepy\n","pip install fuzzysearch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DljCawwaOrxf"},"outputs":[],"source":["import random\n","import pandas as pd\n","import numpy as np\n","import pickle\n","import jsonlines\n","import csv\n","import tweepy\n","import re\n","from twitter import *\n","import json\n","import time\n","from tqdm import tqdm"]},{"cell_type":"markdown","metadata":{"id":"CK0fRZjArvz4"},"source":["###Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vRap3bc7UrUx"},"outputs":[],"source":["from difflib import SequenceMatcher\n","\n","driver_path = '/usr/bin/chromedriver'\n","\n","def generate_or_keyword_list(query_dict: dict):\n","    \"\"\"Generate necessary keyword lists to help selecting final candidates.\"\"\"\n","    or_keyword_list = []\n","    or_keyword_dict = {}\n","    or_keyword_dict['gs_sid'] = ''\n","    domain_labels = []\n","    if 'expertise' in query_dict['profile']['content']:\n","        for keyword in query_dict['profile']['content']['expertise']:\n","            for key in keyword['keywords']:\n","                key = key.strip().lower()\n","                domain_labels.append(key)\n","    or_keyword_dict['domain_labels'] = domain_labels\n","\n","    coauthors = []\n","    if 'relations' in query_dict['profile']['content'] and len(query_dict['profile']['content']['relations']) > 0:\n","        for relation in query_dict['profile']['content']['relations']:\n","            coauthors.append(relation['name'])\n","    or_keyword_dict['coauthors'] = coauthors\n","\n","    if 'history' in query_dict['profile']['content'] and len(query_dict['profile']['content']['history']) > 0:\n","        tmp_dict = query_dict['profile']['content']['history'][0]\n","        if 'position' in tmp_dict:\n","            or_keyword_dict['position'] = tmp_dict['position']\n","        if 'institution' in tmp_dict:\n","            if 'domain' in tmp_dict['institution']:\n","                or_keyword_dict['email_suffix'] = tmp_dict['institution']['domain']\n","            if 'name' in tmp_dict['institution']:\n","                or_keyword_dict['organization'] = tmp_dict['institution']['name']\n","\n","    or_keyword_list.append(or_keyword_dict)\n","\n","    return or_keyword_list\n","\n","def get_str_similarity(a: str, b: str) -> float:\n","    \"\"\"Calculate the similarity of two strings and return a similarity ratio.\"\"\"\n","    return SequenceMatcher(None, a, b).ratio()"]},{"cell_type":"markdown","metadata":{"id":"sgM6xDo9P47w"},"source":["### Scholar78kSearch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iFhE-0YePq3U"},"outputs":[],"source":["import pandas as pd\n","import os\n","import re\n","import numpy as np\n","from typing import Union, List\n","\n","\n","class Scholar78kSearch():\n","    def __init__(self):\n","        self.get_78kdata()\n","        self.simple = False\n","        self.verbose = False\n","        self.print_true = False\n","\n","    def get_78kdata(self, source='gdrive'):\n","        \"\"\"Download and load the 78k dataset data.\n","        \n","        Parameters\n","        ----------\n","        source : default is 'gdrive'.\n","        \"\"\"\n","        path_name = 'gs_scholars_new.npy'\n","        if source == 'gdrive':\n","            self.df = pd.DataFrame.from_records(np.load(\"/content/drive/My Drive/tweets-dataset/gs_scholars_new.npy\", allow_pickle=True))\n","        else:\n","            raise NotImplementedError\n","    \n","    def search_name(self, name: Union[str, list], query_dict: dict = None) -> List[dict]:\n","        \"\"\"Search scholar candidates given name in the 78k AI scholar dataset.\n","        \n","        Parameters\n","        ----------\n","        name : name of the scholar.\n","        query_dict : if this is given, the method will run <self._search_name_others_helper()>\n","\n","        Returns\n","        -------\n","        df_row_list : a list of response dictionaries.\n","        \n","        \"\"\"\n","        if type(name) is list:\n","            name_list = [name[0], name[-1]]\n","            name = f'{name[0]} {name[-1]}' \n","        elif type(name) is str:\n","            name_list = re.sub('[0-9_\\.\\(\\)\\[\\],]', '', name).split(' ')\n","        else:\n","            raise TypeError(f'Argument \"name\" passed to Scholar78kSearch.search_name has the wrong type.')\n","        df_row = self._search_name_only_helper(name, name_list)\n","        if df_row.shape[0] > 0 and query_dict is not None:\n","            df_row = self._search_name_others_helper(df_row, query_dict)\n","        if self.print_true:\n","            print(f'[Info] Found {df_row.shape[0]} scholars are in 78k data.')\n","            print(f'[Debug] Names: {df_row[\"name\"]}')\n","        if self.verbose:\n","            print(df_row)\n","        return self._deal_with_simple(df_row)\n","        # return df_row\n","\n","    def _deal_with_simple(self, df_row):\n","        if self.simple:\n","            df_row = df_row.loc[:, df_row.columns != 'papers']\n","        df_row = df_row.drop(['co_authors_all'], axis=1)\n","        return df_row.to_dict(orient='records')\n","\n","    def _search_name_only_helper(self, name, name_list):\n","        \"\"\"Helper function of search_name\n","\n","        Returns\n","        -------\n","        Boolean : found or not.\n","        DataFrame : if find else None.\n","        \"\"\"\n","        # find the scholar in our dataset\n","        name_df = self.df.loc[self.df['name'] == name].copy()\n","        name_list_df = self.df.loc[self.df['name'].str.contains(pat = f'^{name_list[0].capitalize()} .*{name_list[-1].capitalize()}', regex=True, case=False)].copy()\n","        return pd.concat([name_df, name_list_df]).drop_duplicates(subset=['url']).reset_index(drop=True)\n","\n","    def _search_name_others_helper(self, df_row, query_dict):\n","        # TODO: add a better filter more than by name\n","        return df_row"]},{"cell_type":"markdown","metadata":{"id":"SIiYtVtozmXP"},"source":["### ScholarGsSearch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EeoLydeqzn03"},"outputs":[],"source":["import re\n","import time\n","from typing import Union\n","from selenium.webdriver.chromium.webdriver import ChromiumDriver\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.remote.errorhandler import NoSuchElementException\n","\n","from selenium import webdriver\n","from selenium.webdriver.chrome.options import Options\n","\n","# url = \"https://github.com/googlecolab/colabtools/issues/3347\" \n","\n","class GoogleSearch():\n","    \"\"\"Base class for performing web search on Google using REST API.\"\"\"\n","    def __init__(self, driver_path):\n","        self.setup_webdriver(driver_path)\n","    \n","    def setup_webdriver(self, driver_path):\n","        \"\"\"Setup the webdriver object.\"\"\"\n","\n","        options = Options()\n","        options.add_argument(\"--headless\")\n","        options.add_argument(\"--no-sandbox\")\n","        options.headless = True\n","        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n","        options.add_experimental_option('useAutomationExtension', False)\n","        options.add_argument('--disable-dev-shm-usage')\n","        self.driver = webdriver.Chrome(driver_path, options=options)\n","\n","\n","class ScholarGsSearch(GoogleSearch):\n","    \"\"\"Class that handling searching on Google Scholar webpage using REST GET API.\"\"\"\n","    def __init__(self, driver_path):\n","        super().__init__(driver_path)\n","        self._authsearch = 'https://scholar.google.com/citations?hl=en&view_op=search_authors&mauthors={0}'\n","        self._gsidsearch = 'https://scholar.google.com/citations?hl=en&user={0}'\n","        self.print_true = False\n","    \n","    def change_name(self, name):\n","        new_name = name[1:].split('_')\n","        new_name[-1] = re.sub(r'[0-9]+', '', new_name[-1])\n","        new_name = ' '.join(new_name)\n","        return new_name\n","\n","    def search_gsid(self, gs_sid: str, simple: bool = True):\n","        \"\"\"Search scholar on Google Scholar based given gs_sid.\n","        \n","        Parameters\n","        ----------\n","        gs_sid : google scholar sid\n","        simple : whether return simple information without paper list.\n","\n","        Returns\n","        -------\n","        scholar_dict_list : a list of dicts of responses.\n","        \n","        \"\"\"\n","        url = self._gsidsearch.format(gs_sid)\n","        self.search_gs_url(url, simple=simple)\n","        \n","    def search_gs_url(self, url: str, simple: bool = True):\n","        self.driver.get(url)\n","        scholar_dict = self._search_gsid_helper(self.driver, url, simple=simple)\n","        time.sleep(5)\n","        if scholar_dict is not None:\n","            \n","            return [scholar_dict]\n","        else:\n","            if self.print_true:\n","                print('[Info] No scholars found given gs_sid in search_gs.')\n","            return []\n","\n","        \n","    def _search_gsid_helper(self, driver: ChromiumDriver, url: str, simple: bool = True):\n","        \"\"\"Helper function for search_gsid.\"\"\"\n","\n","        def get_single_author(element):\n","            li=[]\n","            li.append(element.find_elements(By.TAG_NAME, \"a\")[0].get_attribute('href'))\n","            li.append(element.find_elements(By.TAG_NAME, \"a\")[0].get_attribute('textContent'))\n","            for i in element.find_elements(By.CLASS_NAME, \"gsc_rsb_a_ext\"):\n","                li.append(i.get_attribute('textContent'))\n","            return li\n","\n","        html_first_class = driver.find_elements(By.CLASS_NAME, \"gsc_g_hist_wrp\")\n","        if (len(html_first_class)==0):\n","            if self.print_true:\n","                print(\"[Info] len(html_first_class)==0\")\n","            return None\n","        idx_list = html_first_class[0].find_elements(By.CLASS_NAME, \"gsc_md_hist_b\")[0]\n","        years =  [i.get_attribute('textContent') for i in idx_list.find_elements(By.CLASS_NAME, \"gsc_g_t\")]\n","        cites =  [i.get_attribute('innerHTML') for i in idx_list.find_elements(By.CLASS_NAME, \"gsc_g_al\")]\n","        rsb = driver.find_elements(By.CLASS_NAME, \"gsc_rsb\")[0]\n","        Citations_table=[i.get_attribute('textContent') for i in  rsb.find_elements(By.CLASS_NAME, \"gsc_rsb_std\")]\n","        Co_authors = rsb.find_elements(By.CLASS_NAME, \"gsc_rsb_a\")\n","        if len(Co_authors) == 0:\n","            Co_authors = None\n","        else:\n","            Co_authors = [get_single_author(i) for i in rsb.find_element(By.CLASS_NAME, \"gsc_rsb_a\").find_elements(By.CLASS_NAME, \"gsc_rsb_a_desc\")]\n","\n","        Researcher = {\"url\": url}\n","        gs_sid = None\n","        if 'user=' in url:\n","            tmp_gs_sid = url.split('user=', 1)[1]\n","            if len(tmp_gs_sid) >= 12:\n","                gs_sid = tmp_gs_sid[:12]\n","        # gs_sid\n","        Researcher['gs_sid'] = gs_sid\n","        # coauthors that are listed at the lower right of the profile page\n","        Researcher[\"coauthors\"] = Co_authors\n","        # citation table\n","        Researcher[\"citation_table\"] = [Citations_table[0], Citations_table[2]]\n","        # time series citations\n","        Researcher[\"cites\"] = {\"years\":years, \"cites\":cites}\n","        # name\n","        nameList = driver.find_elements(By.ID, \"gsc_prf_in\")\n","        if (len(nameList) != 1):\n","            if self.print_true:\n","                print(\"len(nameList)!=1\")\n","            return None\n","        Researcher[\"name\"] = nameList[0].text\n","        # organization\n","        infoList = driver.find_elements(By.CLASS_NAME, 'gsc_prf_il')\n","        Researcher['organization'] = infoList[0].get_attribute('textContent')\n","        # homepage\n","        homepage_url = infoList[1].find_elements(By.TAG_NAME, 'a')\n","        if len(homepage_url) == 0:\n","            Researcher['homepage_url'] = None\n","        else:\n","            Researcher['homepage_url'] = homepage_url[0].get_attribute('href')\n","        # email address\n","        email_str_match = re.search(r'[\\w-]+\\.[\\w.-]+', infoList[1].text)\n","        if email_str_match is not None:\n","            Researcher['email_info'] = email_str_match.group(0)\n","        # domain labels\n","        Researcher['domain_labels'] = [i.get_attribute('textContent').strip().lower() for i in infoList[2].find_elements(By.CLASS_NAME, 'gsc_prf_inta')]\n","        # if not simple, get paper lists\n","        if not simple:\n","            button = driver.find_elements(By.CLASS_NAME, 'gs_btnPD')\n","            if (len(button) != 1):\n","                if self.print_true:\n","                    print(\"len(button)!=1\")\n","                return None\n","            while (button[0].is_enabled()):\n","                while (button[0].is_enabled()):\n","                    while (button[0].is_enabled()):\n","                        button[0].click()\n","                        time.sleep(5)\n","                    time.sleep(1)\n","                time.sleep(2)\n","            papers = []\n","            items = driver.find_elements(By.CLASS_NAME, 'gsc_a_tr')\n","            for i in items:\n","                item = i.find_element(By.CLASS_NAME, 'gsc_a_at')\n","                url = item.get_attribute(\"href\")\n","                paper_info=[j.text for j in i.find_elements(By.CLASS_NAME, 'gs_gray')]\n","                cite = i.find_element(By.CLASS_NAME, 'gsc_a_ac')\n","                year = i.find_element(By.CLASS_NAME, 'gsc_a_y').find_element(By.CLASS_NAME, \"gsc_a_h\").text\n","                papers.append([url, item.text, \n","                                paper_info,\n","                            cite.text, cite.get_attribute(\"href\"),\n","                            year])\n","            Researcher[\"papers\"] = papers\n","\n","        def generate_single_coauthor(element):\n","            coauthor_dict = {\n","                \"name\":element.find_elements(By.CLASS_NAME, 'gs_ai_name')[0].get_attribute('textContent'),\n","                \"url\":element.find_elements(By.CLASS_NAME, 'gs_ai_pho')[0].get_attribute('href'),\n","                \"description\":element.get_attribute('innerHTML'),\n","            }\n","            return coauthor_dict\n","        extra_coauthors = driver.find_elements(By.CLASS_NAME, \"gsc_ucoar\")\n","        Researcher['extra_co_authors'] = [generate_single_coauthor(i) for i in extra_coauthors]\n","        return Researcher\n","\n","    def search_name(self, name: Union[str, list], query_dict: dict = None, top_n=3, simple=True):\n","        \"\"\"Search on Google Scholar webpage given name.\n","        \n","        Parameters\n","        ----------\n","        name : name of the scholar.\n","        query_dict : a dict containing information of the scholar.\n","        top_n : select <top_n> candidates.\n","        simple : whether return simple information without paper list.\n","\n","        Returns\n","        -------\n","        resp : list of candidate scholars, empty if no candidates are found.\n","\n","        \"\"\"\n","        if type(name) is list:\n","            # current case\n","            name_list = [name[0], name[-1]]\n","            name = f'{name[0]} {name[-1]}' \n","        elif type(name) is str:\n","            name_list = name.split(' ')\n","        else:\n","            raise TypeError('Argument \"name\" passed to ScholarGsSearch.search_name has the wrong type.')\n","        url_fragment = f'{name} '\n","        if query_dict is not None:\n","            # first try (name, email_suffix, position, organization) as url\n","            keyword_list = generate_or_keyword_list(query_dict)[0]\n","            url_fragment_new = url_fragment\n","            # if 'email_suffix' in keyword_list:\n","            #     url_fragment_new = url_fragment_new + keyword_list['email_suffix'] + ' '\n","            # if 'position' in keyword_list:\n","            #     url_fragment_new = url_fragment_new + keyword_list['position'] + ' '\n","            # if 'organization' in keyword_list:\n","            #     url_fragment_new = url_fragment_new + keyword_list['organization'] + ' '\n","\n","            # url = self._authsearch.format(url_fragment_new)\n","            # self.driver.get(url)\n","            # time.sleep(5)\n","            # scholar_list = self._search_name_helper(self.driver, name_list)\n","            # if len(scholar_list) > 0:\n","            #     if wo_full:\n","            #         return scholar_list\n","            #     else:\n","            #         return self._search_name_list_expand(scholar_list, simple=simple)\n","            \n","            # second try (name, email_suffix)\n","            if 'email_suffix' in keyword_list:\n","                url_fragment_new = url_fragment + keyword_list['email_suffix'] # + ' '\n","            url = self._authsearch.format(url_fragment_new)\n","            self.driver.get(url)\n","            time.sleep(5)\n","            scholar_list = self._search_name_helper(self.driver, name_list)\n","            # return scholar_list\n","            if len(scholar_list) > 0:\n","                if self.print_true:\n","                    print(f'[Info] Find {len(scholar_list)} scholars using query without gs_sid in step 1.')\n","                # return self._search_name_list_expand(scholar_list, simple=simple)\n","                return scholar_list\n","        \n","            # third try (name, position)\n","            if 'position' in keyword_list:\n","                url_fragment_new = url_fragment + keyword_list['position'] # + ' '\n","            url = self._authsearch.format(url_fragment_new)\n","            self.driver.get(url)\n","            time.sleep(5)\n","            scholar_list = self._search_name_helper(self.driver, name_list)\n","            # return scholar_list\n","            if len(scholar_list) > 0:\n","                if self.print_true:\n","                    print(f'[Info] Find {len(scholar_list)} scholars using query without gs_sid in step 2.')\n","                # return self._search_name_list_expand(scholar_list, simple=simple)\n","                return scholar_list\n","\n","            # fourth try (name, organization)\n","            if 'organization' in keyword_list:\n","                url_fragment_new = url_fragment + keyword_list['organization'] # + ' '\n","            url = self._authsearch.format(url_fragment_new)\n","            self.driver.get(url)\n","            time.sleep(5)\n","            scholar_list = self._search_name_helper(self.driver, name_list)\n","            # return scholar_list\n","            if len(scholar_list) > 0:\n","                if self.print_true:\n","                    print(f'[Info] Find {len(scholar_list)} scholars using query without gs_sid in step 3.')\n","                # return self._search_name_list_expand(scholar_list, simple=simple)\n","                return scholar_list\n","\n","        # finally, only search (name: firstname and lastname). If only one response returns, mark it as candidate\n","        url = self._authsearch.format(url_fragment)\n","        self.driver.get(url)\n","        time.sleep(5)\n","        scholar_list = self._search_name_helper(self.driver, name_list)\n","        if len(scholar_list) > 0:\n","        # if len(scholar_list) > 0 and len(scholar_list) <= top_n:\n","            if self.print_true:\n","                print(f'[Info] Find {len(scholar_list)} scholars using query without gs_sid in step 4.')\n","            # return self._search_name_list_expand(scholar_list, simple=simple)\n","            return scholar_list\n","        \n","        return []\n","\n","    def _search_name_helper(self, driver, name_list):\n","        \"\"\"Helper function of <self.search_name()>.\"\"\"\n","        # iterate over searched list, find dicts that contains the name (including)\n","        useful_info_list = driver.find_elements(By.CLASS_NAME, 'gs_ai_t')\n","        useful_info_ext_list = []\n","        if len(useful_info_list) != 0:\n","            for scholar_webdriver in useful_info_list:\n","                name = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_name').get_attribute('textContent').strip()\n","                # check whether name is correct\n","                not_a_candidate = False\n","                for name_fragment in name_list:\n","                    if name_fragment.lower() not in name.lower():\n","                        not_a_candidate = True\n","                        break\n","                if not_a_candidate:\n","                    continue\n","                \n","                # grab all the other information\n","                pos_org = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_aff').get_attribute('textContent').strip()\n","                email_str = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_eml').get_attribute('textContent').strip()\n","                cite = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_cby').get_attribute('textContent').strip()\n","                url = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_name').find_element(By.TAG_NAME, 'a').get_attribute('href').strip()\n","                domain_labels = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_int').find_elements(By.CLASS_NAME, 'gs_ai_ont_int')\n","                for idx, domain in enumerate(domain_labels):\n","                    domain_labels[idx] = domain.get_attribute('textContent').strip().lower()\n","\n","                # continue processing\n","                gs_sid = None\n","                if 'user=' in url:\n","                    tmp_gs_sid = url.split('user=', 1)[1]\n","                    if len(tmp_gs_sid) >= 12:\n","                        gs_sid = tmp_gs_sid[:12]\n","\n","                if email_str is not None and email_str != '':\n","                    match = re.search(r'[\\w-]+\\.[\\w.-]+', email_str)\n","                    email_str = match.group(0)\n","\n","                cites = [int(s) for s in cite.split() if s.isdigit()]\n","                useful_info_ext_list.append({\n","                    'name': name,\n","                    'pos_org': pos_org,\n","                    'email': email_str,\n","                    'cite': cites[0] if len(cites)>0 else None,\n","                    'url': url,\n","                    'gs_sid': gs_sid,\n","                    'domain_labels': domain_labels\n","                })\n","        return useful_info_ext_list\n","        \n","    def _search_name_list_expand(self, scholar_list, simple=True):\n","        \"\"\"Expand the name_list to full_name_list.\"\"\"\n","        new_scholar_list = []\n","        for scholar in scholar_list:\n","            if 'gs_sid' in scholar:\n","                url = self._gsidsearch.format(scholar['gs_sid'])\n","                self.driver.get(url)\n","                scholar_dict = self._search_gsid_helper(self.driver, url, simple=simple)\n","                if scholar_dict is not None:\n","                    new_scholar_list.append(scholar_dict)\n","                time.sleep(5)\n","        return new_scholar_list"]},{"cell_type":"markdown","metadata":{"id":"rt7MW4cCP-Jo"},"source":["### GoogleSearch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kEFrgb4FPupy"},"outputs":[],"source":["# import re\n","# import time\n","# from typing import Union\n","# # from selenium import webdriver\n","# # from selenium.webdriver.chrome.options import ChromiumOptions\n","# from selenium.webdriver.chromium.webdriver import ChromiumDriver\n","# from selenium.webdriver.common.by import By\n","# from selenium.webdriver.remote.errorhandler import NoSuchElementException\n","# # from webdriver_manager.chrome import ChromeDriverManager\n","\n","# from selenium import webdriver\n","# from selenium.webdriver.chrome.options import Options\n","\n","# # url = \"https://github.com/googlecolab/colabtools/issues/3347\" \n","\n","# class GoogleSearch():\n","#     \"\"\"Base class for performing web search on Google using REST API.\"\"\"\n","#     def __init__(self, driver_path):\n","#         self.setup_webdriver(driver_path)\n","    \n","#     def setup_webdriver(self, driver_path):\n","#         \"\"\"Setup the webdriver object.\"\"\"\n","\n","#         options = Options()\n","#         options.add_argument(\"--headless\")\n","#         options.add_argument(\"--no-sandbox\")\n","#         options.headless = True\n","#         # options = ChromiumOptions()\n","#         options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n","#         options.add_experimental_option('useAutomationExtension', False)\n","#         # options.add_argument('--headless')\n","#         # options.add_argument('--no-sandbox')\n","#         options.add_argument('--disable-dev-shm-usage')\n","\n","#         self.driver = webdriver.Chrome(driver_path, options=options)\n","\n","#         # self.driver = webdriver.Chrome(driver_path, options=options)\n","#         # # self.driver = webdriver.Chrome(ChromeDriverManager().install())\n","\n","#         # self.driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n","#         #     \"source\": \"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\"\n","#         # })\n","\n","\n","# class ScholarGsSearch(GoogleSearch):\n","#     \"\"\"Class that handling searching on Google Scholar webpage using REST GET API.\"\"\"\n","#     def __init__(self, driver_path):\n","#         super().__init__(driver_path)\n","#         self._authsearch = 'https://scholar.google.com/citations?hl=en&view_op=search_authors&mauthors={0}'\n","#         self._gsidsearch = 'https://scholar.google.com/citations?hl=en&user={0}'\n","#         self.print_true = True\n","#         # print(\"ScholarGsSearch initiated\")\n","    \n","#     def change_name(self, name):\n","#         new_name = name[1:].split('_')\n","#         new_name[-1] = re.sub(r'[0-9]+', '', new_name[-1])\n","#         new_name = ' '.join(new_name)\n","#         return new_name\n","\n","#     def search_gsid(self, gs_sid: str, simple: bool = True):\n","#         \"\"\"Search scholar on Google Scholar based given gs_sid.\n","        \n","#         Parameters\n","#         ----------\n","#         gs_sid : google scholar sid\n","#         simple : whether return simple information without paper list.\n","\n","#         Returns\n","#         -------\n","#         scholar_dict_list : a list of dicts of responses.\n","        \n","#         \"\"\"\n","#         url = self._gsidsearch.format(gs_sid)\n","#         self.search_gs_url(url, simple=simple)\n","        \n","#     def search_gs_url(self, url: str, simple: bool = True):\n","#         self.driver.get(url)\n","#         scholar_dict = self._search_gsid_helper(self.driver, url, simple=simple)\n","#         time.sleep(5)\n","#         if scholar_dict is not None:\n","            \n","#             return [scholar_dict]\n","#         else:\n","#             if self.print_true:\n","#                 print('[Info] No scholars found given gs_sid in search_gs.')\n","#             return []\n","\n","        \n","#     def _search_gsid_helper(self, driver: ChromiumDriver, url: str, simple: bool = True):\n","#         \"\"\"Helper function for search_gsid.\"\"\"\n","\n","#         def get_single_author(element):\n","#             li=[]\n","#             li.append(element.find_elements(By.TAG_NAME, \"a\")[0].get_attribute('href'))\n","#             li.append(element.find_elements(By.TAG_NAME, \"a\")[0].get_attribute('textContent'))\n","#             for i in element.find_elements(By.CLASS_NAME, \"gsc_rsb_a_ext\"):\n","#                 li.append(i.get_attribute('textContent'))\n","#             return li\n","\n","#         html_first_class = driver.find_elements(By.CLASS_NAME, \"gsc_g_hist_wrp\")\n","#         if (len(html_first_class)==0):\n","#             if self.print_true:\n","#                 print(\"[Info] len(html_first_class)==0\")\n","#             return None\n","#         idx_list = html_first_class[0].find_elements(By.CLASS_NAME, \"gsc_md_hist_b\")[0]\n","#         years =  [i.get_attribute('textContent') for i in idx_list.find_elements(By.CLASS_NAME, \"gsc_g_t\")]\n","#         cites =  [i.get_attribute('innerHTML') for i in idx_list.find_elements(By.CLASS_NAME, \"gsc_g_al\")]\n","#         rsb = driver.find_elements(By.CLASS_NAME, \"gsc_rsb\")[0]\n","#         Citations_table=[i.get_attribute('textContent') for i in  rsb.find_elements(By.CLASS_NAME, \"gsc_rsb_std\")]\n","#         Co_authors = rsb.find_elements(By.CLASS_NAME, \"gsc_rsb_a\")\n","#         if len(Co_authors) == 0:\n","#             Co_authors = None\n","#         else:\n","#             Co_authors = [get_single_author(i) for i in rsb.find_element(By.CLASS_NAME, \"gsc_rsb_a\").find_elements(By.CLASS_NAME, \"gsc_rsb_a_desc\")]\n","\n","#         Researcher = {\"url\": url}\n","#         gs_sid = None\n","#         if 'user=' in url:\n","#             tmp_gs_sid = url.split('user=', 1)[1]\n","#             if len(tmp_gs_sid) >= 12:\n","#                 gs_sid = tmp_gs_sid[:12]\n","#         # gs_sid\n","#         Researcher['gs_sid'] = gs_sid\n","#         # coauthors that are listed at the lower right of the profile page\n","#         Researcher[\"coauthors\"] = Co_authors\n","#         # citation table\n","#         Researcher[\"citation_table\"] = [Citations_table[0], Citations_table[2]]\n","#         # time series citations\n","#         Researcher[\"cites\"] = {\"years\":years, \"cites\":cites}\n","#         # name\n","#         nameList = driver.find_elements(By.ID, \"gsc_prf_in\")\n","#         if (len(nameList) != 1):\n","#             if self.print_true:\n","#                 print(\"len(nameList)!=1\")\n","#             return None\n","#         Researcher[\"name\"] = nameList[0].text\n","#         # organization\n","#         infoList = driver.find_elements(By.CLASS_NAME, 'gsc_prf_il')\n","#         Researcher['organization'] = infoList[0].get_attribute('textContent')\n","#         # homepage\n","#         homepage_url = infoList[1].find_elements(By.TAG_NAME, 'a')\n","#         if len(homepage_url) == 0:\n","#             Researcher['homepage_url'] = None\n","#         else:\n","#             Researcher['homepage_url'] = homepage_url[0].get_attribute('href')\n","#         # email address\n","#         email_str_match = re.search(r'[\\w-]+\\.[\\w.-]+', infoList[1].text)\n","#         if email_str_match is not None:\n","#             Researcher['email_info'] = email_str_match.group(0)\n","#         # domain labels\n","#         Researcher['domain_labels'] = [i.get_attribute('textContent').strip().lower() for i in infoList[2].find_elements(By.CLASS_NAME, 'gsc_prf_inta')]\n","#         # if not simple, get paper lists\n","#         if not simple:\n","#             button = driver.find_elements(By.CLASS_NAME, 'gs_btnPD')\n","#             if (len(button) != 1):\n","#                 if self.print_true:\n","#                     print(\"len(button)!=1\")\n","#                 return None\n","#             while (button[0].is_enabled()):\n","#                 while (button[0].is_enabled()):\n","#                     while (button[0].is_enabled()):\n","#                         button[0].click()\n","#                         time.sleep(5)\n","#                     time.sleep(1)\n","#                 time.sleep(2)\n","#             papers = []\n","#             items = driver.find_elements(By.CLASS_NAME, 'gsc_a_tr')\n","#             for i in items:\n","#                 item = i.find_element(By.CLASS_NAME, 'gsc_a_at')\n","#                 url = item.get_attribute(\"href\")\n","#                 paper_info=[j.text for j in i.find_elements(By.CLASS_NAME, 'gs_gray')]\n","#                 cite = i.find_element(By.CLASS_NAME, 'gsc_a_ac')\n","#                 year = i.find_element(By.CLASS_NAME, 'gsc_a_y').find_element(By.CLASS_NAME, \"gsc_a_h\").text\n","#                 papers.append([url, item.text, \n","#                                 paper_info,\n","#                             cite.text, cite.get_attribute(\"href\"),\n","#                             year])\n","#             Researcher[\"papers\"] = papers\n","\n","#         def generate_single_coauthor(element):\n","#             coauthor_dict = {\n","#                 \"name\":element.find_elements(By.CLASS_NAME, 'gs_ai_name')[0].get_attribute('textContent'),\n","#                 \"url\":element.find_elements(By.CLASS_NAME, 'gs_ai_pho')[0].get_attribute('href'),\n","#                 \"description\":element.get_attribute('innerHTML'),\n","#             }\n","#             return coauthor_dict\n","#         extra_coauthors = driver.find_elements(By.CLASS_NAME, \"gsc_ucoar\")\n","#         Researcher['extra_co_authors'] = [generate_single_coauthor(i) for i in extra_coauthors]\n","#         return Researcher\n","\n","#     def search_name(self, name: Union[str, list], query_dict: dict = None, top_n=3, simple=True):\n","#         \"\"\"Search on Google Scholar webpage given name.\n","        \n","#         Parameters\n","#         ----------\n","#         name : name of the scholar.\n","#         query_dict : a dict containing information of the scholar.\n","#         top_n : select <top_n> candidates.\n","#         simple : whether return simple information without paper list.\n","\n","#         Returns\n","#         -------\n","#         resp : list of candidate scholars, empty if no candidates are found.\n","\n","#         \"\"\"\n","#         if type(name) is list:\n","#             # current case\n","#             name_list = [name[0], name[-1]]\n","#             name = f'{name[0]} {name[-1]}' \n","#         elif type(name) is str:\n","#             name_list = name.split(' ')\n","#         else:\n","#             raise TypeError('Argument \"name\" passed to ScholarGsSearch.search_name has the wrong type.')\n","#         url_fragment = f'{name} '\n","#         if query_dict is not None:\n","#             # first try (name, email_suffix, position, organization) as url\n","#             keyword_list = generate_or_keyword_list(query_dict)[0]\n","#             url_fragment_new = url_fragment\n","#             # if 'email_suffix' in keyword_list:\n","#             #     url_fragment_new = url_fragment_new + keyword_list['email_suffix'] + ' '\n","#             # if 'position' in keyword_list:\n","#             #     url_fragment_new = url_fragment_new + keyword_list['position'] + ' '\n","#             # if 'organization' in keyword_list:\n","#             #     url_fragment_new = url_fragment_new + keyword_list['organization'] + ' '\n","\n","#             # url = self._authsearch.format(url_fragment_new)\n","#             # self.driver.get(url)\n","#             # time.sleep(5)\n","#             # scholar_list = self._search_name_helper(self.driver, name_list)\n","#             # if len(scholar_list) > 0:\n","#             #     if wo_full:\n","#             #         return scholar_list\n","#             #     else:\n","#             #         return self._search_name_list_expand(scholar_list, simple=simple)\n","            \n","#             # second try (name, email_suffix)\n","#             if 'email_suffix' in keyword_list:\n","#                 url_fragment_new = url_fragment + keyword_list['email_suffix'] # + ' '\n","#             url = self._authsearch.format(url_fragment_new)\n","#             self.driver.get(url)\n","#             time.sleep(5)\n","#             scholar_list = self._search_name_helper(self.driver, name_list)\n","#             # return scholar_list\n","#             if len(scholar_list) > 0:\n","#                 if self.print_true:\n","#                     print(f'[Info] Find {len(scholar_list)} scholars using query without gs_sid in step 1.')\n","#                 # return self._search_name_list_expand(scholar_list, simple=simple)\n","#                 return scholar_list\n","        \n","#             # third try (name, position)\n","#             if 'position' in keyword_list:\n","#                 url_fragment_new = url_fragment + keyword_list['position'] # + ' '\n","#             url = self._authsearch.format(url_fragment_new)\n","#             self.driver.get(url)\n","#             time.sleep(5)\n","#             scholar_list = self._search_name_helper(self.driver, name_list)\n","#             # return scholar_list\n","#             if len(scholar_list) > 0:\n","#                 if self.print_true:\n","#                     print(f'[Info] Find {len(scholar_list)} scholars using query without gs_sid in step 2.')\n","#                 # return self._search_name_list_expand(scholar_list, simple=simple)\n","#                 return scholar_list\n","\n","#             # fourth try (name, organization)\n","#             if 'organization' in keyword_list:\n","#                 url_fragment_new = url_fragment + keyword_list['organization'] # + ' '\n","#             url = self._authsearch.format(url_fragment_new)\n","#             self.driver.get(url)\n","#             time.sleep(5)\n","#             scholar_list = self._search_name_helper(self.driver, name_list)\n","#             # return scholar_list\n","#             if len(scholar_list) > 0:\n","#                 if self.print_true:\n","#                     print(f'[Info] Find {len(scholar_list)} scholars using query without gs_sid in step 3.')\n","#                 # return self._search_name_list_expand(scholar_list, simple=simple)\n","#                 return scholar_list\n","\n","#         # finally, only search (name: firstname and lastname). If only one response returns, mark it as candidate\n","#         url = self._authsearch.format(url_fragment)\n","#         self.driver.get(url)\n","#         time.sleep(5)\n","#         scholar_list = self._search_name_helper(self.driver, name_list)\n","#         if len(scholar_list) > 0:\n","#         # if len(scholar_list) > 0 and len(scholar_list) <= top_n:\n","#             if self.print_true:\n","#                 print(f'[Info] Find {len(scholar_list)} scholars using query without gs_sid in step 4.')\n","#             # return self._search_name_list_expand(scholar_list, simple=simple)\n","#             return scholar_list\n","        \n","#         return []\n","\n","#     def _search_name_helper(self, driver, name_list):\n","#         \"\"\"Helper function of <self.search_name()>.\"\"\"\n","#         # iterate over searched list, find dicts that contains the name (including)\n","#         useful_info_list = driver.find_elements(By.CLASS_NAME, 'gs_ai_t')\n","#         useful_info_ext_list = []\n","#         if len(useful_info_list) != 0:\n","#             for scholar_webdriver in useful_info_list:\n","#                 name = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_name').get_attribute('textContent').strip()\n","#                 # check whether name is correct\n","#                 not_a_candidate = False\n","#                 for name_fragment in name_list:\n","#                     if name_fragment.lower() not in name.lower():\n","#                         not_a_candidate = True\n","#                         break\n","#                 if not_a_candidate:\n","#                     continue\n","                \n","#                 # grab all the other information\n","#                 pos_org = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_aff').get_attribute('textContent').strip()\n","#                 email_str = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_eml').get_attribute('textContent').strip()\n","#                 cite = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_cby').get_attribute('textContent').strip()\n","#                 url = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_name').find_element(By.TAG_NAME, 'a').get_attribute('href').strip()\n","#                 domain_labels = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_int').find_elements(By.CLASS_NAME, 'gs_ai_ont_int')\n","#                 for idx, domain in enumerate(domain_labels):\n","#                     domain_labels[idx] = domain.get_attribute('textContent').strip().lower()\n","\n","#                 # continue processing\n","#                 gs_sid = None\n","#                 if 'user=' in url:\n","#                     tmp_gs_sid = url.split('user=', 1)[1]\n","#                     if len(tmp_gs_sid) >= 12:\n","#                         gs_sid = tmp_gs_sid[:12]\n","\n","#                 if email_str is not None and email_str != '':\n","#                     match = re.search(r'[\\w-]+\\.[\\w.-]+', email_str)\n","#                     email_str = match.group(0)\n","\n","#                 cites = [int(s) for s in cite.split() if s.isdigit()]\n","#                 useful_info_ext_list.append({\n","#                     'name': name,\n","#                     'pos_org': pos_org,\n","#                     'email': email_str,\n","#                     'cite': cites[0] if len(cites)>0 else None,\n","#                     'url': url,\n","#                     'gs_sid': gs_sid,\n","#                     'domain_labels': domain_labels\n","#                 })\n","#         return useful_info_ext_list\n","        \n","#     def _search_name_list_expand(self, scholar_list, simple=True):\n","#         \"\"\"Expand the name_list to full_name_list.\"\"\"\n","#         new_scholar_list = []\n","#         for scholar in scholar_list:\n","#             if 'gs_sid' in scholar:\n","#                 url = self._gsidsearch.format(scholar['gs_sid'])\n","#                 self.driver.get(url)\n","#                 scholar_dict = self._search_gsid_helper(self.driver, url, simple=simple)\n","#                 if scholar_dict is not None:\n","#                     new_scholar_list.append(scholar_dict)\n","#                 time.sleep(5)\n","#         return new_scholar_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-F3mp0N7QIro"},"outputs":[],"source":["from difflib import SequenceMatcher\n","\n","def get_str_similarity(a: str, b: str) -> float:\n","    \"\"\"Calculate the similarity of two strings and return a similarity ratio.\"\"\"\n","    return SequenceMatcher(None, a, b).ratio()"]},{"cell_type":"markdown","metadata":{"id":"43Ozad0mQCKZ"},"source":["### ScholarSearch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DVB-Cz04PNOv"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import pickle\n","import json\n","import typing\n","from typing import List, Union\n","import os\n","import re\n","import time\n","import sys\n","import requests\n","from bs4 import BeautifulSoup\n","\n","\n","class ScholarSearch():\n","    \"\"\"A class that handles searching over Google Scholar profiles and the 78k AI scholar dataset.\"\"\"\n","    def __init__(self):\n","        # attributes\n","        self.similarity_ratio = 0.8\n","        self.driver_path = '/usr/bin/chromedriver'\n","    \n","    def setup(self):\n","        # self.get_profiles(['review_data/area_chair_id_to_profile.json', 'review_data/reviewer_id_to_profile.json'])\n","        # self.get_profiles(None)\n","        self.search_78k = Scholar78kSearch()\n","        self.search_gs = ScholarGsSearch(self.driver_path)\n","\n","    def reset(self):\n","        pass\n","\n","    def get_profiles(self, filepath_list: List[str] = None) -> None:\n","        \"\"\"In case that you want to get responses of a list of scholars, \n","        the method is implemented for you to load (could be multiple) json data files.\n","\n","        Parameters\n","        ----------\n","        filepath_list : list of json data filepaths to load.\n","\n","        \"\"\"\n","        if filepath_list is None:\n","            return\n","        # set of json data dicts\n","        self.profile = {}\n","        for filepath in filepath_list:\n","            with open(filepath) as file:\n","                profile = json.load(file)\n","                self.profile.update(profile)\n","        # number of unique json data dicts in total\n","        print(f'Number of unique json data dicts in total: {len(self.profile)}')\n","\n","    def get_scholar(\n","        self,\n","        query: Union[str, dict],\n","        field: List[str] = None,\n","        simple: bool = True,\n","        top_n: int = 3,\n","        print_true: bool = True) -> List[dict]:\n","        \"\"\"Get up to <top_n> relevant candidate scholars by searching over Google Scholar profiles and the 78k AI scholar dataset.\n","        \n","        Parameters\n","        ----------\n","        query : a query containing the known scholar information.\n","        field : a list of fields wants to return. If not given, by default full information will be returned.\n","        simple : whether return simple information without paper list. This works only if the argument <field> is not specified.\n","        top_n : return at most <top_n> scholars if the result is not considered as determined.\n","        print_true : print info / debug info of the search process.\n","\n","        Returns\n","        -------\n","        resp : list of candidate scholars, empty if no candidates are found.\n","\n","        \"\"\"\n","\n","        self.search_78k.simple = simple\n","        self.search_78k.print_true = print_true\n","        self.search_gs.print_true = print_true\n","        self.print_true = print_true\n","        self.reset()\n","\n","        scholar_cnt = 0\n","        if type(query) is dict:\n","            # query is dict\n","            resp = self.search_dict(query, simple=simple, top_n=top_n)\n","        elif type(query) is str:\n","            # query is str\n","            resp = self.search_name(query, simple=simple, top_n=top_n)                \n","        else:\n","            raise TypeError(f'[Error] The argument \"query\" must be str or dict, not {type(query)}.')\n","\n","        \n","        # select specific features\n","        if field is not None:\n","            resp_final = []\n","            for resp_item in resp:\n","                resp_dict = {}\n","                for field_item in field:\n","                    if field_item not in resp_item:\n","                        raise KeyError(f'The key {field_item} is not in the response dictionary')\n","                    \n","                    resp_dict[field_item] = resp_item[field_item]\n","                resp_dict['gs_sid'] = resp_item['gs_sid']\n","                resp_dict['url'] = resp_item['url']\n","                resp_dict['citation_table'] = resp_item['citation_table']\n","                resp_final.append(resp_dict)\n","            if print_true:\n","                scholar_cnt = len(resp_final)\n","                if scholar_cnt == 1:\n","                    print(f'[Info] In total 1 scholar is found:')\n","                else:\n","                    print(f'[Info] In total {scholar_cnt} scholars are found:')\n","                resp_str = json.dumps(resp_final, indent=2)\n","                print(resp_str)\n","            return resp_final\n","        else:\n","            if print_true:\n","                scholar_cnt = len(resp)\n","                if scholar_cnt == 1:\n","                    print(f'[Info] In total 1 scholar is found:')\n","                else:\n","                    print(f'[Info] In total {scholar_cnt} scholars are found:')\n","                resp_str = json.dumps(resp, indent=2)\n","                print(resp_str)\n","            return resp\n","    \n","    def search_name(self, name: str, simple: bool = True, top_n: int = 3, from_dict: bool = False, query_dict: dict = None) -> List[dict]:\n","        \"\"\"Search gs profile given name or OpenReview id.\n","        \n","        Parameters\n","        ----------\n","        name : the name of the scholar ([first_name last_name]).\n","        simple : whether return simple information without paper list. This works only if the argument <field> is not specified.\n","        top_n : return at most <top_n> scholars if the result is not considered as determined.\n","        from_dict : default = False. Should be true only if using <get_scholar()> class method.\n","        query_dict : default = None. Should be a dict only if using <get_scholar()> class method.\n","\n","        Returns\n","        -------\n","        resp : list of candidate scholars, empty if no candidates are found.\n","        \"\"\"\n","\n","        self.search_78k.simple = simple\n","        name = name.strip()\n","        dict = None\n","        real_name = True\n","        # OpenReview id\n","        if ' ' not in name and name[0] == '~':\n","            # search over chair id\n","            if name in self.profile:\n","                dict = self.profile[name]\n","            # crawl http api response\n","            if dict is not None and not from_dict:\n","                # name\n","                real_name = False\n","                resp = self.search_dict(dict, simple=simple, top_n=top_n)\n","            else:\n","                # get real name\n","                or_name = name # string\n","                name = name[1:].split('_')\n","                name[-1] = re.sub(r'[0-9]+', '', name[-1]) # list\n","                # name = ' '.join(name) # e.g., Rachel K. E. Bellamy\n","        else:\n","            or_name = name.split(' ') # list\n","            # name string\n","        if real_name:\n","            if from_dict:\n","                print('Not find by gs_sid, search from_dict')\n","                # it inputs a real name (firstname, lastname)\n","                resp = self.search_78k.search_name(name, query_dict)\n","                resp_gs = self.search_gs.search_name(name, query_dict, top_n=top_n, simple=simple)\n","                resp = self.select_final_cands(resp, top_n, query_dict=query_dict, resp_gs_prop={'resp_gs': resp_gs})\n","            else:\n","                # or_resp = self.get_or_scholars(or_name)\n","                # TODO: resp_gs for only searching name is not implemented\n","                # resp = self.select_final_cands(resp, or_resp, top_n, simple=simple)\n","                resp = self.search_78k.search_name(name)\n","                resp_gs = self.search_gs.search_name(name, query_dict=None, top_n=top_n, simple=simple)\n","                resp = self.select_final_cands(resp, top_n, query_dict=None, resp_gs_prop={'resp_gs': resp_gs})\n","        return resp\n","    \n","\n","    def get_or_scholars(self, or_name: Union[str, list]):\n","        \"\"\"Get OpenReview candidate scholars list by name through http api response.\"\"\"\n","        # format the name list to get OpenReview rest api response\n","        if type(or_name) is list:\n","            or_name_list = []\n","            if len(or_name) >= 2:\n","                id_list = []\n","                for idx, name_part in enumerate(or_name):\n","                    if idx == 0 or idx == len(or_name) - 1:\n","                        id_list.append(name_part.capitalize())\n","                    else:\n","                        if len(name_part) > 1:\n","                            id_list.append(f'{name_part[0].upper()}.') # middle name in abbreviate form\n","                        else:\n","                            id_list.append(name_part.upper())\n","                if len(id_list) == 2:\n","                    or_name_list.append(f'~{id_list[0]}_{id_list[-1]}')\n","                elif len(id_list) > 2:\n","                    or_name_list.append(f'~{id_list[0]}_{id_list[-1]}')\n","                    tmp_str = '_'.join(id_list)\n","                    or_name_list.append(f'~{tmp_str}')\n","            else:\n","                raise ValueError('Argument \"or_name\" passed to get_or_scholars is not a valid name list.')\n","        elif type(or_name) is str:\n","            or_name_list = [or_name]\n","        else:\n","            raise TypeError(f'Argument \"or_name\" passed to get_or_scholars has the wrong type.')\n","        del or_name\n","\n","        # get request response\n","        go_ahead = True\n","        resp_list = []\n","        for name in or_name_list:\n","            if name[-1].isnumeric():\n","                name_cur = name\n","                go_ahead = False\n","                name_cur_cnt = 1\n","            else:\n","                name_cur_cnt = 1\n","                name_cur = f'{name}{name_cur_cnt}'\n","\n","            # set accumulative count\n","            acc_cnt = 0\n","            while acc_cnt <= 1:\n","                response = requests.get(f'https://openreview.net/profile?id={name_cur}')\n","                time.sleep(1)\n","\n","                if not response.ok:\n","                    acc_cnt += 1\n","                else:\n","                    soup = BeautifulSoup(response.content.decode('utf-8'), 'html.parser')\n","                    resp_list.append(json.loads(soup.find_all('script', id=\"__NEXT_DATA__\")[0].string))\n","                name_cur_cnt += 1\n","                name_cur = f'{name}{name_cur_cnt}'\n","                if not go_ahead:\n","                    break\n","        if self.print_true:\n","            if len(resp_list) != 1:\n","                print(f'[Info] Found {len(resp_list)} scholars using OpenReview REST API.')\n","            else:\n","                print(f'[Info] Found 1 scholar using OpenReview REST API.')\n","        return resp_list \n","        # NOTE: the dict in this list is in a different format than the dict from OpenReview dataset.\n","\n","    def select_final_cands(self, resp: List[dict], top_n: int, query_dict: dict = None, resp_gs_prop: dict = None, simple: bool = True) -> List[dict]:\n","        \"\"\"Select final candidates according to the response from OpenReview and 78k data.\n","        \n","        Parameters\n","        ----------\n","        resp : response from 78k dataset.\n","        or_resp : prepare the necessary key-value pairs to help filtering.\n","        top_n : return at most <top_n> scholars if the result is not considered as determined.\n","        query_dict : default = None. Should be a dict only if using <get_scholar()> class method.\n","        resp_gs_prop : dict containing the response from Google Scholar webpage.\n","        simple : whether return simple information without paper list. This works only if the argument <field> is not specified.\n","\n","        Returns\n","        -------\n","        resp : list of candidate scholars, empty if no candidates are found.\n","        \n","        \"\"\"\n","        # get useful data from or_resp\n","        if query_dict is not None:\n","            or_keyword_list = generate_or_keyword_list(query_dict)\n","\n","        # merge resp with resp_gs\n","        if resp_gs_prop is not None:\n","            resp_gs = resp_gs_prop['resp_gs']\n","            # if there are one candidate from google scholar pages, we throw out resp from 78k data.\n","            if len(resp_gs) == 1:\n","                resp = []\n","            # iterate over resp_gs\n","            for resp_gs_item in resp_gs:\n","                find_flag = False\n","                # gs_sid\n","                for resp_item in resp:\n","                    if resp_gs_item['gs_sid'] == resp_item['gs_sid']:\n","                        find_flag = True\n","                        break\n","                if find_flag:\n","                    continue\n","                # construct new prep\n","                # generate full dict\n","                self.search_gs.driver.get(resp_gs_item['url'])\n","                time.sleep(5)\n","                if query_dict is not None or (query_dict is None and len(resp) <= top_n):\n","                    resp_gs_full_item = self.search_gs._search_gsid_helper(self.search_gs.driver, resp_gs_item['url'], simple=simple)\n","                    if resp_gs_full_item is not None:\n","                        resp.append(resp_gs_full_item)\n","        \n","        if query_dict is None:\n","            return resp[:top_n]\n","\n","        # calculate rankings\n","        rank = {}\n","        for idx_cand, cand in enumerate(resp):\n","            rank[idx_cand] = []\n","            gs_sid_flag = 0\n","            cnt_true = [0] * len(or_keyword_list) \n","            cnt_all = 0\n","            cnt_true_rel = [0] * len(or_keyword_list) \n","            cnt_all_rel = 0\n","            for idx_or_scholar, or_scholar in enumerate(or_keyword_list):\n","                # gs_sid\n","                if 'gs_sid' in cand:\n","                    if cand['gs_sid'] == or_scholar['gs_sid']: \n","                        gs_sid_flag = 1\n","\n","                # domain_labels\n","                if cand['domain_labels'] is not None:\n","                    for cand_domain_tag in cand['domain_labels']:\n","                        cnt_all += 1\n","                        for or_domain_tag in or_scholar['domain_labels']:\n","                            if get_str_similarity(cand_domain_tag, or_domain_tag) >= self.similarity_ratio:\n","                                cnt_true[idx_or_scholar] += 1\n","                \n","                \n","                # relations\n","                cnt_all_rel = 0\n","                # print(cand)\n","                if cand['coauthors'] is not None:\n","                    for cand_coauth in cand['coauthors']:\n","                        cnt_all_rel += 1\n","                        for or_coauth in or_scholar['coauthors']:\n","                            if get_str_similarity(or_coauth, cand_coauth[1]) >= self.similarity_ratio:\n","                                cnt_true_rel[idx_or_scholar] += 1\n","                \n","            # get the rank list\n","            # gs_sid\n","            if gs_sid_flag:\n","                rank[idx_cand].append(1)\n","            else:\n","                rank[idx_cand].append(0)\n","            \n","            # domain_labels\n","            for i in range(len(cnt_true)):\n","                if cnt_all == 0:\n","                    cnt_true[i] = 0\n","                else:\n","                    cnt_true[i] = cnt_true[i] / cnt_all\n","            rank[idx_cand].append(max(cnt_true))\n","\n","            # relations\n","            for i in range(len(cnt_true_rel)):\n","                if cnt_all_rel == 0:\n","                    cnt_true_rel[i] = 0\n","                else:\n","                    cnt_true_rel[i] = cnt_true_rel[i] / cnt_all_rel\n","            rank[idx_cand].append(max(cnt_true_rel))\n","        \n","        # select final candidate\n","        final_idx = []\n","        for rank_idx in rank:\n","            if rank[rank_idx][0] == 1:\n","                final_idx.append(rank_idx)\n","        \n","        # TODO: or we can set weights to (relations, domain_tags) to rank the scholar candidates\n","        if len(final_idx) < top_n:\n","            domain_tag_rank = []\n","            relation_rank = []\n","            for rank_idx in sorted(rank.keys()):\n","                # print(rank_idx)\n","                domain_tag_rank.append(rank[rank_idx][1])\n","                relation_rank.append(rank[rank_idx][2])\n","            # print(domain_tag_rank, relation_rank)\n","            domain_tag_idxes = np.argsort(domain_tag_rank)[::-1]\n","            relation_idxes = np.argsort(relation_rank)[::-1]\n","            for idx in relation_idxes:\n","                if relation_rank[idx] == 0:\n","                    break\n","                if len(final_idx) < top_n:\n","                    if idx not in final_idx:\n","                        final_idx.append(idx)\n","                else:\n","                    break\n","            for idx in domain_tag_idxes:\n","                if domain_tag_rank[idx] == 0:\n","                    break\n","                if len(final_idx) < top_n:\n","                    if idx not in final_idx:\n","                        final_idx.append(idx)\n","                else:\n","                    break\n","            if len(final_idx) == 0 and len(rank.keys()) > 0:\n","                    for rank_idx in sorted(rank.keys()):\n","                        if len(final_idx) >= top_n:\n","                            break\n","                        else:\n","                            final_idx.append(rank_idx)\n","        # print(resp)\n","        # print(or_keyword_list)\n","        # print(rank)\n","        # print(final_idx)\n","        resp = [resp[i] for i in final_idx]\n","        return resp\n","\n","    def search_dict(self, query_dict: dict, simple: bool = True, top_n: int = 3):\n","        \"\"\"Search candidates given a dictionary.\n","        \n","        Parameters\n","        ----------\n","        query_dict : default = None. Should be a dict only if using <get_scholar()> class method.\n","        simple : whether return simple information without paper list. This works only if the argument <field> is not specified.\n","        top_n : return at most <top_n> scholars if the result is not considered as determined.\n","\n","        Returns\n","        -------\n","        resp : list of candidate scholars, empty if no candidates are found.\n","\n","        \"\"\"\n","        self.search_78k.simple = simple\n","        # gs_sid\n","        if 'gscholar' in query_dict['profile']['content'] and 'user=' in query_dict['profile']['content']['gscholar']:\n","            tmp_gs_sid = query_dict['profile']['content']['gscholar'].split('user=', 1)[1]\n","            if len(tmp_gs_sid) >= 12:\n","                gs_sid = tmp_gs_sid[:12]\n","                name_df = self.search_78k.df.loc[self.search_78k.df['gs_sid'] == gs_sid].copy()\n","                if name_df.shape[0] != 0:\n","                    print(f'[Info] Found a scholar using 78k gs_sid')\n","                    return self.search_78k._deal_with_simple(name_df)\n","                else:\n","                    print(f'[Info] Found a scholar using query dict gs_sid')\n","                    resp = self.search_gs.search_gsid(gs_sid, simple=simple)\n","                    if len(resp) > 0:\n","                        return resp\n","                    \n","        \n","        # search_name\n","        return self.search_name(query_dict['profile']['id'], simple=simple, top_n=top_n, from_dict=True, query_dict=query_dict)"]},{"cell_type":"markdown","metadata":{"id":"fTs792JvydZW"},"source":["###TwitterSearch - original"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AaDSu28Vyao0"},"outputs":[],"source":["import re\n","import time\n","from typing import Union\n","from collections import defaultdict\n","from selenium import webdriver\n","from selenium.webdriver.chromium.webdriver import ChromiumDriver\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.remote.errorhandler import WebDriverException\n","from bs4 import BeautifulSoup\n","\n","url_search_dict = {\n","    'google': 'https://www.google.com/search?q={0}'\n","}\n","\n","\n","def get_search_result(**kwargs):\n","    return kwargs\n","\n","\n","class TwitterSearch(GoogleSearch):\n","    \"\"\"Class that handling searching on Google search bar using REST API.\"\"\"\n","\n","    def __init__(self, driver_path):\n","        super().__init__(driver_path)\n","        self._urlsearch = url_search_dict['google']\n","        self.print_true = False #NOTE: should be set by users\n","        # initialize scholar_search object\n","        self.scholar_search = ScholarSearch()\n","        self.scholar_search.setup()\n","\n","    def search_scholar(self, str_type: str, term: str):\n","        \"\"\"\n","        NOTE: now the only allowed str_type is name\n","        \"\"\"\n","        if str_type == 'name':\n","            scholar_search_result = self.scholar_search.get_scholar(query=term, simple=True, top_n=1, print_true=False)\n","            branch_type = None\n","            if len(scholar_search_result) == 0:\n","                # directly search\n","                url_fragment = self._urlsearch.format(f'{term} \"twitter\"')\n","                result = self._search_google_helper(url_fragment)\n","                twitter_ids = self.filter_result(result, term, web_source='google')\n","                branch_type = 'directly search'\n","            else:\n","                twitter_ids = None\n","                # try directly get twitter account through homepage\n","                if 'homepage_url' in scholar_search_result[0] and scholar_search_result[0]['homepage_url'] is not None:\n","                    twitter_ids = self._search_twitter_from_homepage(scholar_search_result[0]['homepage_url'], name=term, name_from_gs=scholar_search_result[0][\"name\"])\n","                if twitter_ids is not None:\n","                    branch_type = 'homepage'\n","\n","                # then try (google_name email_suffix \"twitter\")\n","                if \"email_info\" in scholar_search_result[0] and scholar_search_result[0][\"email_info\"] != '' and twitter_ids is None:\n","                    url_fragment = self._urlsearch.format(f'{scholar_search_result[0][\"name\"]} {scholar_search_result[0][\"email_info\"]} \"twitter\"')\n","                    result = self._search_google_helper(url_fragment)\n","                    twitter_ids = self.filter_result(result, term, web_source='google')\n","                if twitter_ids is not None:\n","                    branch_type = 'name + email'\n","\n","                # then try (google_name organization \"twitter\")\n","                if \"organization\" in scholar_search_result[0] and scholar_search_result[0][\"organization\"] != '' and twitter_ids is None:\n","                    url_fragment = self._urlsearch.format(f'{scholar_search_result[0][\"name\"]} {scholar_search_result[0][\"organization\"]} \"twitter\"')\n","                    result = self._search_google_helper(url_fragment)\n","                    twitter_ids = self.filter_result(result, term, web_source='google')\n","                if twitter_ids is not None:\n","                    branch_type = 'name + organization'\n","\n","                # then try (google_name \"twitter\")\n","                if twitter_ids is None:\n","                    url_fragment = self._urlsearch.format(f'{scholar_search_result[0][\"name\"]} \"twitter\"')\n","                    result = self._search_google_helper(url_fragment)\n","                    twitter_ids = self.filter_result(result, term, web_source='google')\n","                if twitter_ids is not None:\n","                    branch_type = 'name'\n","\n","\n","            print(f'[INFO] branch_type: {branch_type}')\n","            print(f'[INFO] twitter_ids: {twitter_ids}')\n","\n","            if twitter_ids is None or len(twitter_ids) == 0:\n","                return None, twitter_ids\n","            elif type(twitter_ids) == dict:\n","                twitter_ids_list = list(twitter_ids.keys())\n","                highest_occurrence = twitter_ids[twitter_ids_list[0]]\n","                candidate_list = []\n","                for item in twitter_ids.items():\n","                    if item[1] == highest_occurrence:\n","                        if self._rank_by_similarity(item[0], term, scholar_search_result[0][\"name\"]) >= 0.15:\n","                            candidate_list.append(item[0])\n","                if len(candidate_list) > 0:\n","                    return self._rank_by_similarity(candidate_list, term, scholar_search_result[0]['name'])[0], twitter_ids\n","                return None, twitter_ids\n","            else:\n","                return twitter_ids[0], twitter_ids\n","\n","            # TODO: match profile images of google_scholar/homepage with twitter profile images\n","            # result = self._search_name_helper(term)\n","            # return self.filter_result(result, term, web_source='google')\n","        elif str_type == 'gs_url':\n","            raise NotImplementedError\n","        else:\n","            raise NotImplementedError\n","\n","    def _rank_by_similarity(self, twitter_url_origin_list: Union[list, str], name: str=None, name_from_gs: str=None):\n","        # process name and name_from_gs\n","        if name is not None:\n","            name = re.sub('[0-9_\\., ]', '', name.lower())\n","        if name_from_gs is not None:\n","            name_from_gs = re.sub('[0-9_\\., ]', '', name_from_gs.lower())\n","\n","        if type(twitter_url_origin_list) == list:\n","            \n","            # else\n","            twitter_url_list = [re.sub('[0-9_\\., ]', '', item) for item in twitter_url_origin_list]\n","            twitter_url_map_dict = {re.sub('[0-9_\\., ]', '', item): item for item in twitter_url_origin_list}\n","            # rank twitter_url_origin_list\n","            if name is not None and name_from_gs is not None:\n","                twitter_url_list = sorted(twitter_url_list, key=lambda x: max(get_str_similarity(x, name), get_str_similarity(x, name_from_gs)), reverse=True)\n","            elif name is not None:\n","                twitter_url_list = sorted(twitter_url_list, key=lambda x: get_str_similarity(x, name), reverse=True)\n","            elif name_from_gs is not None:\n","                twitter_url_list = sorted(twitter_url_list, key=lambda x: get_str_similarity(x, name_from_gs), reverse=True)\n","            else:\n","                # do not consider this branch at the moment\n","                pass\n","            twitter_url_origin_list = [twitter_url_map_dict[item] for item in twitter_url_list]\n","            return twitter_url_origin_list\n","        else:\n","            twitter_url_origin_str = twitter_url_origin_list\n","            twitter_url_str = re.sub('[0-9_\\., ]', '', twitter_url_origin_str)\n","            rank = 0\n","            if name is not None and name_from_gs is not None:\n","                rank = max(get_str_similarity(twitter_url_str, name), get_str_similarity(twitter_url_origin_list, name_from_gs))\n","            elif name is not None:\n","                rank = get_str_similarity(twitter_url_str, name)\n","            elif name_from_gs is not None:\n","                rank = get_str_similarity(twitter_url_str, name_from_gs)\n","            \n","            return rank\n","\n","    def _search_twitter_from_homepage(self, homepage_url: str, name: str=None, name_from_gs: str=None):\n","        # get content of scholar homepage using chromedriver\n","        try:\n","            self.driver.get(homepage_url)\n","        except WebDriverException as e:\n","            if self.print_true:\n","                print('[DEBUG] WebDriverException while getting homepage: %s' % homepage_url)\n","                print(e)\n","        time.sleep(3)\n","\n","        page = self.driver.page_source\n","        soup = BeautifulSoup(page, \"html.parser\")\n","        twitter_url_origin_list = list(set([re.findall('twitter.com/([^\\/?]+)', item['href'])[0]\n","            for item in soup.find_all(\n","                href=re.compile('twitter.com/([^\\/?]+)'))]))\n","        print(soup.find_all(\n","                href=re.compile('twitter.com/([^\\/?]+)')))\n","        # if there are no candidates for twitter account url, return None\n","        if len(twitter_url_origin_list) == 0:\n","            # return soup\n","            return None\n","\n","        twitter_url_origin_list = self._rank_by_similarity(twitter_url_origin_list, name=name, name_from_gs=name_from_gs)\n","\n","        if self.print_true:\n","            print(f'[DEBUG] Find a set of twitter ids on the provided homepage:\\n{twitter_url_origin_list}')\n","        \n","        # only return the highest rank twitter account id\n","        return twitter_url_origin_list\n","\n","    def _search_google_helper(self, google_url: str):\n","        self.driver.get(google_url)\n","        time.sleep(3)\n","\n","        page = self.driver.page_source\n","        soup = BeautifulSoup(page, \"html.parser\")\n","\n","        result_list = []\n","        result_block = soup.find_all('div', attrs={'class': 'g'})\n","        for result in result_block:\n","            # Find link, title, description\n","            link = result.find('a', href=True)\n","            title = result.find('h3')\n","            description_box = result.find(\n","                'div', {'style': '-webkit-line-clamp:2'})\n","            if link and title and description_box:\n","                result_list.append(get_search_result(\n","                    href=link['href'], title=title.text, description=description_box.text))\n","        if self.print_true:\n","            print(result_list)\n","        time.sleep(5)\n","        return result_list\n","        \n","\n","    def filter_result(self, result_list, term, web_source):\n","        \"\"\"\n","        web_source: google, twitter\n","        \"\"\"\n","        # sort twitter ids by occurrence frequency\n","        if web_source == 'google':\n","            twitter_id_dict = defaultdict(int)\n","            for result in result_list:\n","                if 'twitter.com/' in result['href']:\n","                    twitter_id_dict[re.findall('twitter.com/([^\\/?]+)', result['href'])[0]] += 1\n","        twitter_id_dict = dict(sorted(twitter_id_dict.items(), key=lambda item: item[1], reverse=True))\n","        # then, sort twitter ids by str similarity?\n","        # TODO\n","        # TODO: enter into twitter page to check profile information\n","        # Step 1: twitter profile vs google scholar profile\n","        # Step 2: twitter tweets: check whether google scholar domains are in twitter tweets\n","        # Step 3: twitter profile image (ask Yvonne about the performance)\n","        \n","        if len(twitter_id_dict) == 0:\n","            return None\n","        else:\n","            return twitter_id_dict\n","    \n","    def search_scholar_batch(self, name_list: list):\n","        self.result_list = []\n","        for name in name_list:\n","            self.result_list.append(self.search_scholar('name', name))\n","        return self.result_list\n","\n","    def get_scholar_twitter(self, str_type: str, term: str, only_one=True):\n","        \"\"\"\n","        Final function that search a scholar's twitter account\n","        # TODO\n","        \"\"\"\n","        raise NotImplementedError\n","        result = self.search_scholar(str_type=str_type, term=term)\n","\n","        # first, google web search: name \"twitter\", get a list of top results, and check whatever name matches exactly\n","        # if matches, then get the twitter account id, use tweepy API search of the id to get the user profile and do further check\n","\n","        # if no matches, then search by name directly using Tweepy\n","\n","        # if there are candidates, do type 1, 2, 3 check of the result\n","        #\n","\n","        # '''\n","        #     The current code and the data is on the folder /cluster/project/sachan/zhiheng/twiteer at Euler server, because of the security reason, I save the twitter key as this structure, and use get_auth.py to load the key in the file. If you need more APIs, please contact me\n","        #     {\n","        #     \"API_key\" : \"ilH6jnBJdh9HQdsufmygvUwMB\",\n","        #     \"API_secret_key\" : \"LqErCdWfdP6BWf3LH3Q0RrJAXHoFvmweBUNtI1WljJ2A8SMelW\"\n","        #     }\n","        #     The current algorithm has the follow steps\n","        #     Step1: Find all twitter’s screen name by simply search the GS_name on twitter save_twitter_metainfo.py\n","        #     The problem now is that simply search the GS_name have a low recall rate, which is seen as the current bottleneck, about 52% of valid user loose in this step (see the below information)\n","        #     Step2: Use match_and_save.py to make a sketch match by the type 1, 2, 3 match and save those users tweets\n","        #     type 1: matched by personal website\n","        #     type 2: matched by keyword\n","        #     type3: matched by similar description with the information in GS\n","        #     Step3: Process the tweets (not important in current step)\n","        #     For the current 400 datapoint, there are 136 valid twitter accounts. I can match 20 of them by personal website, 36 of them by using type 1,2,3 match(with FN=20), and only 66 of them appeared in our search by users name (for example, if I simply search \"Mohammad Moradi\", I can not find the correspondent user moradideli by https://twitter.com/search?q=Mohammad%20Moradi&src=typed_query&f=user).\n","        #     8:45\n","        #     Here is some useful info about how a person annotator find the ground truth twitter user:\n","        #     8:46\n","        #     I followed the instruction in this doc by searching the name + Twitter in the Google first, and click top results to see if there is any match. If none, I will go search the name in Twitter and also browse through the top results. Sometimes I will also search their LinkedIn page to get their most up-to-date information. (the current institute in Google Scholar is not as accurate as their LinkedIn, and LinkedIn has a full history of where they worked. Moreover, they tend to put their photos in LinkedIn)\n","        # '''\n","\n","\n","# literature:\n","\n","# https://direct.mit.edu/qss/article/1/2/771/96149/Large-scale-identification-and-characterization-of"]},{"cell_type":"markdown","metadata":{"id":"YzGtaaqfJ22P"},"source":["# Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZneOBag1VtSX","executionInfo":{"status":"ok","timestamp":1677681509326,"user_tz":-330,"elapsed":62266,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c69c9a11-771c-456a-b666-dedc5a6a90ed"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-6-36f21521b1d1>:24: DeprecationWarning: headless property is deprecated, instead use add_argument('--headless') or add_argument('--headless=new')\n","  options.headless = True\n","<ipython-input-6-36f21521b1d1>:28: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n","  self.driver = webdriver.Chrome(driver_path, options=options)\n"]}],"source":["ts = TwitterSearch(driver_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0upNmvcM1s3B"},"outputs":[],"source":["with open(\"/content/drive/My Drive/tweets-dataset/gs_scholars_matched_with_twitter_accounts_500.tsv\", \"r\") as fr, open('/content/drive/My Drive/tweets-dataset/gs_scholars_candidate_twitter_accounts_v0_original_algo.tsv','a') as fw:\n","  reader = csv.reader(fr, delimiter=\"\\t\")\n","  fw.write('\\n')\n","  num_times = 0\n","  for i, line in tqdm(enumerate(reader)):\n","    index = line[0]\n","    if i==0 or int(index)<=477:\n","      continue\n","    name = line[1]\n","    org = line[2]\n","    gs_url = line[3]\n","    twitter_url = line[4]\n","    if i>0:\n","      # get twitter IDs\n","      twitter_id, candidate_ids = ts.search_scholar('name', name)\n","      num_times += 1\n","      if twitter_id is not None:\n","        # save list of twitter IDs in the CSV\n","        line.append(twitter_id)\n","      else:\n","        line.append(\"N/A\")\n","        time.sleep(random.randint(2, 4))\n","        print(i)\n","      if candidate_ids is not None:\n","        if type(candidate_ids) == dict:\n","          line.extend(list(candidate_ids.keys()))\n","        else:\n","          line.extend(candidate_ids)\n","    fw.write(\"\\t\".join(line))\n","    fw.write('\\n')\n","    if num_times >= 10:\n","      num_times = 0\n","      del ts\n","      ts = TwitterSearch(driver_path)\n","      time.sleep(random.randint(4, 8))"]},{"cell_type":"markdown","source":["# Error Analysis"],"metadata":{"id":"eY0AVpyf5QN7"}},{"cell_type":"code","source":["from pandas.core.internals.managers import ensure_block_shape\n","gs_link_prefix = \"https://scholar.google.com/citations?user=\" \n","twitter_link_prefix = \"https://twitter.com/\"\n","\n","twitter_id_in_top_10 = 0\n","twitter_id_in_top_1 = 0\n","twitter_id_with_no_candidates = 0\n","twitter_id_not_in_candidates = 0\n","null_id_with_no_candidates = 0\n","null_id_with_candidates = 0\n","\n","indices_not_found = []\n","\n","with open(\"/content/drive/My Drive/tweets-dataset/gs_scholars_candidate_twitter_accounts_v0_original_algo.tsv\", \"r\") as f:\n","  reader = csv.reader(f, delimiter=\"\\t\")\n","  for i, line in enumerate(reader):\n","    if i==0:\n","      continue\n","    index = line[0]\n","    name = line[1]\n","    org = line[2]\n","    gs_url = line[3]\n","    twitter_url = line[5]\n","    top_1 = line[6].lower() if line[6] != \"N/A\" and len(line[6]) > 0 else None\n","    top_10 = [id.lower() for id in line[7:] if len(id)>0]\n","    \n","    if twitter_url==\"N/A\":\n","      if len(top_10) == 0:\n","        null_id_with_no_candidates += 1\n","      else:\n","        null_id_with_candidates += 1\n","    else:\n","      twitter_id = twitter_url.split(\"twitter.com/\")[-1]\n","      if len(top_10) == 0:\n","        twitter_id_with_no_candidates += 1\n","      else:\n","        if twitter_id.lower() in top_10:\n","          twitter_id_in_top_10 += 1\n","          if twitter_id.lower() == top_1:\n","            twitter_id_in_top_1 += 1\n","        else:\n","          twitter_id_not_in_candidates += 1"],"metadata":{"id":"IfivVMzq6BXM","executionInfo":{"status":"error","timestamp":1678607410971,"user_tz":-330,"elapsed":1152,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}},"colab":{"base_uri":"https://localhost:8080/","height":249},"outputId":"c283e95e-0559-445d-8a70-1e0af06d521c"},"execution_count":1,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-87802c3e3ae4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mindices_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/tweets-dataset/gs_scholars_candidate_twitter_accounts_v2_all_changes.tsv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\\t\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/tweets-dataset/gs_scholars_candidate_twitter_accounts_v2_all_changes.tsv'"]}]},{"cell_type":"code","source":["print(f'twitter_id_in_top_10 :          {twitter_id_in_top_10}')\n","print(f'twitter_id_in_top_1 :           {twitter_id_in_top_1}')\n","print(f'twitter_id_with_no_candidates : {twitter_id_with_no_candidates}')\n","print(f'twitter_id_not_in_candidates :  {twitter_id_not_in_candidates}')\n","print(f'null_id_with_no_candidates :    {null_id_with_no_candidates}')\n","print(f'null_id_with_candidates :       {null_id_with_candidates}')"],"metadata":{"id":"UtX5O8zF8SDA","executionInfo":{"status":"aborted","timestamp":1678607410972,"user_tz":-330,"elapsed":16,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["precision_at_10 = twitter_id_in_top_10 / (twitter_id_in_top_10 + twitter_id_not_in_candidates + null_id_with_candidates)\n","precision_at_10"],"metadata":{"id":"tG5itA9q8TUo","executionInfo":{"status":"aborted","timestamp":1678607410973,"user_tz":-330,"elapsed":17,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["recall_at_10 = twitter_id_in_top_10 / (twitter_id_in_top_10 + twitter_id_with_no_candidates + twitter_id_not_in_candidates)\n","recall_at_10"],"metadata":{"id":"R6a5lpLA8UQb","executionInfo":{"status":"aborted","timestamp":1678607410974,"user_tz":-330,"elapsed":17,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f_at_10 = (2*precision_at_10*recall_at_10) / (precision_at_10 + recall_at_10)\n","f_at_10"],"metadata":{"id":"3fzB35HI8VSk","executionInfo":{"status":"aborted","timestamp":1678607410974,"user_tz":-330,"elapsed":17,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["precision_at_1 = twitter_id_in_top_1 / (twitter_id_in_top_10 + twitter_id_not_in_candidates + null_id_with_candidates)\n","precision_at_1"],"metadata":{"id":"G7-IN5iA8Wa4","executionInfo":{"status":"aborted","timestamp":1678607410974,"user_tz":-330,"elapsed":16,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["recall_at_1 = twitter_id_in_top_1 / (twitter_id_in_top_10 + twitter_id_with_no_candidates + twitter_id_not_in_candidates)\n","recall_at_1"],"metadata":{"id":"n_QJwBlo8XUv","executionInfo":{"status":"aborted","timestamp":1678607410974,"user_tz":-330,"elapsed":16,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f_at_1 = (2*precision_at_1*recall_at_1) / (precision_at_1 + recall_at_1)\n","f_at_1"],"metadata":{"id":"hpE6y3PB8Yxr","executionInfo":{"status":"aborted","timestamp":1678607410975,"user_tz":-330,"elapsed":17,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"-aDyVyvszFJu"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["PEWY6iMCOrxf","VlhwyVP3QbSU","CK0fRZjArvz4","sgM6xDo9P47w","SIiYtVtozmXP","rt7MW4cCP-Jo","43Ozad0mQCKZ","fTs792JvydZW"],"provenance":[{"file_id":"1wu5F9jW3mcQTm8YKxQudd572fcANJQh3","timestamp":1677248439644},{"file_id":"1hyPc9u8C9DA3WpLwOxTrJkVmuA5sYbLu","timestamp":1676829237557},{"file_id":"1auhNPM8A6flAc5ai9GipgdsF6Xp2rRWQ","timestamp":1673365201119}],"toc_visible":true},"interpreter":{"hash":"ffc4551cdfa24de1e4d6ff6a879c16b8d7cadd8b756628488549bbbf21e2c19d"},"kernelspec":{"display_name":"Python 3.7.13 ('res')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"}},"nbformat":4,"nbformat_minor":0}