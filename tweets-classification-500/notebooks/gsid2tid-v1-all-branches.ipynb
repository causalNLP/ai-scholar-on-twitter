{"cells":[{"cell_type":"markdown","source":["Version 1 - Google Scholar to Twitter ID mapping Algorithm"],"metadata":{"id":"s1Vm3l0l0SUi"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tdCiCGpaOrxc"},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"markdown","metadata":{"id":"PEWY6iMCOrxf"},"source":["# google scholar search"]},{"cell_type":"markdown","metadata":{"id":"VlhwyVP3QbSU"},"source":["### Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25420,"status":"ok","timestamp":1677686805554,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"X4OwlpQLgPhH","outputId":"6f91728a-9892-46c5-92ee-4f2a6c004216"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49730,"status":"ok","timestamp":1677408478645,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"GMEIh6fFkiLk","outputId":"a15dce4a-42d2-418c-8fff-5b45166f3c22"},"outputs":[{"output_type":"stream","name":"stdout","text":["Executing: /tmp/apt-key-gpghome.Y31s0aY3PT/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n","gpg: key DCC9EFBF77E11517: public key \"Debian Stable Release Key (10/buster) <debian-release@lists.debian.org>\" imported\n","gpg: Total number processed: 1\n","gpg:               imported: 1\n","Executing: /tmp/apt-key-gpghome.2rXwEanVl7/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n","gpg: key DC30D7C23CBBABEE: public key \"Debian Archive Automatic Signing Key (10/buster) <ftpmaster@debian.org>\" imported\n","gpg: Total number processed: 1\n","gpg:               imported: 1\n","Executing: /tmp/apt-key-gpghome.WodpKwZq1v/gpg.1.sh --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n","gpg: key 4DFAB270CAA96DFA: public key \"Debian Security Archive Automatic Signing Key (10/buster) <ftpmaster@debian.org>\" imported\n","gpg: Total number processed: 1\n","gpg:               imported: 1\n","Warning: apt-key output should not be parsed (stdout is not a terminal)\n","Warning: apt-key output should not be parsed (stdout is not a terminal)\n","Warning: apt-key output should not be parsed (stdout is not a terminal)\n","Get:1 http://deb.debian.org/debian buster InRelease [122 kB]\n","Get:2 http://deb.debian.org/debian buster-updates InRelease [56.6 kB]\n","Get:3 http://deb.debian.org/debian-security buster/updates InRelease [34.8 kB]\n","Get:4 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease [3,622 B]\n","Hit:5 http://archive.ubuntu.com/ubuntu focal InRelease\n","Get:6 http://deb.debian.org/debian buster/main amd64 Packages [10.7 MB]\n","Get:7 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n","Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n","Get:9 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n","Get:11 http://deb.debian.org/debian buster-updates/main amd64 Packages [9,745 B]\n","Get:12 http://deb.debian.org/debian-security buster/updates/main amd64 Packages [585 kB]\n","Hit:13 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n","Hit:14 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n","Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n","Get:16 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1,299 kB]\n","Hit:17 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n","Get:18 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [2,972 kB]\n","Get:19 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2,069 kB]\n","Ign:20 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n","Hit:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n","Hit:22 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n","Get:23 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1,000 kB]\n","Fetched 19.2 MB in 2s (8,196 kB/s)\n","Reading package lists... Done\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  chromium-common chromium-sandbox libevent-2.1-6 libicu63 libimobiledevice6\n","  libjpeg62-turbo libplist3 libre2-5 libu2f-udev libusbmuxd6 libvpx5\n","  libxxf86dga1 upower usbmuxd x11-utils\n","Suggested packages:\n","  chromium-l10n chromium-shell libusbmuxd-tools mesa-utils\n","The following NEW packages will be installed:\n","  chromium chromium-common chromium-driver chromium-sandbox libevent-2.1-6\n","  libicu63 libimobiledevice6 libjpeg62-turbo libplist3 libre2-5 libu2f-udev\n","  libusbmuxd6 libvpx5 libxxf86dga1 upower usbmuxd x11-utils\n","0 upgraded, 17 newly installed, 0 to remove and 19 not upgraded.\n","Need to get 74.6 MB of archives.\n","After this operation, 256 MB of additional disk space will be used.\n","Get:1 http://deb.debian.org/debian buster/main amd64 libevent-2.1-6 amd64 2.1.8-stable-4 [177 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 libre2-5 amd64 20200101+dfsg-1build1 [162 kB]\n","Get:3 http://deb.debian.org/debian buster/main amd64 libicu63 amd64 63.1-6+deb10u3 [8,293 kB]\n","Get:4 http://deb.debian.org/debian buster/main amd64 libjpeg62-turbo amd64 1:1.5.2-2+deb10u1 [133 kB]\n","Get:5 http://deb.debian.org/debian buster/main amd64 libvpx5 amd64 1.7.0-3+deb10u1 [800 kB]\n","Get:6 http://deb.debian.org/debian buster/main amd64 chromium-common amd64 90.0.4430.212-1~deb10u1 [1,423 kB]\n","Get:7 http://deb.debian.org/debian buster/main amd64 chromium amd64 90.0.4430.212-1~deb10u1 [58.3 MB]\n","Get:8 http://archive.ubuntu.com/ubuntu focal/main amd64 libxxf86dga1 amd64 2:1.1.5-0ubuntu1 [12.0 kB]\n","Get:9 http://archive.ubuntu.com/ubuntu focal/main amd64 x11-utils amd64 7.7+5 [199 kB]\n","Get:10 http://archive.ubuntu.com/ubuntu focal/main amd64 libplist3 amd64 2.1.0-4build2 [31.6 kB]\n","Get:11 http://archive.ubuntu.com/ubuntu focal/main amd64 libusbmuxd6 amd64 2.0.1-2 [19.1 kB]\n","Get:12 http://archive.ubuntu.com/ubuntu focal/main amd64 libimobiledevice6 amd64 1.2.1~git20191129.9f79242-1build1 [65.2 kB]\n","Get:13 http://archive.ubuntu.com/ubuntu focal/main amd64 libu2f-udev all 1.1.10-1 [6,108 B]\n","Get:14 http://archive.ubuntu.com/ubuntu focal/main amd64 upower amd64 0.99.11-1build2 [104 kB]\n","Get:15 http://archive.ubuntu.com/ubuntu focal/main amd64 usbmuxd amd64 1.1.1~git20191130.9af2b12-1 [38.4 kB]\n","Get:16 http://deb.debian.org/debian buster/main amd64 chromium-driver amd64 90.0.4430.212-1~deb10u1 [4,703 kB]\n","Get:17 http://deb.debian.org/debian buster/main amd64 chromium-sandbox amd64 90.0.4430.212-1~deb10u1 [146 kB]\n","Fetched 74.6 MB in 1s (106 MB/s)\n","Selecting previously unselected package libevent-2.1-6:amd64.\n","(Reading database ... 128208 files and directories currently installed.)\n","Preparing to unpack .../00-libevent-2.1-6_2.1.8-stable-4_amd64.deb ...\n","Unpacking libevent-2.1-6:amd64 (2.1.8-stable-4) ...\n","Selecting previously unselected package libicu63:amd64.\n","Preparing to unpack .../01-libicu63_63.1-6+deb10u3_amd64.deb ...\n","Unpacking libicu63:amd64 (63.1-6+deb10u3) ...\n","Selecting previously unselected package libjpeg62-turbo:amd64.\n","Preparing to unpack .../02-libjpeg62-turbo_1%3a1.5.2-2+deb10u1_amd64.deb ...\n","Unpacking libjpeg62-turbo:amd64 (1:1.5.2-2+deb10u1) ...\n","Selecting previously unselected package libre2-5:amd64.\n","Preparing to unpack .../03-libre2-5_20200101+dfsg-1build1_amd64.deb ...\n","Unpacking libre2-5:amd64 (20200101+dfsg-1build1) ...\n","Selecting previously unselected package libvpx5:amd64.\n","Preparing to unpack .../04-libvpx5_1.7.0-3+deb10u1_amd64.deb ...\n","Unpacking libvpx5:amd64 (1.7.0-3+deb10u1) ...\n","Selecting previously unselected package libxxf86dga1:amd64.\n","Preparing to unpack .../05-libxxf86dga1_2%3a1.1.5-0ubuntu1_amd64.deb ...\n","Unpacking libxxf86dga1:amd64 (2:1.1.5-0ubuntu1) ...\n","Selecting previously unselected package x11-utils.\n","Preparing to unpack .../06-x11-utils_7.7+5_amd64.deb ...\n","Unpacking x11-utils (7.7+5) ...\n","Selecting previously unselected package chromium-common.\n","Preparing to unpack .../07-chromium-common_90.0.4430.212-1~deb10u1_amd64.deb ...\n","Unpacking chromium-common (90.0.4430.212-1~deb10u1) ...\n","Selecting previously unselected package chromium.\n","Preparing to unpack .../08-chromium_90.0.4430.212-1~deb10u1_amd64.deb ...\n","Unpacking chromium (90.0.4430.212-1~deb10u1) ...\n","Selecting previously unselected package chromium-driver.\n","Preparing to unpack .../09-chromium-driver_90.0.4430.212-1~deb10u1_amd64.deb ...\n","Unpacking chromium-driver (90.0.4430.212-1~deb10u1) ...\n","Selecting previously unselected package chromium-sandbox.\n","Preparing to unpack .../10-chromium-sandbox_90.0.4430.212-1~deb10u1_amd64.deb ...\n","Unpacking chromium-sandbox (90.0.4430.212-1~deb10u1) ...\n","Selecting previously unselected package libplist3:amd64.\n","Preparing to unpack .../11-libplist3_2.1.0-4build2_amd64.deb ...\n","Unpacking libplist3:amd64 (2.1.0-4build2) ...\n","Selecting previously unselected package libusbmuxd6:amd64.\n","Preparing to unpack .../12-libusbmuxd6_2.0.1-2_amd64.deb ...\n","Unpacking libusbmuxd6:amd64 (2.0.1-2) ...\n","Selecting previously unselected package libimobiledevice6:amd64.\n","Preparing to unpack .../13-libimobiledevice6_1.2.1~git20191129.9f79242-1build1_amd64.deb ...\n","Unpacking libimobiledevice6:amd64 (1.2.1~git20191129.9f79242-1build1) ...\n","Selecting previously unselected package libu2f-udev.\n","Preparing to unpack .../14-libu2f-udev_1.1.10-1_all.deb ...\n","Unpacking libu2f-udev (1.1.10-1) ...\n","Selecting previously unselected package upower.\n","Preparing to unpack .../15-upower_0.99.11-1build2_amd64.deb ...\n","Unpacking upower (0.99.11-1build2) ...\n","Selecting previously unselected package usbmuxd.\n","Preparing to unpack .../16-usbmuxd_1.1.1~git20191130.9af2b12-1_amd64.deb ...\n","Unpacking usbmuxd (1.1.1~git20191130.9af2b12-1) ...\n","Setting up libplist3:amd64 (2.1.0-4build2) ...\n","Setting up libu2f-udev (1.1.10-1) ...\n","Failed to send reload request: No such file or directory\n","Setting up libxxf86dga1:amd64 (2:1.1.5-0ubuntu1) ...\n","Setting up chromium-sandbox (90.0.4430.212-1~deb10u1) ...\n","Setting up libicu63:amd64 (63.1-6+deb10u3) ...\n","Setting up libjpeg62-turbo:amd64 (1:1.5.2-2+deb10u1) ...\n","Setting up libevent-2.1-6:amd64 (2.1.8-stable-4) ...\n","Setting up libusbmuxd6:amd64 (2.0.1-2) ...\n","Setting up x11-utils (7.7+5) ...\n","Setting up libre2-5:amd64 (20200101+dfsg-1build1) ...\n","Setting up chromium-common (90.0.4430.212-1~deb10u1) ...\n","Setting up libimobiledevice6:amd64 (1.2.1~git20191129.9f79242-1build1) ...\n","Setting up libvpx5:amd64 (1.7.0-3+deb10u1) ...\n","Setting up upower (0.99.11-1build2) ...\n","Setting up usbmuxd (1.1.1~git20191130.9af2b12-1) ...\n","Warning: The home dir /var/lib/usbmux you specified can't be accessed: No such file or directory\n","Adding system user `usbmux' (UID 107) ...\n","Adding new user `usbmux' (UID 107) with group `plugdev' ...\n","Not creating home directory `/var/lib/usbmux'.\n","Setting up chromium (90.0.4430.212-1~deb10u1) ...\n","update-alternatives: using /usr/bin/chromium to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n","update-alternatives: using /usr/bin/chromium to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n","Setting up chromium-driver (90.0.4430.212-1~deb10u1) ...\n","Processing triggers for man-db (2.9.1-1) ...\n","Processing triggers for dbus (1.12.16-2ubuntu2.3) ...\n","Processing triggers for mime-support (3.64ubuntu1) ...\n","Processing triggers for hicolor-icon-theme (0.17-2) ...\n","Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting selenium\n","  Downloading selenium-4.8.2-py3-none-any.whl (6.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.8/dist-packages (from selenium) (2022.12.7)\n","Collecting trio-websocket~=0.9\n","  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n","Collecting urllib3[socks]~=1.26\n","  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting trio~=0.17\n","  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 KB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting outcome\n","  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium) (22.2.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium) (2.10)\n","Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium) (2.4.0)\n","Collecting sniffio\n","  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n","Collecting async-generator>=1.9\n","  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n","Collecting exceptiongroup>=1.0.0rc9\n","  Downloading exceptiongroup-1.1.0-py3-none-any.whl (14 kB)\n","Collecting wsproto>=0.14\n","  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n","Collecting h11<1,>=0.9.0\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: urllib3, sniffio, outcome, h11, exceptiongroup, async-generator, wsproto, trio, trio-websocket, selenium\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed async-generator-1.10 exceptiongroup-1.1.0 h11-0.14.0 outcome-1.2.0 selenium-4.8.2 sniffio-1.3.0 trio-0.22.0 trio-websocket-0.9.2 urllib3-1.26.14 wsproto-1.2.0\n"]},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":2}],"source":["%%shell\n","# Ubuntu no longer distributes chromium-browser outside of snap\n","#\n","# Proposed solution: https://askubuntu.com/questions/1204571/how-to-install-chromium-without-snap\n","\n","# Add debian buster\n","cat > /etc/apt/sources.list.d/debian.list <<'EOF'\n","deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster.gpg] http://deb.debian.org/debian buster main\n","deb [arch=amd64 signed-by=/usr/share/keyrings/debian-buster-updates.gpg] http://deb.debian.org/debian buster-updates main\n","deb [arch=amd64 signed-by=/usr/share/keyrings/debian-security-buster.gpg] http://deb.debian.org/debian-security buster/updates main\n","EOF\n","\n","# Add keys\n","apt-key adv --keyserver keyserver.ubuntu.com --recv-keys DCC9EFBF77E11517\n","apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 648ACFD622F3D138\n","apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 112695A0E562B32A\n","\n","apt-key export 77E11517 | gpg --dearmour -o /usr/share/keyrings/debian-buster.gpg\n","apt-key export 22F3D138 | gpg --dearmour -o /usr/share/keyrings/debian-buster-updates.gpg\n","apt-key export E562B32A | gpg --dearmour -o /usr/share/keyrings/debian-security-buster.gpg\n","\n","# Prefer debian repo for chromium* packages only\n","# Note the double-blank lines between entries\n","cat > /etc/apt/preferences.d/chromium.pref << 'EOF'\n","Package: *\n","Pin: release a=eoan\n","Pin-Priority: 500\n","\n","\n","Package: *\n","Pin: origin \"deb.debian.org\"\n","Pin-Priority: 300\n","\n","\n","Package: chromium*\n","Pin: origin \"deb.debian.org\"\n","Pin-Priority: 700\n","EOF\n","\n","# Install chromium and chromium-driver\n","apt-get update\n","apt-get install chromium chromium-driver\n","\n","# Install selenium\n","pip install selenium"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uqc0S4N-ksyb"},"outputs":[],"source":["# from selenium import webdriver\n","# from selenium.webdriver.chrome.options import Options\n","\n","# url = \"https://github.com/googlecolab/colabtools/issues/3347\" \n","# options = Options()\n","# options.add_argument(\"--headless\")\n","# options.add_argument(\"--no-sandbox\")\n","\n","# # options.headless = True\n","\n","# driver = webdriver.Chrome(\"/usr/bin/chromedriver\", options=options)\n","\n","# driver.get(url)\n","# print(driver.title)\n","# driver.quit()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6771,"status":"ok","timestamp":1677686812316,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"Ta260ht0TFGX","outputId":"cc891f05-e4fb-4b7c-f7b0-4d90ab9c63f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting jsonlines\n","  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n","Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from jsonlines) (22.2.0)\n","Installing collected packages: jsonlines\n","Successfully installed jsonlines-3.1.0\n"]}],"source":["!pip install jsonlines\n","# !pip install selenium\n","# !pip install webdriver-manager\n","# !pip install webdriver-manager==3.8.5\n","# !apt-get update\n","# !apt-get install chromium\n","# !apt install chromium-chromedriver\n","# !cp /usr/lib/chromium-browser/chromedriver /usr/bin"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DljCawwaOrxf"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import pickle\n","import json\n","import jsonlines\n","import csv"]},{"cell_type":"markdown","metadata":{"id":"CK0fRZjArvz4"},"source":["###Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vRap3bc7UrUx"},"outputs":[],"source":["def generate_or_keyword_list(query_dict: dict):\n","    \"\"\"Generate necessary keyword lists to help selecting final candidates.\"\"\"\n","    or_keyword_list = []\n","    or_keyword_dict = {}\n","    or_keyword_dict['gs_sid'] = ''\n","    domain_labels = []\n","    if 'expertise' in query_dict['profile']['content']:\n","        for keyword in query_dict['profile']['content']['expertise']:\n","            for key in keyword['keywords']:\n","                key = key.strip().lower()\n","                domain_labels.append(key)\n","    or_keyword_dict['domain_labels'] = domain_labels\n","\n","    coauthors = []\n","    if 'relations' in query_dict['profile']['content'] and len(query_dict['profile']['content']['relations']) > 0:\n","        for relation in query_dict['profile']['content']['relations']:\n","            coauthors.append(relation['name'])\n","    or_keyword_dict['coauthors'] = coauthors\n","\n","    if 'history' in query_dict['profile']['content'] and len(query_dict['profile']['content']['history']) > 0:\n","        tmp_dict = query_dict['profile']['content']['history'][0]\n","        if 'position' in tmp_dict:\n","            or_keyword_dict['position'] = tmp_dict['position']\n","        if 'institution' in tmp_dict:\n","            if 'domain' in tmp_dict['institution']:\n","                or_keyword_dict['email_suffix'] = tmp_dict['institution']['domain']\n","            if 'name' in tmp_dict['institution']:\n","                or_keyword_dict['organization'] = tmp_dict['institution']['name']\n","\n","    or_keyword_list.append(or_keyword_dict)\n","\n","    return or_keyword_list\n","\n","from difflib import SequenceMatcher\n","\n","def get_str_similarity(a: str, b: str) -> float:\n","    \"\"\"Calculate the similarity of two strings and return a similarity ratio.\"\"\"\n","    return SequenceMatcher(None, a, b).ratio()"]},{"cell_type":"markdown","metadata":{"id":"sgM6xDo9P47w"},"source":["### Scholar78kSearch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iFhE-0YePq3U"},"outputs":[],"source":["import pandas as pd\n","import os\n","import re\n","import numpy as np\n","from typing import Union, List\n","\n","\n","class Scholar78kSearch():\n","    def __init__(self):\n","        self.get_78kdata()\n","        self.simple = False\n","        self.verbose = False\n","        self.print_true = False\n","\n","    def get_78kdata(self, source='gdrive'):\n","        \"\"\"Download and load the 78k dataset data.\n","        \n","        Parameters\n","        ----------\n","        source : default is 'gdrive'.\n","        \"\"\"\n","        # path_name = 'gs_scholars_all.npy'\n","        path_name = 'gs_scholars_new.npy'\n","        if source == 'gdrive':\n","            # import gdown\n","            # if not os.path.exists('source'):\n","            #     os.mkdir('source')\n","            # if not os.path.exists(f'source/{path_name}'):\n","            #     gdown.download(\n","            #         'https://self.drive.google.com/uc?id=1NTvn_HiGX3Lr0FtTw5ot3UxcdeNLsv7h',\n","            #         f'source/{path_name}'\n","            #     )\n","            # self.df = pd.DataFrame.from_records(np.load(f'source/{path_name}', allow_pickle=True))\n","            self.df = pd.DataFrame.from_records(np.load(\"/content/drive/My Drive/tweets-dataset/gs_scholars_new.npy\", allow_pickle=True))\n","        else:\n","            raise NotImplementedError\n","    \n","    def search_name(self, name: Union[str, list], query_dict: dict = None) -> List[dict]:\n","        \"\"\"Search scholar candidates given name in the 78k AI scholar dataset.\n","        \n","        Parameters\n","        ----------\n","        name : name of the scholar.\n","        query_dict : if this is given, the method will run <self._search_name_others_helper()>\n","\n","        Returns\n","        -------\n","        df_row_list : a list of response dictionaries.\n","        \n","        \"\"\"\n","        if type(name) is list:\n","            name_list = [name[0], name[-1]]\n","            name = f'{name[0]} {name[-1]}' \n","        elif type(name) is str:\n","            name_list = re.sub('[0-9_\\.\\(\\)\\[\\],]', '', name).split(' ')\n","        else:\n","            raise TypeError(f'Argument \"name\" passed to Scholar78kSearch.search_name has the wrong type.')\n","        df_row = self._search_name_only_helper(name, name_list)\n","        if df_row.shape[0] > 0 and query_dict is not None:\n","            df_row = self._search_name_others_helper(df_row, query_dict)\n","        if self.print_true:\n","            print(f'[Info] Found {df_row.shape[0]} scholars are in 78k data.')\n","            print(f'[Debug] Names: {df_row[\"name\"]}')\n","        if self.verbose:\n","            print(df_row)\n","        return self._deal_with_simple(df_row)\n","        # return df_row\n","\n","    def _deal_with_simple(self, df_row):\n","        if self.simple:\n","            df_row = df_row.loc[:, df_row.columns != 'papers']\n","        df_row = df_row.drop(['co_authors_all'], axis=1)\n","        return df_row.to_dict(orient='records')\n","\n","    def _search_name_only_helper(self, name, name_list):\n","        \"\"\"Helper function of search_name\n","\n","        Returns\n","        -------\n","        Boolean : found or not.\n","        DataFrame : if find else None.\n","        \"\"\"\n","        # find the scholar in our dataset\n","        name_df = self.df.loc[self.df['name'] == name].copy()\n","        name_list_df = self.df.loc[self.df['name'].str.contains(pat = f'^{name_list[0].capitalize()} .*{name_list[-1].capitalize()}', regex=True, case=False)].copy()\n","        return pd.concat([name_df, name_list_df]).drop_duplicates(subset=['url']).reset_index(drop=True)\n","\n","    def _search_name_others_helper(self, df_row, query_dict):\n","        # TODO: add a better filter more than by name\n","        return df_row"]},{"cell_type":"markdown","metadata":{"id":"rt7MW4cCP-Jo"},"source":["### GoogleSearch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kEFrgb4FPupy"},"outputs":[],"source":["import re\n","import time\n","from typing import Union\n","# from selenium import webdriver\n","# from selenium.webdriver.chrome.options import ChromiumOptions\n","from selenium.webdriver.chromium.webdriver import ChromiumDriver\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.remote.errorhandler import NoSuchElementException\n","# from webdriver_manager.chrome import ChromeDriverManager\n","\n","from selenium import webdriver\n","from selenium.webdriver.chrome.options import Options\n","\n","# url = \"https://github.com/googlecolab/colabtools/issues/3347\" \n","\n","class GoogleSearch():\n","    \"\"\"Base class for performing web search on Google using REST API.\"\"\"\n","    def __init__(self, driver_path):\n","        self.setup_webdriver(driver_path)\n","    \n","    def setup_webdriver(self, driver_path):\n","        \"\"\"Setup the webdriver object.\"\"\"\n","\n","        options = Options()\n","        options.add_argument(\"--headless\")\n","        options.add_argument(\"--no-sandbox\")\n","        options.headless = True\n","        # options = ChromiumOptions()\n","        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n","        options.add_experimental_option('useAutomationExtension', False)\n","        # options.add_argument('--headless')\n","        # options.add_argument('--no-sandbox')\n","        options.add_argument('--disable-dev-shm-usage')\n","\n","        self.driver = webdriver.Chrome(driver_path, options=options)\n","\n","        # self.driver = webdriver.Chrome(driver_path, options=options)\n","        # # self.driver = webdriver.Chrome(ChromeDriverManager().install())\n","\n","        # self.driver.execute_cdp_cmd(\"Page.addScriptToEvaluateOnNewDocument\", {\n","        #     \"source\": \"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\"\n","        # })\n","\n","\n","class ScholarGsSearch(GoogleSearch):\n","    \"\"\"Class that handling searching on Google Scholar webpage using REST GET API.\"\"\"\n","    def __init__(self, driver_path):\n","        super().__init__(driver_path)\n","        self._authsearch = 'https://scholar.google.com/citations?hl=en&view_op=search_authors&mauthors={0}'\n","        self._gsidsearch = 'https://scholar.google.com/citations?hl=en&user={0}'\n","        self.print_true = False\n","        # print(\"ScholarGsSearch initiated\")\n","    \n","    def change_name(self, name):\n","        new_name = name[1:].split('_')\n","        new_name[-1] = re.sub(r'[0-9]+', '', new_name[-1])\n","        new_name = ' '.join(new_name)\n","        return new_name\n","\n","    def search_gsid(self, gs_sid: str, simple: bool = True):\n","        \"\"\"Search scholar on Google Scholar based given gs_sid.\n","        \n","        Parameters\n","        ----------\n","        gs_sid : google scholar sid\n","        simple : whether return simple information without paper list.\n","\n","        Returns\n","        -------\n","        scholar_dict_list : a list of dicts of responses.\n","        \n","        \"\"\"\n","        url = self._gsidsearch.format(gs_sid)\n","        self.search_gs_url(url, simple=simple)\n","        \n","    def search_gs_url(self, url: str, simple: bool = True):\n","        self.driver.get(url)\n","        scholar_dict = self._search_gsid_helper(self.driver, url, simple=simple)\n","        time.sleep(5)\n","        if scholar_dict is not None:\n","            \n","            return [scholar_dict]\n","        else:\n","            if self.print_true:\n","                print('[Info] No scholars found given gs_sid in search_gs.')\n","            return []\n","\n","        \n","    def _search_gsid_helper(self, driver: ChromiumDriver, url: str, simple: bool = True):\n","        \"\"\"Helper function for search_gsid.\"\"\"\n","\n","        def get_single_author(element):\n","            li=[]\n","            li.append(element.find_elements(By.TAG_NAME, \"a\")[0].get_attribute('href'))\n","            li.append(element.find_elements(By.TAG_NAME, \"a\")[0].get_attribute('textContent'))\n","            for i in element.find_elements(By.CLASS_NAME, \"gsc_rsb_a_ext\"):\n","                li.append(i.get_attribute('textContent'))\n","            return li\n","\n","        html_first_class = driver.find_elements(By.CLASS_NAME, \"gsc_g_hist_wrp\")\n","        if (len(html_first_class)==0):\n","            if self.print_true:\n","                print(\"[Info] len(html_first_class)==0\")\n","            return None\n","        idx_list = html_first_class[0].find_elements(By.CLASS_NAME, \"gsc_md_hist_b\")[0]\n","        years =  [i.get_attribute('textContent') for i in idx_list.find_elements(By.CLASS_NAME, \"gsc_g_t\")]\n","        cites =  [i.get_attribute('innerHTML') for i in idx_list.find_elements(By.CLASS_NAME, \"gsc_g_al\")]\n","        rsb = driver.find_elements(By.CLASS_NAME, \"gsc_rsb\")[0]\n","        Citations_table=[i.get_attribute('textContent') for i in  rsb.find_elements(By.CLASS_NAME, \"gsc_rsb_std\")]\n","        Co_authors = rsb.find_elements(By.CLASS_NAME, \"gsc_rsb_a\")\n","        if len(Co_authors) == 0:\n","            Co_authors = None\n","        else:\n","            Co_authors = [get_single_author(i) for i in rsb.find_element(By.CLASS_NAME, \"gsc_rsb_a\").find_elements(By.CLASS_NAME, \"gsc_rsb_a_desc\")]\n","\n","        Researcher = {\"url\": url}\n","        gs_sid = None\n","        if 'user=' in url:\n","            tmp_gs_sid = url.split('user=', 1)[1]\n","            if len(tmp_gs_sid) >= 12:\n","                gs_sid = tmp_gs_sid[:12]\n","        # gs_sid\n","        Researcher['gs_sid'] = gs_sid\n","        # coauthors that are listed at the lower right of the profile page\n","        Researcher[\"coauthors\"] = Co_authors\n","        # citation table\n","        Researcher[\"citation_table\"] = [Citations_table[0], Citations_table[2]]\n","        # time series citations\n","        Researcher[\"cites\"] = {\"years\":years, \"cites\":cites}\n","        # name\n","        nameList = driver.find_elements(By.ID, \"gsc_prf_in\")\n","        if (len(nameList) != 1):\n","            if self.print_true:\n","                print(\"len(nameList)!=1\")\n","            return None\n","        Researcher[\"name\"] = nameList[0].text\n","        # organization\n","        infoList = driver.find_elements(By.CLASS_NAME, 'gsc_prf_il')\n","        Researcher['organization'] = infoList[0].get_attribute('textContent')\n","        # homepage\n","        homepage_url = infoList[1].find_elements(By.TAG_NAME, 'a')\n","        if len(homepage_url) == 0:\n","            Researcher['homepage_url'] = None\n","        else:\n","            Researcher['homepage_url'] = homepage_url[0].get_attribute('href')\n","        # email address\n","        email_str_match = re.search(r'[\\w-]+\\.[\\w.-]+', infoList[1].text)\n","        if email_str_match is not None:\n","            Researcher['email_info'] = email_str_match.group(0)\n","        # domain labels\n","        Researcher['domain_labels'] = [i.get_attribute('textContent').strip().lower() for i in infoList[2].find_elements(By.CLASS_NAME, 'gsc_prf_inta')]\n","        # if not simple, get paper lists\n","        if not simple:\n","            button = driver.find_elements(By.CLASS_NAME, 'gs_btnPD')\n","            if (len(button) != 1):\n","                if self.print_true:\n","                    print(\"len(button)!=1\")\n","                return None\n","            while (button[0].is_enabled()):\n","                while (button[0].is_enabled()):\n","                    while (button[0].is_enabled()):\n","                        button[0].click()\n","                        time.sleep(5)\n","                    time.sleep(1)\n","                time.sleep(2)\n","            papers = []\n","            items = driver.find_elements(By.CLASS_NAME, 'gsc_a_tr')\n","            for i in items:\n","                item = i.find_element(By.CLASS_NAME, 'gsc_a_at')\n","                url = item.get_attribute(\"href\")\n","                paper_info=[j.text for j in i.find_elements(By.CLASS_NAME, 'gs_gray')]\n","                cite = i.find_element(By.CLASS_NAME, 'gsc_a_ac')\n","                year = i.find_element(By.CLASS_NAME, 'gsc_a_y').find_element(By.CLASS_NAME, \"gsc_a_h\").text\n","                papers.append([url, item.text, \n","                                paper_info,\n","                            cite.text, cite.get_attribute(\"href\"),\n","                            year])\n","            Researcher[\"papers\"] = papers\n","\n","        def generate_single_coauthor(element):\n","            coauthor_dict = {\n","                \"name\":element.find_elements(By.CLASS_NAME, 'gs_ai_name')[0].get_attribute('textContent'),\n","                \"url\":element.find_elements(By.CLASS_NAME, 'gs_ai_pho')[0].get_attribute('href'),\n","                \"description\":element.get_attribute('innerHTML'),\n","            }\n","            return coauthor_dict\n","        extra_coauthors = driver.find_elements(By.CLASS_NAME, \"gsc_ucoar\")\n","        Researcher['extra_co_authors'] = [generate_single_coauthor(i) for i in extra_coauthors]\n","        return Researcher\n","\n","    def search_name(self, name: Union[str, list], query_dict: dict = None, top_n=3, simple=True):\n","        \"\"\"Search on Google Scholar webpage given name.\n","        \n","        Parameters\n","        ----------\n","        name : name of the scholar.\n","        query_dict : a dict containing information of the scholar.\n","        top_n : select <top_n> candidates.\n","        simple : whether return simple information without paper list.\n","\n","        Returns\n","        -------\n","        resp : list of candidate scholars, empty if no candidates are found.\n","\n","        \"\"\"\n","        if type(name) is list:\n","            # current case\n","            name_list = [name[0], name[-1]]\n","            name = f'{name[0]} {name[-1]}' \n","        elif type(name) is str:\n","            name_list = name.split(' ')\n","        else:\n","            raise TypeError('Argument \"name\" passed to ScholarGsSearch.search_name has the wrong type.')\n","        url_fragment = f'{name} '\n","        if query_dict is not None:\n","            # first try (name, email_suffix, position, organization) as url\n","            keyword_list = generate_or_keyword_list(query_dict)[0]\n","            url_fragment_new = url_fragment\n","            # if 'email_suffix' in keyword_list:\n","            #     url_fragment_new = url_fragment_new + keyword_list['email_suffix'] + ' '\n","            # if 'position' in keyword_list:\n","            #     url_fragment_new = url_fragment_new + keyword_list['position'] + ' '\n","            # if 'organization' in keyword_list:\n","            #     url_fragment_new = url_fragment_new + keyword_list['organization'] + ' '\n","\n","            # url = self._authsearch.format(url_fragment_new)\n","            # self.driver.get(url)\n","            # time.sleep(5)\n","            # scholar_list = self._search_name_helper(self.driver, name_list)\n","            # if len(scholar_list) > 0:\n","            #     if wo_full:\n","            #         return scholar_list\n","            #     else:\n","            #         return self._search_name_list_expand(scholar_list, simple=simple)\n","            \n","            # second try (name, email_suffix)\n","            if 'email_suffix' in keyword_list:\n","                url_fragment_new = url_fragment + keyword_list['email_suffix'] # + ' '\n","            url = self._authsearch.format(url_fragment_new)\n","            self.driver.get(url)\n","            time.sleep(5)\n","            scholar_list = self._search_name_helper(self.driver, name_list)\n","            # return scholar_list\n","            if len(scholar_list) > 0:\n","                if self.print_true:\n","                    print(f'[Info] Find {len(scholar_list)} scholars using query without gs_sid in step 1.')\n","                # return self._search_name_list_expand(scholar_list, simple=simple)\n","                return scholar_list\n","        \n","            # third try (name, position)\n","            if 'position' in keyword_list:\n","                url_fragment_new = url_fragment + keyword_list['position'] # + ' '\n","            url = self._authsearch.format(url_fragment_new)\n","            self.driver.get(url)\n","            time.sleep(5)\n","            scholar_list = self._search_name_helper(self.driver, name_list)\n","            # return scholar_list\n","            if len(scholar_list) > 0:\n","                if self.print_true:\n","                    print(f'[Info] Find {len(scholar_list)} scholars using query without gs_sid in step 2.')\n","                # return self._search_name_list_expand(scholar_list, simple=simple)\n","                return scholar_list\n","\n","            # fourth try (name, organization)\n","            if 'organization' in keyword_list:\n","                url_fragment_new = url_fragment + keyword_list['organization'] # + ' '\n","            url = self._authsearch.format(url_fragment_new)\n","            self.driver.get(url)\n","            time.sleep(5)\n","            scholar_list = self._search_name_helper(self.driver, name_list)\n","            # return scholar_list\n","            if len(scholar_list) > 0:\n","                if self.print_true:\n","                    print(f'[Info] Find {len(scholar_list)} scholars using query without gs_sid in step 3.')\n","                # return self._search_name_list_expand(scholar_list, simple=simple)\n","                return scholar_list\n","\n","        # finally, only search (name: firstname and lastname). If only one response returns, mark it as candidate\n","        url = self._authsearch.format(url_fragment)\n","        self.driver.get(url)\n","        time.sleep(5)\n","        scholar_list = self._search_name_helper(self.driver, name_list)\n","        if len(scholar_list) > 0:\n","        # if len(scholar_list) > 0 and len(scholar_list) <= top_n:\n","            if self.print_true:\n","                print(f'[Info] Find {len(scholar_list)} scholars using query without gs_sid in step 4.')\n","            # return self._search_name_list_expand(scholar_list, simple=simple)\n","            return scholar_list\n","        \n","        return []\n","\n","    def _search_name_helper(self, driver, name_list):\n","        \"\"\"Helper function of <self.search_name()>.\"\"\"\n","        # iterate over searched list, find dicts that contains the name (including)\n","        useful_info_list = driver.find_elements(By.CLASS_NAME, 'gs_ai_t')\n","        useful_info_ext_list = []\n","        if len(useful_info_list) != 0:\n","            for scholar_webdriver in useful_info_list:\n","                name = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_name').get_attribute('textContent').strip()\n","                # check whether name is correct\n","                not_a_candidate = False\n","                for name_fragment in name_list:\n","                    if name_fragment.lower() not in name.lower():\n","                        not_a_candidate = True\n","                        break\n","                if not_a_candidate:\n","                    continue\n","                \n","                # grab all the other information\n","                pos_org = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_aff').get_attribute('textContent').strip()\n","                email_str = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_eml').get_attribute('textContent').strip()\n","                cite = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_cby').get_attribute('textContent').strip()\n","                url = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_name').find_element(By.TAG_NAME, 'a').get_attribute('href').strip()\n","                domain_labels = scholar_webdriver.find_element(By.CLASS_NAME, 'gs_ai_int').find_elements(By.CLASS_NAME, 'gs_ai_ont_int')\n","                for idx, domain in enumerate(domain_labels):\n","                    domain_labels[idx] = domain.get_attribute('textContent').strip().lower()\n","\n","                # continue processing\n","                gs_sid = None\n","                if 'user=' in url:\n","                    tmp_gs_sid = url.split('user=', 1)[1]\n","                    if len(tmp_gs_sid) >= 12:\n","                        gs_sid = tmp_gs_sid[:12]\n","\n","                if email_str is not None and email_str != '':\n","                    match = re.search(r'[\\w-]+\\.[\\w.-]+', email_str)\n","                    email_str = match.group(0)\n","\n","                cites = [int(s) for s in cite.split() if s.isdigit()]\n","                useful_info_ext_list.append({\n","                    'name': name,\n","                    'pos_org': pos_org,\n","                    'email': email_str,\n","                    'cite': cites[0] if len(cites)>0 else None,\n","                    'url': url,\n","                    'gs_sid': gs_sid,\n","                    'domain_labels': domain_labels\n","                })\n","        return useful_info_ext_list\n","        \n","    def _search_name_list_expand(self, scholar_list, simple=True):\n","        \"\"\"Expand the name_list to full_name_list.\"\"\"\n","        new_scholar_list = []\n","        for scholar in scholar_list:\n","            if 'gs_sid' in scholar:\n","                url = self._gsidsearch.format(scholar['gs_sid'])\n","                self.driver.get(url)\n","                scholar_dict = self._search_gsid_helper(self.driver, url, simple=simple)\n","                if scholar_dict is not None:\n","                    new_scholar_list.append(scholar_dict)\n","                time.sleep(5)\n","        return new_scholar_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-F3mp0N7QIro"},"outputs":[],"source":["from difflib import SequenceMatcher\n","\n","def get_str_similarity(a: str, b: str) -> float:\n","    \"\"\"Calculate the similarity of two strings and return a similarity ratio.\"\"\"\n","    return SequenceMatcher(None, a, b).ratio()"]},{"cell_type":"markdown","metadata":{"id":"43Ozad0mQCKZ"},"source":["### ScholarSearch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DVB-Cz04PNOv"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import pickle\n","import json\n","import typing\n","from typing import List, Union\n","import os\n","import re\n","import time\n","import sys\n","import requests\n","from bs4 import BeautifulSoup\n","\n","\n","class ScholarSearch():\n","    \"\"\"A class that handles searching over Google Scholar profiles and the 78k AI scholar dataset.\"\"\"\n","    def __init__(self):\n","        # attributes\n","        self.similarity_ratio = 0.8\n","        # self.driver_path = '../chromedriver'\n","        self.driver_path = '/usr/bin/chromedriver'\n","    \n","    def setup(self):\n","        # self.get_profiles(['review_data/area_chair_id_to_profile.json', 'review_data/reviewer_id_to_profile.json'])\n","        # self.get_profiles(None)\n","        self.search_78k = Scholar78kSearch()\n","        # print(\"Scholar78kSearch initiated\" )\n","        self.search_gs = ScholarGsSearch(self.driver_path)\n","\n","    def reset(self):\n","        pass\n","\n","    def get_profiles(self, filepath_list: List[str] = None) -> None:\n","        \"\"\"In case that you want to get responses of a list of scholars, \n","        the method is implemented for you to load (could be multiple) json data files.\n","\n","        Parameters\n","        ----------\n","        filepath_list : list of json data filepaths to load.\n","\n","        \"\"\"\n","        if filepath_list is None:\n","            return\n","        # set of json data dicts\n","        self.profile = {}\n","        for filepath in filepath_list:\n","            with open(filepath) as file:\n","                profile = json.load(file)\n","                self.profile.update(profile)\n","        # number of unique json data dicts in total\n","        # print(f'Number of unique json data dicts in total: {len(self.profile)}')\n","\n","    def get_scholar(\n","        self,\n","        query: Union[str, dict],\n","        field: List[str] = None,\n","        simple: bool = True,\n","        top_n: int = 3,\n","        print_true: bool = True) -> List[dict]:\n","        \"\"\"Get up to <top_n> relevant candidate scholars by searching over Google Scholar profiles and the 78k AI scholar dataset.\n","        \n","        Parameters\n","        ----------\n","        query : a query containing the known scholar information.\n","        field : a list of fields wants to return. If not given, by default full information will be returned.\n","        simple : whether return simple information without paper list. This works only if the argument <field> is not specified.\n","        top_n : return at most <top_n> scholars if the result is not considered as determined.\n","        print_true : print info / debug info of the search process.\n","\n","        Returns\n","        -------\n","        resp : list of candidate scholars, empty if no candidates are found.\n","\n","        \"\"\"\n","\n","        self.search_78k.simple = simple\n","        self.search_78k.print_true = print_true\n","        self.search_gs.print_true = print_true\n","        self.print_true = print_true\n","        self.reset()\n","\n","        scholar_cnt = 0\n","        if type(query) is dict:\n","            # query is dict\n","            resp = self.search_dict(query, simple=simple, top_n=top_n)\n","        elif type(query) is str:\n","            # query is str\n","            resp = self.search_name(query, simple=simple, top_n=top_n)                \n","        else:\n","            raise TypeError(f'[Error] The argument \"query\" must be str or dict, not {type(query)}.')\n","\n","        \n","        # select specific features\n","        if field is not None:\n","            resp_final = []\n","            for resp_item in resp:\n","                resp_dict = {}\n","                for field_item in field:\n","                    if field_item not in resp_item:\n","                        raise KeyError(f'The key {field_item} is not in the response dictionary')\n","                    \n","                    resp_dict[field_item] = resp_item[field_item]\n","                resp_dict['gs_sid'] = resp_item['gs_sid']\n","                resp_dict['url'] = resp_item['url']\n","                resp_dict['citation_table'] = resp_item['citation_table']\n","                resp_final.append(resp_dict)\n","            if print_true:\n","                scholar_cnt = len(resp_final)\n","                if scholar_cnt == 1:\n","                    print(f'[Info] In total 1 scholar is found:')\n","                else:\n","                    print(f'[Info] In total {scholar_cnt} scholars are found:')\n","                resp_str = json.dumps(resp_final, indent=2)\n","                print(resp_str)\n","            return resp_final\n","        else:\n","            if print_true:\n","                scholar_cnt = len(resp)\n","                if scholar_cnt == 1:\n","                    print(f'[Info] In total 1 scholar is found:')\n","                else:\n","                    print(f'[Info] In total {scholar_cnt} scholars are found:')\n","                resp_str = json.dumps(resp, indent=2)\n","                print(resp_str)\n","            return resp\n","    \n","    def search_name(self, name: str, simple: bool = True, top_n: int = 3, from_dict: bool = False, query_dict: dict = None) -> List[dict]:\n","        \"\"\"Search gs profile given name or OpenReview id.\n","        \n","        Parameters\n","        ----------\n","        name : the name of the scholar ([first_name last_name]).\n","        simple : whether return simple information without paper list. This works only if the argument <field> is not specified.\n","        top_n : return at most <top_n> scholars if the result is not considered as determined.\n","        from_dict : default = False. Should be true only if using <get_scholar()> class method.\n","        query_dict : default = None. Should be a dict only if using <get_scholar()> class method.\n","\n","        Returns\n","        -------\n","        resp : list of candidate scholars, empty if no candidates are found.\n","        \"\"\"\n","\n","        self.search_78k.simple = simple\n","        name = name.strip()\n","        dict = None\n","        real_name = True\n","        # OpenReview id\n","        if ' ' not in name and name[0] == '~':\n","            # search over chair id\n","            if name in self.profile:\n","                dict = self.profile[name]\n","            # crawl http api response\n","            if dict is not None and not from_dict:\n","                # name\n","                real_name = False\n","                resp = self.search_dict(dict, simple=simple, top_n=top_n)\n","            else:\n","                # get real name\n","                or_name = name # string\n","                name = name[1:].split('_')\n","                name[-1] = re.sub(r'[0-9]+', '', name[-1]) # list\n","                # name = ' '.join(name) # e.g., Rachel K. E. Bellamy\n","        else:\n","            or_name = name.split(' ') # list\n","            # name string\n","        if real_name:\n","            if from_dict:\n","                print('Not find by gs_sid, search from_dict')\n","                # it inputs a real name (firstname, lastname)\n","                resp = self.search_78k.search_name(name, query_dict)\n","                resp_gs = self.search_gs.search_name(name, query_dict, top_n=top_n, simple=simple)\n","                resp = self.select_final_cands(resp, top_n, query_dict=query_dict, resp_gs_prop={'resp_gs': resp_gs})\n","            else:\n","                # or_resp = self.get_or_scholars(or_name)\n","                # TODO: resp_gs for only searching name is not implemented\n","                # resp = self.select_final_cands(resp, or_resp, top_n, simple=simple)\n","                resp = self.search_78k.search_name(name)\n","                resp_gs = self.search_gs.search_name(name, query_dict=None, top_n=top_n, simple=simple)\n","                resp = self.select_final_cands(resp, top_n, query_dict=None, resp_gs_prop={'resp_gs': resp_gs})\n","        return resp\n","    \n","\n","    def get_or_scholars(self, or_name: Union[str, list]):\n","        \"\"\"Get OpenReview candidate scholars list by name through http api response.\"\"\"\n","        # format the name list to get OpenReview rest api response\n","        if type(or_name) is list:\n","            or_name_list = []\n","            if len(or_name) >= 2:\n","                id_list = []\n","                for idx, name_part in enumerate(or_name):\n","                    if idx == 0 or idx == len(or_name) - 1:\n","                        id_list.append(name_part.capitalize())\n","                    else:\n","                        if len(name_part) > 1:\n","                            id_list.append(f'{name_part[0].upper()}.') # middle name in abbreviate form\n","                        else:\n","                            id_list.append(name_part.upper())\n","                if len(id_list) == 2:\n","                    or_name_list.append(f'~{id_list[0]}_{id_list[-1]}')\n","                elif len(id_list) > 2:\n","                    or_name_list.append(f'~{id_list[0]}_{id_list[-1]}')\n","                    tmp_str = '_'.join(id_list)\n","                    or_name_list.append(f'~{tmp_str}')\n","            else:\n","                raise ValueError('Argument \"or_name\" passed to get_or_scholars is not a valid name list.')\n","        elif type(or_name) is str:\n","            or_name_list = [or_name]\n","        else:\n","            raise TypeError(f'Argument \"or_name\" passed to get_or_scholars has the wrong type.')\n","        del or_name\n","\n","        # get request response\n","        go_ahead = True\n","        resp_list = []\n","        for name in or_name_list:\n","            if name[-1].isnumeric():\n","                name_cur = name\n","                go_ahead = False\n","                name_cur_cnt = 1\n","            else:\n","                name_cur_cnt = 1\n","                name_cur = f'{name}{name_cur_cnt}'\n","\n","            # set accumulative count\n","            acc_cnt = 0\n","            while acc_cnt <= 1:\n","                response = requests.get(f'https://openreview.net/profile?id={name_cur}')\n","                time.sleep(1)\n","\n","                if not response.ok:\n","                    acc_cnt += 1\n","                else:\n","                    soup = BeautifulSoup(response.content.decode('utf-8'), 'html.parser')\n","                    resp_list.append(json.loads(soup.find_all('script', id=\"__NEXT_DATA__\")[0].string))\n","                name_cur_cnt += 1\n","                name_cur = f'{name}{name_cur_cnt}'\n","                if not go_ahead:\n","                    break\n","        if self.print_true:\n","            if len(resp_list) != 1:\n","                print(f'[Info] Found {len(resp_list)} scholars using OpenReview REST API.')\n","            else:\n","                print(f'[Info] Found 1 scholar using OpenReview REST API.')\n","        return resp_list \n","        # NOTE: the dict in this list is in a different format than the dict from OpenReview dataset.\n","\n","    def select_final_cands(self, resp: List[dict], top_n: int, query_dict: dict = None, resp_gs_prop: dict = None, simple: bool = True) -> List[dict]:\n","        \"\"\"Select final candidates according to the response from OpenReview and 78k data.\n","        \n","        Parameters\n","        ----------\n","        resp : response from 78k dataset.\n","        or_resp : prepare the necessary key-value pairs to help filtering.\n","        top_n : return at most <top_n> scholars if the result is not considered as determined.\n","        query_dict : default = None. Should be a dict only if using <get_scholar()> class method.\n","        resp_gs_prop : dict containing the response from Google Scholar webpage.\n","        simple : whether return simple information without paper list. This works only if the argument <field> is not specified.\n","\n","        Returns\n","        -------\n","        resp : list of candidate scholars, empty if no candidates are found.\n","        \n","        \"\"\"\n","        # get useful data from or_resp\n","        if query_dict is not None:\n","            or_keyword_list = generate_or_keyword_list(query_dict)\n","\n","        # merge resp with resp_gs\n","        if resp_gs_prop is not None:\n","            resp_gs = resp_gs_prop['resp_gs']\n","            # if there are one candidate from google scholar pages, we throw out resp from 78k data.\n","            if len(resp_gs) == 1:\n","                resp = []\n","            # iterate over resp_gs\n","            for resp_gs_item in resp_gs:\n","                find_flag = False\n","                # gs_sid\n","                for resp_item in resp:\n","                    if resp_gs_item['gs_sid'] == resp_item['gs_sid']:\n","                        find_flag = True\n","                        break\n","                if find_flag:\n","                    continue\n","                # construct new prep\n","                # generate full dict\n","                self.search_gs.driver.get(resp_gs_item['url'])\n","                time.sleep(5)\n","                if query_dict is not None or (query_dict is None and len(resp) <= top_n):\n","                    resp_gs_full_item = self.search_gs._search_gsid_helper(self.search_gs.driver, resp_gs_item['url'], simple=simple)\n","                    if resp_gs_full_item is not None:\n","                        resp.append(resp_gs_full_item)\n","        \n","        if query_dict is None:\n","            return resp[:top_n]\n","\n","        # calculate rankings\n","        rank = {}\n","        for idx_cand, cand in enumerate(resp):\n","            rank[idx_cand] = []\n","            gs_sid_flag = 0\n","            cnt_true = [0] * len(or_keyword_list) \n","            cnt_all = 0\n","            cnt_true_rel = [0] * len(or_keyword_list) \n","            cnt_all_rel = 0\n","            for idx_or_scholar, or_scholar in enumerate(or_keyword_list):\n","                # gs_sid\n","                if 'gs_sid' in cand:\n","                    if cand['gs_sid'] == or_scholar['gs_sid']: \n","                        gs_sid_flag = 1\n","\n","                # domain_labels\n","                if cand['domain_labels'] is not None:\n","                    for cand_domain_tag in cand['domain_labels']:\n","                        cnt_all += 1\n","                        for or_domain_tag in or_scholar['domain_labels']:\n","                            if get_str_similarity(cand_domain_tag, or_domain_tag) >= self.similarity_ratio:\n","                                cnt_true[idx_or_scholar] += 1\n","                \n","                \n","                # relations\n","                cnt_all_rel = 0\n","                # print(cand)\n","                if cand['coauthors'] is not None:\n","                    for cand_coauth in cand['coauthors']:\n","                        cnt_all_rel += 1\n","                        for or_coauth in or_scholar['coauthors']:\n","                            if get_str_similarity(or_coauth, cand_coauth[1]) >= self.similarity_ratio:\n","                                cnt_true_rel[idx_or_scholar] += 1\n","                \n","            # get the rank list\n","            # gs_sid\n","            if gs_sid_flag:\n","                rank[idx_cand].append(1)\n","            else:\n","                rank[idx_cand].append(0)\n","            \n","            # domain_labels\n","            for i in range(len(cnt_true)):\n","                if cnt_all == 0:\n","                    cnt_true[i] = 0\n","                else:\n","                    cnt_true[i] = cnt_true[i] / cnt_all\n","            rank[idx_cand].append(max(cnt_true))\n","\n","            # relations\n","            for i in range(len(cnt_true_rel)):\n","                if cnt_all_rel == 0:\n","                    cnt_true_rel[i] = 0\n","                else:\n","                    cnt_true_rel[i] = cnt_true_rel[i] / cnt_all_rel\n","            rank[idx_cand].append(max(cnt_true_rel))\n","        \n","        # select final candidate\n","        final_idx = []\n","        for rank_idx in rank:\n","            if rank[rank_idx][0] == 1:\n","                final_idx.append(rank_idx)\n","        \n","        # TODO: or we can set weights to (relations, domain_tags) to rank the scholar candidates\n","        if len(final_idx) < top_n:\n","            domain_tag_rank = []\n","            relation_rank = []\n","            for rank_idx in sorted(rank.keys()):\n","                # print(rank_idx)\n","                domain_tag_rank.append(rank[rank_idx][1])\n","                relation_rank.append(rank[rank_idx][2])\n","            # print(domain_tag_rank, relation_rank)\n","            domain_tag_idxes = np.argsort(domain_tag_rank)[::-1]\n","            relation_idxes = np.argsort(relation_rank)[::-1]\n","            for idx in relation_idxes:\n","                if relation_rank[idx] == 0:\n","                    break\n","                if len(final_idx) < top_n:\n","                    if idx not in final_idx:\n","                        final_idx.append(idx)\n","                else:\n","                    break\n","            for idx in domain_tag_idxes:\n","                if domain_tag_rank[idx] == 0:\n","                    break\n","                if len(final_idx) < top_n:\n","                    if idx not in final_idx:\n","                        final_idx.append(idx)\n","                else:\n","                    break\n","            if len(final_idx) == 0 and len(rank.keys()) > 0:\n","                    for rank_idx in sorted(rank.keys()):\n","                        if len(final_idx) >= top_n:\n","                            break\n","                        else:\n","                            final_idx.append(rank_idx)\n","        # print(resp)\n","        # print(or_keyword_list)\n","        # print(rank)\n","        # print(final_idx)\n","        resp = [resp[i] for i in final_idx]\n","        return resp\n","\n","    def search_dict(self, query_dict: dict, simple: bool = True, top_n: int = 3):\n","        \"\"\"Search candidates given a dictionary.\n","        \n","        Parameters\n","        ----------\n","        query_dict : default = None. Should be a dict only if using <get_scholar()> class method.\n","        simple : whether return simple information without paper list. This works only if the argument <field> is not specified.\n","        top_n : return at most <top_n> scholars if the result is not considered as determined.\n","\n","        Returns\n","        -------\n","        resp : list of candidate scholars, empty if no candidates are found.\n","\n","        \"\"\"\n","        self.search_78k.simple = simple\n","        # gs_sid\n","        if 'gscholar' in query_dict['profile']['content'] and 'user=' in query_dict['profile']['content']['gscholar']:\n","            tmp_gs_sid = query_dict['profile']['content']['gscholar'].split('user=', 1)[1]\n","            if len(tmp_gs_sid) >= 12:\n","                gs_sid = tmp_gs_sid[:12]\n","                name_df = self.search_78k.df.loc[self.search_78k.df['gs_sid'] == gs_sid].copy()\n","                if name_df.shape[0] != 0:\n","                    print(f'[Info] Found a scholar using 78k gs_sid')\n","                    return self.search_78k._deal_with_simple(name_df)\n","                else:\n","                    print(f'[Info] Found a scholar using query dict gs_sid')\n","                    resp = self.search_gs.search_gsid(gs_sid, simple=simple)\n","                    if len(resp) > 0:\n","                        return resp\n","                    \n","        \n","        # search_name\n","        return self.search_name(query_dict['profile']['id'], simple=simple, top_n=top_n, from_dict=True, query_dict=query_dict)"]},{"cell_type":"markdown","metadata":{"id":"0GF_xG8UVOK6"},"source":["### TwitterSearch - edited - v1\n","\n","|Total|\tHave Twitter ID|\tTwitter ID in top 10|\tTwitter ID in top 1\t|P@10|\tP@1|\n","| -- | -- | -- | -- | -- | -- |\n","|280|\t94|\t72|\t40|\t76.59%|\t42.55%|"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"237sT5XKVK7Z"},"outputs":[],"source":["import time\n","from typing import Union\n","from collections import defaultdict\n","from selenium import webdriver\n","from selenium.webdriver.chromium.webdriver import ChromiumDriver\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.remote.errorhandler import WebDriverException\n","from bs4 import BeautifulSoup\n","\n","url_search_dict = {\n","    'google': 'https://www.google.com/search?q={0}'\n","}\n","\n","\n","def get_search_result(**kwargs):\n","    return kwargs\n","\n","\n","class TwitterSearch(GoogleSearch):\n","    \"\"\"Class that handling searching on Google search bar using REST API.\"\"\"\n","\n","    def __init__(self, driver_path):\n","        super().__init__(driver_path)\n","        # print(\"GoogleSearch initiated\")\n","        self._urlsearch = url_search_dict['google']\n","        self.print_true = False #NOTE: should be set by users\n","        # initialize scholar_search object\n","        self.scholar_search = ScholarSearch()\n","        # print(\"ScholarSearch initiated\")\n","        self.scholar_search.setup()\n","        # print(\"setup done\")\n","\n","    def search_scholar(self, str_type: str, term: str):\n","        \"\"\"\n","        NOTE: now the only allowed str_type is name\n","        \"\"\"\n","        if str_type == 'name':\n","            scholar_search_result = self.scholar_search.get_scholar(query=term, simple=True, top_n=1, print_true=False)\n","            branch_type = \"\"\n","            if len(scholar_search_result) == 0:\n","                # directly search\n","                url_fragment = self._urlsearch.format(f'{term} \"twitter\"')\n","                result = self._search_google_helper(url_fragment)\n","                twitter_ids = self.filter_result(result, term, web_source='google')\n","                branch_type += 'directly search,'\n","            else:\n","                # twitter_ids = None\n","                twitter_ids = []\n","                len_twitter_ids = 0\n","                # try directly get twitter account through homepage\n","                if 'homepage_url' in scholar_search_result[0] and scholar_search_result[0]['homepage_url'] is not None:\n","                    new_ids = self._search_twitter_from_homepage(scholar_search_result[0]['homepage_url'], name=term, name_from_gs=scholar_search_result[0][\"name\"])\n","                    if new_ids:\n","                      twitter_ids.extend(new_ids)\n","                if len(twitter_ids)>len_twitter_ids:\n","                    branch_type += 'homepage,'\n","                    len_twitter_ids = len(twitter_ids)\n","\n","                # then try (google_name email_suffix \"twitter\")\n","                # print(\"searching name + email\")\n","                if \"email_info\" in scholar_search_result[0] and scholar_search_result[0][\"email_info\"] != '':# and twitter_ids is None:\n","                    url_fragment = self._urlsearch.format(f'{scholar_search_result[0][\"name\"]} {scholar_search_result[0][\"email_info\"]} \"twitter\"')\n","                    result = self._search_google_helper(url_fragment)\n","                    new_ids = self.filter_result(result, term, web_source='google')\n","                    if new_ids:\n","                      twitter_ids.extend(new_ids)\n","                # print(twitter_ids)\n","                if len(twitter_ids)>len_twitter_ids:\n","                    branch_type += 'name + email,'\n","                    len_twitter_ids = len(twitter_ids)\n","\n","                # then try (google_name organization \"twitter\")\n","                # print(\"searching name + organization\")\n","                if \"organization\" in scholar_search_result[0] and scholar_search_result[0][\"organization\"] != '':# and twitter_ids is None:\n","                    url_fragment = self._urlsearch.format(f'{scholar_search_result[0][\"name\"]} {scholar_search_result[0][\"organization\"]} \"twitter\"')\n","                    result = self._search_google_helper(url_fragment)\n","                    new_ids = self.filter_result(result, term, web_source='google')\n","                    if new_ids:\n","                      twitter_ids.extend(new_ids)\n","                # print(twitter_ids)\n","                if len(twitter_ids)>len_twitter_ids:\n","                    branch_type += 'name + organization,'\n","                    len_twitter_ids = len(twitter_ids)\n","\n","                # then try (google_name \"twitter\")\n","                # print(\"searching name \\\"twitter\\\"\")\n","                # if len(twitter_ids):\n","                # TODO - preprocess google_name to include only tokens which have length greater than one\n","                # gs_name = re.sub('[0-9_\\.,]', '', scholar_search_result[0][\"name\"])\n","                url_fragment = self._urlsearch.format(f'{scholar_search_result[0][\"name\"]} \"twitter\"')\n","                result = self._search_google_helper(url_fragment)\n","                new_ids = self.filter_result(result, term, web_source='google')\n","                if new_ids:\n","                      twitter_ids.extend(new_ids)\n","                # print(twitter_ids)\n","                if len(twitter_ids)>len_twitter_ids:\n","                    branch_type += 'name'\n","                    len_twitter_ids = len(twitter_ids)\n","            twitter_ids = list(set(twitter_ids))\n","\n","            # if self.print_true:\n","            print(f'[INFO] branch_type: {branch_type}')\n","            print(f'[INFO] twitter_ids: {twitter_ids}')\n","\n","            if twitter_ids is None or len(twitter_ids) == 0:\n","                return []\n","\n","            # print(\"ranking by similarity\")\n","            twitter_ids = self._rank_by_similarity(twitter_ids, name=term, name_from_gs=scholar_search_result[0][\"name\"])\n","            # print(twitter_ids)\n","            return twitter_ids[:10] if len(twitter_ids)>=10 else twitter_ids\n","            # # elif type(twitter_ids) == dict:\n","            # twitter_ids_list = list(twitter_ids.keys())\n","            # # print(\"twitter_ids_list\", twitter_ids_list)\n","            # highest_occurrence = twitter_ids[twitter_ids_list[0]]\n","            # # print(\"highest_occurrence\", highest_occurrence)\n","            # candidate_list = []\n","            # for item in twitter_ids.items():\n","            #     # print(\"item[1] == highest_occurrence ?\", item, item[0], item[1], highest_occurrence)\n","            #     if item[1] == highest_occurrence:\n","            #         similarity_rank = self._rank_by_similarity(item[0], term, scholar_search_result[0][\"name\"])\n","            #         # print(\"similarity_rank\", item[0], term, scholar_search_result[0][\"name\"], similarity_rank)\n","            #         if similarity_rank >= 0.15:\n","            #         # if self._rank_by_similarity(item[0], term, scholar_search_result[0][\"name\"]) >= 0.15:\n","            #             candidate_list.append(item[0])\n","            # # print(\"candidate_list\", candidate_list)\n","            # if len(candidate_list) > 0:\n","            #     return self._rank_by_similarity(candidate_list, term, scholar_search_result[0]['name'])[0]\n","            # return None\n","            # else:\n","            #     return twitter_ids[0]\n","\n","            # TODO: match profile images of google_scholar/homepage with twitter profile images\n","            # result = self._search_name_helper(term)\n","            # return self.filter_result(result, term, web_source='google')\n","        elif str_type == 'gs_url':\n","            raise NotImplementedError\n","        else:\n","            raise NotImplementedError\n","\n","    def _rank_by_similarity(self, twitter_url_origin_list: Union[list, str], name: str=None, name_from_gs: str=None):\n","        # process name and name_from_gs\n","        if name is not None:\n","            name = re.sub('[0-9_\\., ]', '', name.lower())\n","        if name_from_gs is not None:\n","            name_from_gs = re.sub('[0-9_\\., ]', '', name_from_gs.lower())\n","\n","        if type(twitter_url_origin_list) == list:\n","            \n","            # else\n","            twitter_url_list = [re.sub('[0-9_\\., ]', '', item) for item in twitter_url_origin_list]\n","            twitter_url_map_dict = {re.sub('[0-9_\\., ]', '', item): item for item in twitter_url_origin_list}\n","            # rank twitter_url_origin_list\n","            if name is not None and name_from_gs is not None:\n","                twitter_url_list = sorted(twitter_url_list, key=lambda x: max(get_str_similarity(x, name), get_str_similarity(x, name_from_gs)), reverse=True)\n","            elif name is not None:\n","                twitter_url_list = sorted(twitter_url_list, key=lambda x: get_str_similarity(x, name), reverse=True)\n","            elif name_from_gs is not None:\n","                twitter_url_list = sorted(twitter_url_list, key=lambda x: get_str_similarity(x, name_from_gs), reverse=True)\n","            else:\n","                # do not consider this branch at the moment\n","                pass\n","            twitter_url_origin_list = [twitter_url_map_dict[item] for item in twitter_url_list]\n","            # print(\"twitter_url_origin_list\", twitter_url_origin_list)\n","            return twitter_url_origin_list\n","        else:\n","            twitter_url_origin_str = twitter_url_origin_list\n","            twitter_url_str = re.sub('[0-9_\\., ]', '', twitter_url_origin_str)\n","            rank = 0\n","            if name is not None and name_from_gs is not None:\n","                rank = max(get_str_similarity(twitter_url_str, name), get_str_similarity(twitter_url_origin_list, name_from_gs))\n","            elif name is not None:\n","                rank = get_str_similarity(twitter_url_str, name)\n","            elif name_from_gs is not None:\n","                rank = get_str_similarity(twitter_url_str, name_from_gs)\n","            \n","            return rank\n","\n","    def _search_twitter_from_homepage(self, homepage_url: str, name: str=None, name_from_gs: str=None):\n","        # get content of scholar homepage using chromedriver\n","        try:\n","            self.driver.implicitly_wait(5)\n","            self.driver.get(homepage_url)\n","        except WebDriverException as e:\n","            if self.print_true:\n","                print('[DEBUG] WebDriverException while getting homepage: %s' % homepage_url)\n","                print(e)\n","        time.sleep(8)\n","\n","        page = self.driver.page_source\n","        soup = BeautifulSoup(page, \"html.parser\")\n","        twitter_url_origin_list = list(set([re.findall('twitter.com/([^\\/?]+)', item['href'])[0]\n","            for item in soup.find_all(\n","                href=re.compile('twitter.com/([^\\/?]+)'))]))\n","        print(soup.find_all(\n","                href=re.compile('twitter.com/([^\\/?]+)')))\n","        \n","        # if there are no candidates for twitter account url, return None\n","        if len(twitter_url_origin_list) == 0:\n","            return None\n","        twitter_url_origin_list = self._rank_by_similarity(twitter_url_origin_list, name=name, name_from_gs=name_from_gs)\n","        if self.print_true:\n","            print(f'[DEBUG] Find a set of twitter ids on the provided homepage:\\n{twitter_url_origin_list}')\n","        # only return the highest rank twitter account id\n","        return twitter_url_origin_list\n","\n","    def _search_google_helper(self, google_url: str):\n","        self.driver.implicitly_wait(5)  \n","        self.driver.get(google_url)\n","        time.sleep(8)\n","\n","        page = self.driver.page_source\n","        soup = BeautifulSoup(page, \"html.parser\")\n","\n","        result_list = []\n","        result_block = soup.find_all('div', attrs={'class': 'g'})\n","        for result in result_block:\n","            # Find link, title, description\n","            link = result.find('a', href=True)\n","            title = result.find('h3')\n","            description_box = result.find(\n","                'div', {'style': '-webkit-line-clamp:2'})\n","            if link and title and description_box:\n","                result_list.append(get_search_result(\n","                    href=link['href'], title=title.text, description=description_box.text))\n","        if self.print_true:\n","            print(result_list)\n","        time.sleep(8)\n","        return result_list\n","        \n","\n","    def filter_result(self, result_list, term, web_source):\n","        \"\"\"\n","        web_source: google, twitter\n","        \"\"\"\n","        # sort twitter ids by occurrence frequency\n","        if web_source == 'google':\n","            twitter_id_dict = defaultdict(int)\n","            for result in result_list:\n","                if 'twitter.com/' in result['href']:\n","                    twitter_id_dict[re.findall('twitter.com/([^\\/?]+)', result['href'])[0]] += 1\n","        twitter_id_dict = dict(sorted(twitter_id_dict.items(), key=lambda item: item[1], reverse=True))\n","        # then, sort twitter ids by str similarity?\n","        # TODO\n","        # TODO: enter into twitter page to check profile information\n","        # Step 1: twitter profile vs google scholar profile\n","        # Step 2: twitter tweets: check whether google scholar domains are in twitter tweets\n","        # Step 3: twitter profile image (ask Yvonne about the performance)\n","        \n","        if len(twitter_id_dict) == 0:\n","            return None\n","        else:\n","            return twitter_id_dict\n","    \n","    def search_scholar_batch(self, name_list: list):\n","        # self.result_list = []\n","        with jsonlines.open(\"/content/drive/My Drive/tweets-dataset/gs_id2twitter_id_500k.csv\", 'a') as writer:\n","          for i, name in enumerate(name_list):\n","            t_id_name = self.search_scholar('name', name)\n","            writer.write((i, name, t_id_name))\n","            # self.result_list.append(t_id_name)\n","        # return self.result_list\n","\n","    def get_scholar_twitter(self, str_type: str, term: str, only_one=True):\n","        \"\"\"\n","        Final function that search a scholar's twitter account\n","        # TODO\n","        \"\"\"\n","        raise NotImplementedError\n","        result = self.search_scholar(str_type=str_type, term=term)\n","\n","        # first, google web search: name \"twitter\", get a list of top results, and check whatever name matches exactly\n","        # if matches, then get the twitter account id, use tweepy API search of the id to get the user profile and do further check\n","\n","        # if no matches, then search by name directly using Tweepy\n","\n","        # if there are candidates, do type 1, 2, 3 check of the result\n","        #\n","\n","        # '''\n","        #     The current code and the data is on the folder /cluster/project/sachan/zhiheng/twiteer at Euler server, because of the security reason, I save the twitter key as this structure, and use get_auth.py to load the key in the file. If you need more APIs, please contact me\n","        #     {\n","        #     \"API_key\" : \"ilH6jnBJdh9HQdsufmygvUwMB\",\n","        #     \"API_secret_key\" : \"LqErCdWfdP6BWf3LH3Q0RrJAXHoFvmweBUNtI1WljJ2A8SMelW\"\n","        #     }\n","        #     The current algorithm has the follow steps\n","        #     Step1: Find all twitter’s screen name by simply search the GS_name on twitter save_twitter_metainfo.py\n","        #     The problem now is that simply search the GS_name have a low recall rate, which is seen as the current bottleneck, about 52% of valid user loose in this step (see the below information)\n","        #     Step2: Use match_and_save.py to make a sketch match by the type 1, 2, 3 match and save those users tweets\n","        #     type 1: matched by personal website\n","        #     type 2: matched by keyword\n","        #     type3: matched by similar description with the information in GS\n","        #     Step3: Process the tweets (not important in current step)\n","        #     For the current 400 datapoint, there are 136 valid twitter accounts. I can match 20 of them by personal website, 36 of them by using type 1,2,3 match(with FN=20), and only 66 of them appeared in our search by users name (for example, if I simply search \"Mohammad Moradi\", I can not find the correspondent user moradideli by https://twitter.com/search?q=Mohammad%20Moradi&src=typed_query&f=user).\n","        #     8:45\n","        #     Here is some useful info about how a person annotator find the ground truth twitter user:\n","        #     8:46\n","        #     I followed the instruction in this doc by searching the name + Twitter in the Google first, and click top results to see if there is any match. If none, I will go search the name in Twitter and also browse through the top results. Sometimes I will also search their LinkedIn page to get their most up-to-date information. (the current institute in Google Scholar is not as accurate as their LinkedIn, and LinkedIn has a full history of where they worked. Moreover, they tend to put their photos in LinkedIn)\n","        # '''\n","\n","\n","# literature:\n","\n","# https://direct.mit.edu/qss/article/1/2/771/96149/Large-scale-identification-and-characterization-of"]},{"cell_type":"markdown","metadata":{"id":"6mK1zSXBe6NK"},"source":["### TwitterSearch-original"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"75S61xAce8dw"},"outputs":[],"source":["# import re\n","# import time\n","# from typing import Union\n","# from collections import defaultdict\n","# from selenium import webdriver\n","# from selenium.webdriver.chromium.webdriver import ChromiumDriver\n","# from selenium.webdriver.common.by import By\n","# from selenium.webdriver.remote.errorhandler import WebDriverException\n","# from bs4 import BeautifulSoup\n","\n","# url_search_dict = {\n","#     'google': 'https://www.google.com/search?q={0}'\n","# }\n","\n","\n","# def get_search_result(**kwargs):\n","#     return kwargs\n","\n","\n","# class TwitterSearch(GoogleSearch):\n","#     \"\"\"Class that handling searching on Google search bar using REST API.\"\"\"\n","\n","#     def __init__(self, driver_path):\n","#         super().__init__(driver_path)\n","#         self._urlsearch = url_search_dict['google']\n","#         self.print_true = True #NOTE: should be set by users\n","#         # initialize scholar_search object\n","#         self.scholar_search = ScholarSearch()\n","#         self.scholar_search.setup()\n","\n","#     def search_scholar(self, str_type: str, term: str):\n","#         \"\"\"\n","#         NOTE: now the only allowed str_type is name\n","#         \"\"\"\n","#         if str_type == 'name':\n","#             scholar_search_result = self.scholar_search.get_scholar(query=term, simple=True, top_n=1, print_true=True)\n","#             branch_type = None\n","#             if len(scholar_search_result) == 0:\n","#                 # directly search\n","#                 url_fragment = self._urlsearch.format(f'{term} \"twitter\"')\n","#                 result = self._search_google_helper(url_fragment)\n","#                 twitter_ids = self.filter_result(result, term, web_source='google')\n","#                 branch_type = 'directly search'\n","#             else:\n","#                 twitter_ids = None\n","#                 # try directly get twitter account through homepage\n","#                 if 'homepage_url' in scholar_search_result[0] and scholar_search_result[0]['homepage_url'] is not None:\n","#                     twitter_ids = self._search_twitter_from_homepage(scholar_search_result[0]['homepage_url'], name=term, name_from_gs=scholar_search_result[0][\"name\"])\n","#                 if twitter_ids is not None:\n","#                     branch_type = 'homepage'\n","\n","#                 # then try (google_name email_suffix \"twitter\")\n","#                 if \"email_info\" in scholar_search_result[0] and scholar_search_result[0][\"email_info\"] != '' and twitter_ids is None:\n","#                     url_fragment = self._urlsearch.format(f'{scholar_search_result[0][\"name\"]} {scholar_search_result[0][\"email_info\"]} \"twitter\"')\n","#                     result = self._search_google_helper(url_fragment)\n","#                     twitter_ids = self.filter_result(result, term, web_source='google')\n","#                 if twitter_ids is not None:\n","#                     branch_type = 'name + email'\n","\n","#                 # then try (google_name organization \"twitter\")\n","#                 if \"organization\" in scholar_search_result[0] and scholar_search_result[0][\"organization\"] != '' and twitter_ids is None:\n","#                     url_fragment = self._urlsearch.format(f'{scholar_search_result[0][\"name\"]} {scholar_search_result[0][\"organization\"]} \"twitter\"')\n","#                     result = self._search_google_helper(url_fragment)\n","#                     twitter_ids = self.filter_result(result, term, web_source='google')\n","#                 if twitter_ids is not None:\n","#                     branch_type = 'name + organization'\n","\n","#                 # then try (google_name \"twitter\")\n","#                 if twitter_ids is None:\n","#                     url_fragment = self._urlsearch.format(f'{scholar_search_result[0][\"name\"]} \"twitter\"')\n","#                     result = self._search_google_helper(url_fragment)\n","#                     twitter_ids = self.filter_result(result, term, web_source='google')\n","#                 if twitter_ids is not None:\n","#                     branch_type = 'name'\n","\n","\n","#             if self.print_true:\n","#                 print(f'[INFO] branch_type: {branch_type}')\n","#                 print(f'[INFO] twitter_ids: {twitter_ids}')\n","\n","#             if twitter_ids is None or len(twitter_ids) == 0:\n","#                 return None\n","#             elif type(twitter_ids) == dict:\n","#                 twitter_ids_list = list(twitter_ids.keys())\n","#                 highest_occurrence = twitter_ids[twitter_ids_list[0]]\n","#                 candidate_list = []\n","#                 for item in twitter_ids:\n","#                     if item[1] == highest_occurrence:\n","#                         if self._rank_by_similarity(item[0], term, scholar_search_result[0][\"name\"]) >= 0.15:\n","#                             candidate_list.append(item[0])\n","#                 if len(candidate_list) > 0:\n","#                     return self._rank_by_similarity(candidate_list, term, scholar_search_result[0]['name'])[0]\n","#                 return None\n","#             else:\n","#                 return twitter_ids[0]\n","\n","#             # TODO: match profile images of google_scholar/homepage with twitter profile images\n","#             # result = self._search_name_helper(term)\n","#             # return self.filter_result(result, term, web_source='google')\n","#         elif str_type == 'gs_url':\n","#             raise NotImplementedError\n","#         else:\n","#             raise NotImplementedError\n","\n","#     def _rank_by_similarity(self, twitter_url_origin_list: Union[list, str], name: str=None, name_from_gs: str=None):\n","#         # process name and name_from_gs\n","#         if name is not None:\n","#             name = re.sub('[0-9_\\., ]', '', name.lower())\n","#         if name_from_gs is not None:\n","#             name_from_gs = re.sub('[0-9_\\., ]', '', name_from_gs.lower())\n","\n","#         if type(twitter_url_origin_list) == list:\n","            \n","#             # else\n","#             twitter_url_list = [re.sub('[0-9_\\., ]', '', item) for item in twitter_url_origin_list]\n","#             twitter_url_map_dict = {re.sub('[0-9_\\., ]', '', item): item for item in twitter_url_origin_list}\n","#             # rank twitter_url_origin_list\n","#             if name is not None and name_from_gs is not None:\n","#                 twitter_url_list = sorted(twitter_url_list, key=lambda x: max(get_str_similarity(x, name), get_str_similarity(x, name_from_gs)), reverse=True)\n","#             elif name is not None:\n","#                 twitter_url_list = sorted(twitter_url_list, key=lambda x: get_str_similarity(x, name), reverse=True)\n","#             elif name_from_gs is not None:\n","#                 twitter_url_list = sorted(twitter_url_list, key=lambda x: get_str_similarity(x, name_from_gs), reverse=True)\n","#             else:\n","#                 # do not consider this branch at the moment\n","#                 pass\n","#             twitter_url_origin_list = [twitter_url_map_dict[item] for item in twitter_url_list]\n","#             return twitter_url_origin_list\n","#         else:\n","#             twitter_url_origin_str = twitter_url_origin_list\n","#             twitter_url_str = re.sub('[0-9_\\., ]', '', twitter_url_origin_str)\n","#             rank = 0\n","#             if name is not None and name_from_gs is not None:\n","#                 rank = max(get_str_similarity(twitter_url_str, name), get_str_similarity(twitter_url_origin_list, name_from_gs))\n","#             elif name is not None:\n","#                 rank = get_str_similarity(twitter_url_str, name)\n","#             elif name_from_gs is not None:\n","#                 rank = get_str_similarity(twitter_url_str, name_from_gs)\n","            \n","#             return rank\n","\n","#     def _search_twitter_from_homepage(self, homepage_url: str, name: str=None, name_from_gs: str=None):\n","#         # get content of scholar homepage using chromedriver\n","#         try:\n","#             self.driver.get(homepage_url)\n","#         except WebDriverException as e:\n","#             if self.print_true:\n","#                 print('[DEBUG] WebDriverException while getting homepage: %s' % homepage_url)\n","#                 print(e)\n","#         time.sleep(3)\n","\n","#         page = self.driver.page_source\n","#         soup = BeautifulSoup(page, \"html.parser\")\n","#         twitter_url_origin_list = list(set([re.findall('twitter.com/([^\\/?]+)', item['href'])[0]\n","#             for item in soup.find_all(\n","#                 href=re.compile('twitter.com/([^\\/?]+)'))]))\n","#         print(soup.find_all(\n","#                 href=re.compile('twitter.com/([^\\/?]+)')))\n","#         # if there are no candidates for twitter account url, return None\n","#         if len(twitter_url_origin_list) == 0:\n","#             # return soup\n","#             return None\n","\n","#         twitter_url_origin_list = self._rank_by_similarity(twitter_url_origin_list, name=name, name_from_gs=name_from_gs)\n","\n","#         if self.print_true:\n","#             print(f'[DEBUG] Find a set of twitter ids on the provided homepage:\\n{twitter_url_origin_list}')\n","        \n","#         # only return the highest rank twitter account id\n","#         return twitter_url_origin_list\n","\n","#     def _search_google_helper(self, google_url: str):\n","#         self.driver.get(google_url)\n","#         time.sleep(3)\n","\n","#         page = self.driver.page_source\n","#         soup = BeautifulSoup(page, \"html.parser\")\n","\n","#         result_list = []\n","#         result_block = soup.find_all('div', attrs={'class': 'g'})\n","#         for result in result_block:\n","#             # Find link, title, description\n","#             link = result.find('a', href=True)\n","#             title = result.find('h3')\n","#             description_box = result.find(\n","#                 'div', {'style': '-webkit-line-clamp:2'})\n","#             if link and title and description_box:\n","#                 result_list.append(get_search_result(\n","#                     href=link['href'], title=title.text, description=description_box.text))\n","#         if self.print_true:\n","#             print(result_list)\n","#         time.sleep(5)\n","#         return result_list\n","        \n","\n","#     def filter_result(self, result_list, term, web_source):\n","#         \"\"\"\n","#         web_source: google, twitter\n","#         \"\"\"\n","#         # sort twitter ids by occurrence frequency\n","#         if web_source == 'google':\n","#             twitter_id_dict = defaultdict(int)\n","#             for result in result_list:\n","#                 if 'twitter.com/' in result['href']:\n","#                     twitter_id_dict[re.findall('twitter.com/([^\\/?]+)', result['href'])[0]] += 1\n","#         twitter_id_dict = dict(sorted(twitter_id_dict.items(), key=lambda item: item[1], reverse=True))\n","#         # then, sort twitter ids by str similarity?\n","#         # TODO\n","#         # TODO: enter into twitter page to check profile information\n","#         # Step 1: twitter profile vs google scholar profile\n","#         # Step 2: twitter tweets: check whether google scholar domains are in twitter tweets\n","#         # Step 3: twitter profile image (ask Yvonne about the performance)\n","        \n","#         if len(twitter_id_dict) == 0:\n","#             return None\n","#         else:\n","#             return twitter_id_dict\n","    \n","#     def search_scholar_batch(self, name_list: list):\n","#         self.result_list = []\n","#         for name in name_list:\n","#             self.result_list.append(self.search_scholar('name', name))\n","#         return self.result_list\n","\n","#     def get_scholar_twitter(self, str_type: str, term: str, only_one=True):\n","#         \"\"\"\n","#         Final function that search a scholar's twitter account\n","#         # TODO\n","#         \"\"\"\n","#         raise NotImplementedError\n","#         result = self.search_scholar(str_type=str_type, term=term)\n","\n","#         # first, google web search: name \"twitter\", get a list of top results, and check whatever name matches exactly\n","#         # if matches, then get the twitter account id, use tweepy API search of the id to get the user profile and do further check\n","\n","#         # if no matches, then search by name directly using Tweepy\n","\n","#         # if there are candidates, do type 1, 2, 3 check of the result\n","#         #\n","\n","#         # '''\n","#         #     The current code and the data is on the folder /cluster/project/sachan/zhiheng/twiteer at Euler server, because of the security reason, I save the twitter key as this structure, and use get_auth.py to load the key in the file. If you need more APIs, please contact me\n","#         #     {\n","#         #     \"API_key\" : \"ilH6jnBJdh9HQdsufmygvUwMB\",\n","#         #     \"API_secret_key\" : \"LqErCdWfdP6BWf3LH3Q0RrJAXHoFvmweBUNtI1WljJ2A8SMelW\"\n","#         #     }\n","#         #     The current algorithm has the follow steps\n","#         #     Step1: Find all twitter’s screen name by simply search the GS_name on twitter save_twitter_metainfo.py\n","#         #     The problem now is that simply search the GS_name have a low recall rate, which is seen as the current bottleneck, about 52% of valid user loose in this step (see the below information)\n","#         #     Step2: Use match_and_save.py to make a sketch match by the type 1, 2, 3 match and save those users tweets\n","#         #     type 1: matched by personal website\n","#         #     type 2: matched by keyword\n","#         #     type3: matched by similar description with the information in GS\n","#         #     Step3: Process the tweets (not important in current step)\n","#         #     For the current 400 datapoint, there are 136 valid twitter accounts. I can match 20 of them by personal website, 36 of them by using type 1,2,3 match(with FN=20), and only 66 of them appeared in our search by users name (for example, if I simply search \"Mohammad Moradi\", I can not find the correspondent user moradideli by https://twitter.com/search?q=Mohammad%20Moradi&src=typed_query&f=user).\n","#         #     8:45\n","#         #     Here is some useful info about how a person annotator find the ground truth twitter user:\n","#         #     8:46\n","#         #     I followed the instruction in this doc by searching the name + Twitter in the Google first, and click top results to see if there is any match. If none, I will go search the name in Twitter and also browse through the top results. Sometimes I will also search their LinkedIn page to get their most up-to-date information. (the current institute in Google Scholar is not as accurate as their LinkedIn, and LinkedIn has a full history of where they worked. Moreover, they tend to put their photos in LinkedIn)\n","#         # '''\n","\n","\n","# # literature:\n","\n","# # https://direct.mit.edu/qss/article/1/2/771/96149/Large-scale-identification-and-characterization-of"]},{"cell_type":"markdown","metadata":{"id":"fTs792JvydZW"},"source":["###TwitterSearch - edited - v2 (with improvements)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AaDSu28Vyao0"},"outputs":[],"source":["import time\n","from typing import Union\n","from collections import defaultdict\n","from selenium import webdriver\n","from selenium.webdriver.chromium.webdriver import ChromiumDriver\n","from selenium.webdriver.common.by import By\n","from selenium.webdriver.remote.errorhandler import WebDriverException\n","from bs4 import BeautifulSoup\n","\n","url_search_dict = {\n","    'google': 'https://www.google.com/search?q={0}'\n","}\n","\n","\n","def get_search_result(**kwargs):\n","    return kwargs\n","\n","\n","class TwitterSearch(GoogleSearch):\n","    \"\"\"Class that handling searching on Google search bar using REST API.\"\"\"\n","\n","    def __init__(self, driver_path):\n","        super().__init__(driver_path)\n","        # print(\"GoogleSearch initiated\")\n","        self._urlsearch = url_search_dict['google']\n","        self.print_true = False #NOTE: should be set by users\n","        # initialize scholar_search object\n","        self.scholar_search = ScholarSearch()\n","        # print(\"ScholarSearch initiated\")\n","        self.scholar_search.setup()\n","        # print(\"setup done\")\n","\n","    def search_scholar(self, str_type: str, term: str):\n","        \"\"\"\n","        NOTE: now the only allowed str_type is name\n","        \"\"\"\n","        if str_type == 'name':\n","            scholar_search_result = self.scholar_search.get_scholar(query=term, simple=True, top_n=1, print_true=False)\n","            branch_type = \"\"\n","            if len(scholar_search_result) == 0:\n","                # directly search\n","                url_fragment = self._urlsearch.format(f'{term} \"twitter\"')\n","                result = self._search_google_helper(url_fragment)\n","                twitter_ids = self.filter_result(result, term, web_source='google')\n","                branch_type += 'directly search,'\n","            else:\n","                # twitter_ids = None\n","                twitter_ids = []\n","                len_twitter_ids = 0\n","                # try directly get twitter account through homepage\n","                if 'homepage_url' in scholar_search_result[0] and scholar_search_result[0]['homepage_url'] is not None:\n","                    new_ids = self._search_twitter_from_homepage(scholar_search_result[0]['homepage_url'], name=term, name_from_gs=scholar_search_result[0][\"name\"])\n","                    if new_ids:\n","                      twitter_ids.extend(new_ids)\n","                if len(twitter_ids)>len_twitter_ids:\n","                    branch_type += 'homepage,'\n","                    len_twitter_ids = len(twitter_ids)\n","\n","                # then try (google_name email_suffix \"twitter\")\n","                # print(\"searching name + email\")\n","                if \"email_info\" in scholar_search_result[0] and scholar_search_result[0][\"email_info\"] != '':# and twitter_ids is None:\n","                    url_fragment = self._urlsearch.format(f'{scholar_search_result[0][\"name\"]} {scholar_search_result[0][\"email_info\"]} \"twitter\"')\n","                    result = self._search_google_helper(url_fragment)\n","                    new_ids = self.filter_result(result, term, web_source='google')\n","                    if new_ids:\n","                      twitter_ids.extend(new_ids)\n","                # print(twitter_ids)\n","                if len(twitter_ids)>len_twitter_ids:\n","                    branch_type += 'name + email,'\n","                    len_twitter_ids = len(twitter_ids)\n","\n","                # then try (google_name organization \"twitter\")\n","                # print(\"searching name + organization\")\n","                if \"organization\" in scholar_search_result[0] and scholar_search_result[0][\"organization\"] != '':# and twitter_ids is None:\n","                    url_fragment = self._urlsearch.format(f'{scholar_search_result[0][\"name\"]} {scholar_search_result[0][\"organization\"]} \"twitter\"')\n","                    result = self._search_google_helper(url_fragment)\n","                    new_ids = self.filter_result(result, term, web_source='google')\n","                    if new_ids:\n","                      twitter_ids.extend(new_ids)\n","                # print(twitter_ids)\n","                if len(twitter_ids)>len_twitter_ids:\n","                    branch_type += 'name + organization,'\n","                    len_twitter_ids = len(twitter_ids)\n","\n","                # then try (google_name \"twitter\")\n","                # print(\"searching name \\\"twitter\\\"\")\n","                # if len(twitter_ids):\n","                # TODO - preprocess google_name to include only tokens which have length greater than one\n","                # gs_name = re.sub('[0-9_\\.,]', '', scholar_search_result[0][\"name\"])\n","                url_fragment = self._urlsearch.format(f'{scholar_search_result[0][\"name\"]} \"twitter\"')\n","                result = self._search_google_helper(url_fragment)\n","                new_ids = self.filter_result(result, term, web_source='google')\n","                if new_ids:\n","                      twitter_ids.extend(new_ids)\n","                # print(twitter_ids)\n","                if len(twitter_ids)>len_twitter_ids:\n","                    branch_type += 'name'\n","                    len_twitter_ids = len(twitter_ids)\n","            twitter_ids = list(set(twitter_ids))\n","\n","            # if self.print_true:\n","            print(f'[INFO] branch_type: {branch_type}')\n","            print(f'[INFO] twitter_ids: {twitter_ids}')\n","\n","            if twitter_ids is None or len(twitter_ids) == 0:\n","                return []\n","\n","            # print(\"ranking by similarity\")\n","            twitter_ids = self._rank_by_similarity(twitter_ids, name=term, name_from_gs=scholar_search_result[0][\"name\"])\n","            # print(twitter_ids)\n","            return twitter_ids[:10] if len(twitter_ids)>=10 else twitter_ids\n","            # # elif type(twitter_ids) == dict:\n","            # twitter_ids_list = list(twitter_ids.keys())\n","            # # print(\"twitter_ids_list\", twitter_ids_list)\n","            # highest_occurrence = twitter_ids[twitter_ids_list[0]]\n","            # # print(\"highest_occurrence\", highest_occurrence)\n","            # candidate_list = []\n","            # for item in twitter_ids.items():\n","            #     # print(\"item[1] == highest_occurrence ?\", item, item[0], item[1], highest_occurrence)\n","            #     if item[1] == highest_occurrence:\n","            #         similarity_rank = self._rank_by_similarity(item[0], term, scholar_search_result[0][\"name\"])\n","            #         # print(\"similarity_rank\", item[0], term, scholar_search_result[0][\"name\"], similarity_rank)\n","            #         if similarity_rank >= 0.15:\n","            #         # if self._rank_by_similarity(item[0], term, scholar_search_result[0][\"name\"]) >= 0.15:\n","            #             candidate_list.append(item[0])\n","            # # print(\"candidate_list\", candidate_list)\n","            # if len(candidate_list) > 0:\n","            #     return self._rank_by_similarity(candidate_list, term, scholar_search_result[0]['name'])[0]\n","            # return None\n","            # else:\n","            #     return twitter_ids[0]\n","\n","            # TODO: match profile images of google_scholar/homepage with twitter profile images\n","            # result = self._search_name_helper(term)\n","            # return self.filter_result(result, term, web_source='google')\n","        elif str_type == 'gs_url':\n","            raise NotImplementedError\n","        else:\n","            raise NotImplementedError\n","\n","    def _rank_by_similarity(self, twitter_url_origin_list: Union[list, str], name: str=None, name_from_gs: str=None):\n","        # process name and name_from_gs\n","        if name is not None:\n","            name = re.sub('[0-9_\\., ]', '', name.lower())\n","        if name_from_gs is not None:\n","            name_from_gs = re.sub('[0-9_\\., ]', '', name_from_gs.lower())\n","\n","        if type(twitter_url_origin_list) == list:\n","            \n","            # else\n","            twitter_url_list = [re.sub('[0-9_\\., ]', '', item) for item in twitter_url_origin_list]\n","            twitter_url_map_dict = {re.sub('[0-9_\\., ]', '', item): item for item in twitter_url_origin_list}\n","            # rank twitter_url_origin_list\n","            if name is not None and name_from_gs is not None:\n","                twitter_url_list = sorted(twitter_url_list, key=lambda x: max(get_str_similarity(x, name), get_str_similarity(x, name_from_gs)), reverse=True)\n","            elif name is not None:\n","                twitter_url_list = sorted(twitter_url_list, key=lambda x: get_str_similarity(x, name), reverse=True)\n","            elif name_from_gs is not None:\n","                twitter_url_list = sorted(twitter_url_list, key=lambda x: get_str_similarity(x, name_from_gs), reverse=True)\n","            else:\n","                # do not consider this branch at the moment\n","                pass\n","            twitter_url_origin_list = [twitter_url_map_dict[item] for item in twitter_url_list]\n","            # print(\"twitter_url_origin_list\", twitter_url_origin_list)\n","            return twitter_url_origin_list\n","        else:\n","            twitter_url_origin_str = twitter_url_origin_list\n","            twitter_url_str = re.sub('[0-9_\\., ]', '', twitter_url_origin_str)\n","            rank = 0\n","            if name is not None and name_from_gs is not None:\n","                rank = max(get_str_similarity(twitter_url_str, name), get_str_similarity(twitter_url_origin_list, name_from_gs))\n","            elif name is not None:\n","                rank = get_str_similarity(twitter_url_str, name)\n","            elif name_from_gs is not None:\n","                rank = get_str_similarity(twitter_url_str, name_from_gs)\n","            \n","            return rank\n","\n","    def _search_twitter_from_homepage(self, homepage_url: str, name: str=None, name_from_gs: str=None):\n","        # get content of scholar homepage using chromedriver\n","        try:\n","            self.driver.implicitly_wait(5)\n","            self.driver.get(homepage_url)\n","        except WebDriverException as e:\n","            if self.print_true:\n","                print('[DEBUG] WebDriverException while getting homepage: %s' % homepage_url)\n","                print(e)\n","        time.sleep(8)\n","\n","        page = self.driver.page_source\n","        soup = BeautifulSoup(page, \"html.parser\")\n","        twitter_url_origin_list = list(set([re.findall('twitter.com/([^\\/?]+)', item['href'])[0]\n","            for item in soup.find_all(\n","                href=re.compile('twitter.com/([^\\/?]+)'))]))\n","        print(soup.find_all(\n","                href=re.compile('twitter.com/([^\\/?]+)')))\n","        \n","        # if there are no candidates for twitter account url, return None\n","        if len(twitter_url_origin_list) == 0:\n","            return None\n","        twitter_url_origin_list = self._rank_by_similarity(twitter_url_origin_list, name=name, name_from_gs=name_from_gs)\n","        if self.print_true:\n","            print(f'[DEBUG] Find a set of twitter ids on the provided homepage:\\n{twitter_url_origin_list}')\n","        # only return the highest rank twitter account id\n","        return twitter_url_origin_list\n","\n","    def _search_google_helper(self, google_url: str):\n","        self.driver.implicitly_wait(5)  \n","        self.driver.get(google_url)\n","        time.sleep(8)\n","\n","        page = self.driver.page_source\n","        soup = BeautifulSoup(page, \"html.parser\")\n","\n","        result_list = []\n","        result_block = soup.find_all('div', attrs={'class': 'g'})\n","        for result in result_block:\n","            # Find link, title, description\n","            link = result.find('a', href=True)\n","            title = result.find('h3')\n","            description_box = result.find(\n","                'div', {'style': '-webkit-line-clamp:2'})\n","            if link and title and description_box:\n","                result_list.append(get_search_result(\n","                    href=link['href'], title=title.text, description=description_box.text))\n","        if self.print_true:\n","            print(result_list)\n","        time.sleep(8)\n","        return result_list\n","        \n","\n","    def filter_result(self, result_list, term, web_source):\n","        \"\"\"\n","        web_source: google, twitter\n","        \"\"\"\n","        # sort twitter ids by occurrence frequency\n","        if web_source == 'google':\n","            twitter_id_dict = defaultdict(int)\n","            for result in result_list:\n","                if 'twitter.com/' in result['href']:\n","                    twitter_id_dict[re.findall('twitter.com/([^\\/?]+)', result['href'])[0]] += 1\n","        twitter_id_dict = dict(sorted(twitter_id_dict.items(), key=lambda item: item[1], reverse=True))\n","        # then, sort twitter ids by str similarity?\n","        # TODO\n","        # TODO: enter into twitter page to check profile information\n","        # Step 1: twitter profile vs google scholar profile\n","        # Step 2: twitter tweets: check whether google scholar domains are in twitter tweets\n","        # Step 3: twitter profile image (ask Yvonne about the performance)\n","        \n","        if len(twitter_id_dict) == 0:\n","            return None\n","        else:\n","            return twitter_id_dict\n","    \n","    def search_scholar_batch(self, name_list: list):\n","        # self.result_list = []\n","        with jsonlines.open(\"/content/drive/My Drive/tweets-dataset/gs_id2twitter_id_500k.csv\", 'a') as writer:\n","          for i, name in enumerate(name_list):\n","            t_id_name = self.search_scholar('name', name)\n","            writer.write((i, name, t_id_name))\n","            # self.result_list.append(t_id_name)\n","        # return self.result_list\n","\n","    def get_scholar_twitter(self, str_type: str, term: str, only_one=True):\n","        \"\"\"\n","        Final function that search a scholar's twitter account\n","        # TODO\n","        \"\"\"\n","        raise NotImplementedError\n","        result = self.search_scholar(str_type=str_type, term=term)\n","\n","        # first, google web search: name \"twitter\", get a list of top results, and check whatever name matches exactly\n","        # if matches, then get the twitter account id, use tweepy API search of the id to get the user profile and do further check\n","\n","        # if no matches, then search by name directly using Tweepy\n","\n","        # if there are candidates, do type 1, 2, 3 check of the result\n","        #\n","\n","        # '''\n","        #     The current code and the data is on the folder /cluster/project/sachan/zhiheng/twiteer at Euler server, because of the security reason, I save the twitter key as this structure, and use get_auth.py to load the key in the file. If you need more APIs, please contact me\n","        #     {\n","        #     \"API_key\" : \"ilH6jnBJdh9HQdsufmygvUwMB\",\n","        #     \"API_secret_key\" : \"LqErCdWfdP6BWf3LH3Q0RrJAXHoFvmweBUNtI1WljJ2A8SMelW\"\n","        #     }\n","        #     The current algorithm has the follow steps\n","        #     Step1: Find all twitter’s screen name by simply search the GS_name on twitter save_twitter_metainfo.py\n","        #     The problem now is that simply search the GS_name have a low recall rate, which is seen as the current bottleneck, about 52% of valid user loose in this step (see the below information)\n","        #     Step2: Use match_and_save.py to make a sketch match by the type 1, 2, 3 match and save those users tweets\n","        #     type 1: matched by personal website\n","        #     type 2: matched by keyword\n","        #     type3: matched by similar description with the information in GS\n","        #     Step3: Process the tweets (not important in current step)\n","        #     For the current 400 datapoint, there are 136 valid twitter accounts. I can match 20 of them by personal website, 36 of them by using type 1,2,3 match(with FN=20), and only 66 of them appeared in our search by users name (for example, if I simply search \"Mohammad Moradi\", I can not find the correspondent user moradideli by https://twitter.com/search?q=Mohammad%20Moradi&src=typed_query&f=user).\n","        #     8:45\n","        #     Here is some useful info about how a person annotator find the ground truth twitter user:\n","        #     8:46\n","        #     I followed the instruction in this doc by searching the name + Twitter in the Google first, and click top results to see if there is any match. If none, I will go search the name in Twitter and also browse through the top results. Sometimes I will also search their LinkedIn page to get their most up-to-date information. (the current institute in Google Scholar is not as accurate as their LinkedIn, and LinkedIn has a full history of where they worked. Moreover, they tend to put their photos in LinkedIn)\n","        # '''\n","\n","\n","# literature:\n","\n","# https://direct.mit.edu/qss/article/1/2/771/96149/Large-scale-identification-and-characterization-of"]},{"cell_type":"markdown","metadata":{"id":"ivicehnTyRRO"},"source":["#Get top 10 names"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vb7EKi2fZltc"},"outputs":[],"source":["# del ts"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zo7uH-sfyUBn","executionInfo":{"status":"ok","timestamp":1677159521888,"user_tz":-330,"elapsed":73187,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}},"outputId":"f941ed3f-806e-40d5-8735-f83a7db315d7"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-8-8b5ec295fe04>:27: DeprecationWarning: headless property is deprecated, instead use add_argument('--headless') or add_argument('--headless=new')\n","  options.headless = True\n","<ipython-input-8-8b5ec295fe04>:35: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n","  self.driver = webdriver.Chrome(driver_path, options=options)\n"]}],"source":["ts = TwitterSearch('/usr/bin/chromedriver')\n","# df = pd.read_csv('/content/drive/My Drive/tweets-dataset/gs_scholars_matched_with_twitter_accounts_500.csv', index_col=0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KsNdniiFW5cp"},"outputs":[],"source":["num = 0\n","num_empty = 0\n","with open(\"/content/drive/My Drive/tweets-dataset/gs_scholars_matched_with_twitter_accounts_500.tsv\", \"r\") as fr, open('/content/drive/My Drive/tweets-dataset/gs_scholars_candidate_twitter_accounts.tsv','a') as fw:\n","  reader = csv.reader(fr, delimiter=\"\\t\")\n","  fw.write('\\n')\n","  for i, line in enumerate(reader):\n","    index = line[0]\n","    if i==0 or int(index)<=494:\n","      continue\n","    name = line[1]\n","    org = line[2]\n","    gs_url = line[3]\n","    twitter_url = line[4]\n","    if i>0:\n","      # get top 10 candidates for twitter IDs\n","      candidate_twitter_ids = ts.search_scholar('name', name)\n","      num+=1\n","      if candidate_twitter_ids:\n","        # save list of twitter IDs in the CSV\n","        line.extend(candidate_twitter_ids)\n","      else:\n","        if num_empty>3:\n","        #   print(\"Re-starting TwitterSearch . . . . . . \")\n","        #   del ts\n","        #   ts = TwitterSearch('/usr/bin/chromedriver')\n","        #   num_empty = 0\n","        # else:\n","          time.sleep(5)\n","        print(i)\n","    fw.write(\"\\t\".join(line))\n","    fw.write('\\n')\n","    # time.sleep(3)"]},{"cell_type":"markdown","metadata":{"id":"iIl812kGQR2a"},"source":["# Run google scholar search"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCvkhtbhOrxh"},"outputs":[],"source":["resp = scholar_search.get_scholar(query='Asifullah Khan', simple=True, top_n=1, print_true=True)\n","resp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j44ssKbFOrxg"},"outputs":[],"source":["scholar_search = ScholarSearch()\n","scholar_search.setup()"]},{"cell_type":"markdown","metadata":{"id":"z7SUA2oFqicu"},"source":["# Precision@10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"976wM-M4qlgi"},"outputs":[],"source":["ts = TwitterSearch('/usr/bin/chromedriver')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QELKCJrzqpxW"},"outputs":[],"source":["df = pd.read_csv('/content/drive/My Drive/tweets-dataset/gs_scholars_matched_with_twitter_accounts_500.csv', index_col=0)\n","# df = pd.read_csv('tmp/gs_scholars_matched_with_twitter_accounts_500 - gs_scholars_1k.csv', index_col=0)\n","df['t_id'] = df['url of their twitter'].apply(lambda x : x.split(\".com/\")[-1] if x==x else x)\n","\n","\n","name_list = df.head(100)['name'].values.tolist()\n","df.head()"]},{"cell_type":"markdown","metadata":{"id":"b9c5CEQtVvm7"},"source":["# Run CSV test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Do60OD7r354g"},"outputs":[],"source":["ts = TwitterSearch('chromedriver')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":333},"executionInfo":{"elapsed":1438,"status":"ok","timestamp":1673447166479,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"HWYa3qT9Orxj","outputId":"fa2cb263-5d5d-4757-d66d-286a311e95fa"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-2a76a0d5-4e62-4ed2-a6a2-5e1ab23b2c61\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>organization</th>\n","      <th>url</th>\n","      <th>url of their twitter</th>\n","      <th>t_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sebastian Gerke</td>\n","      <td>Ree Technology</td>\n","      <td>https://scholar.google.com/citations?hl=en&amp;use...</td>\n","      <td>https://twitter.com/sebgerke</td>\n","      <td>sebgerke</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Yang Liu</td>\n","      <td>Computer Science, harbin institute of technology</td>\n","      <td>https://scholar.google.com/citations?hl=en&amp;use...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Son N. Tran</td>\n","      <td>University of Tasmania</td>\n","      <td>https://scholar.google.com/citations?hl=en&amp;use...</td>\n","      <td>https://twitter.com/sondinhtran</td>\n","      <td>sondinhtran</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Massimiliano Ciaramita</td>\n","      <td>Google</td>\n","      <td>https://scholar.google.com/citations?hl=en&amp;use...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Marco Baity-Jesi</td>\n","      <td>Eawag</td>\n","      <td>https://scholar.google.com/citations?hl=en&amp;use...</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a76a0d5-4e62-4ed2-a6a2-5e1ab23b2c61')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2a76a0d5-4e62-4ed2-a6a2-5e1ab23b2c61 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2a76a0d5-4e62-4ed2-a6a2-5e1ab23b2c61');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                     name                                      organization  \\\n","0         Sebastian Gerke                                    Ree Technology   \n","1                Yang Liu  Computer Science, harbin institute of technology   \n","2             Son N. Tran                            University of Tasmania   \n","3  Massimiliano Ciaramita                                            Google   \n","4        Marco Baity-Jesi                                             Eawag   \n","\n","                                                 url  \\\n","0  https://scholar.google.com/citations?hl=en&use...   \n","1  https://scholar.google.com/citations?hl=en&use...   \n","2  https://scholar.google.com/citations?hl=en&use...   \n","3  https://scholar.google.com/citations?hl=en&use...   \n","4  https://scholar.google.com/citations?hl=en&use...   \n","\n","              url of their twitter         t_id  \n","0     https://twitter.com/sebgerke     sebgerke  \n","1                              NaN          NaN  \n","2  https://twitter.com/sondinhtran  sondinhtran  \n","3                              NaN          NaN  \n","4                              NaN          NaN  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# first 500 are hand-labeled\n","df = pd.read_csv('/content/drive/My Drive/tweets-dataset/gs_scholars_matched_with_twitter_accounts_500.csv', index_col=0)\n","# df = pd.read_csv('tmp/gs_scholars_matched_with_twitter_accounts_500 - gs_scholars_1k.csv', index_col=0)\n","df['t_id'] = df['url of their twitter'].apply(lambda x : x.split(\".com/\")[-1] if x==x else x)\n","name_list = df.head(100)['name'].values.tolist()\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cLEBO0ZmhZ_u"},"outputs":[],"source":[" ts.search_scholar_batch(name_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1673451473358,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"zAFNfVdnOrxj","outputId":"8cef50c6-4106-4d3a-a9ef-7a5f242c6b2e"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-190a6a29-6fdc-41bb-8214-6af487004ada\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>twitter_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sebastian Gerke</td>\n","      <td>sebasgerke]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Yang Liu</td>\n","      <td>air_tsinghua]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Son N. Tran</td>\n","      <td>rj_saintgeorge]</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Massimiliano Ciaramita</td>\n","      <td>sobigdata]</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Marco Baity-Jesi</td>\n","      <td>PeterIsles]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-190a6a29-6fdc-41bb-8214-6af487004ada')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-190a6a29-6fdc-41bb-8214-6af487004ada button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-190a6a29-6fdc-41bb-8214-6af487004ada');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                      name        twitter_id\n","0          Sebastian Gerke       sebasgerke]\n","1                 Yang Liu     air_tsinghua]\n","2              Son N. Tran   rj_saintgeorge]\n","3   Massimiliano Ciaramita        sobigdata]\n","4         Marco Baity-Jesi       PeterIsles]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["result_name_df = pd.read_csv('/content/drive/My Drive/tweets-dataset/gs_id2twitter_id_500k.csv', names=[\"index\", \"name\", \"twitter_id\"])\n","result_name_df['index'] = result_name_df['index'].apply(lambda x : int(x[1:]))\n","result_name_df.drop(['index'], axis=1, inplace=True)\n","result_name_df['name'] = result_name_df['name'].apply(lambda x : x.replace('\"', ''))\n","result_name_df['twitter_id'] = result_name_df['twitter_id'].apply(lambda x : x.replace('\"', ''))\n","result_name_df['twitter_id'] = result_name_df['twitter_id'].apply(lambda x : x.replace(']', ''))\n","result_name_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1673451843517,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"qaBeUaXMe2N-","outputId":"d696e18d-47d4-437d-d6d1-c07462ec4ab9"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-cf3e79c1-0179-4fd6-9190-328eb43fc3d4\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>name</th>\n","      <th>twitter_id</th>\n","      <th>pred</th>\n","      <th>actual</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sebastian Gerke</td>\n","      <td>sebasgerke</td>\n","      <td>sebasgerke</td>\n","      <td>sebgerke</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Yang Liu</td>\n","      <td>air_tsinghua</td>\n","      <td>air_tsinghua</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Son N. Tran</td>\n","      <td>rj_saintgeorge</td>\n","      <td>rj_saintgeorge</td>\n","      <td>sondinhtran</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Massimiliano Ciaramita</td>\n","      <td>sobigdata</td>\n","      <td>sobigdata</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Marco Baity-Jesi</td>\n","      <td>PeterIsles</td>\n","      <td>PeterIsles</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>Michel Tokic</td>\n","      <td>null</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>Giovanni Acampora</td>\n","      <td>null</td>\n","      <td>NaN</td>\n","      <td>g_acampora</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>Berdakh Abibullaev</td>\n","      <td>null</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>Xi Cheng</td>\n","      <td>null</td>\n","      <td>NaN</td>\n","      <td>xicheng33</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>Alexander Ihler</td>\n","      <td>null</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf3e79c1-0179-4fd6-9190-328eb43fc3d4')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-cf3e79c1-0179-4fd6-9190-328eb43fc3d4 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cf3e79c1-0179-4fd6-9190-328eb43fc3d4');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                       name       twitter_id             pred       actual\n","0           Sebastian Gerke       sebasgerke       sebasgerke     sebgerke\n","1                  Yang Liu     air_tsinghua     air_tsinghua          NaN\n","2               Son N. Tran   rj_saintgeorge   rj_saintgeorge  sondinhtran\n","3    Massimiliano Ciaramita        sobigdata        sobigdata          NaN\n","4          Marco Baity-Jesi       PeterIsles       PeterIsles          NaN\n","..                      ...              ...              ...          ...\n","95             Michel Tokic             null              NaN          NaN\n","96        Giovanni Acampora             null              NaN   g_acampora\n","97       Berdakh Abibullaev             null              NaN          NaN\n","98                 Xi Cheng             null              NaN    xicheng33\n","99          Alexander Ihler             null              NaN          NaN\n","\n","[100 rows x 4 columns]"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["result_name_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MpNd35r_ylxN"},"outputs":[],"source":["result_name_df['pred'] = result_name_df['twitter_id']\n","result_name_df['pred'] = result_name_df['pred'].apply(lambda x : np.NaN if x==' null' else x)\n","result_name_df['actual'] = df.head(len(result_name_df))['t_id'].values.tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UUTbHamAhovW"},"outputs":[],"source":["result_name_df['pred'] = result_name_df['pred'].apply(lambda x : x.replace(\" \",\"\") if x==x else x)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1673452253501,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"8H5220VXzdav","outputId":"e140db30-dfbc-4071-b4a5-744c7d139ffd"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-bbb20de9-0852-488f-a0c4-921be88596d2\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>pred</th>\n","      <th>actual</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>sebasgerke</td>\n","      <td>sebgerke</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>air_tsinghua</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>rj_saintgeorge</td>\n","      <td>sondinhtran</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>sobigdata</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>PeterIsles</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>NaN</td>\n","      <td>g_acampora</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>NaN</td>\n","      <td>xicheng33</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bbb20de9-0852-488f-a0c4-921be88596d2')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-bbb20de9-0852-488f-a0c4-921be88596d2 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-bbb20de9-0852-488f-a0c4-921be88596d2');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["              pred       actual\n","0       sebasgerke     sebgerke\n","1     air_tsinghua          NaN\n","2   rj_saintgeorge  sondinhtran\n","3        sobigdata          NaN\n","4       PeterIsles          NaN\n","..             ...          ...\n","95             NaN          NaN\n","96             NaN   g_acampora\n","97             NaN          NaN\n","98             NaN    xicheng33\n","99             NaN          NaN\n","\n","[100 rows x 2 columns]"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["# results = pd.DataFrame({'pred':list(result_name_df['pred']), 'actual': df.head(len(result_name_df))['t_id'].values.tolist()})\n","results = pd.DataFrame({'pred':result_name_df['pred'], 'actual':result_name_df['actual']})\n","results"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1673452258102,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"fNdzB7xLg_Oc","outputId":"85bfeab4-0bb9-45ef-bb4e-97e2de35abbe"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'@ahafizk20'"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["results['pred'][88]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1673452279619,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"Zxu4qoYTbiwr","outputId":"ca388e4f-ef64-465f-ab00-7c313f5750c0"},"outputs":[{"name":"stdout","output_type":"stream","text":["bshallamichhane nan\n","vikbar nan\n","hornungroman nan\n","simonwells nan\n","moinnadeem nan\n","skokrys nan\n","ornakl nan\n","jawad_alkhatib nan\n","othmansoufan nan\n","gbortsova nan\n","em_dinan nan\n","bennyplo nan\n","paragnamjoshi nan\n","dreavjr nan\n","fdelamuerte nan\n","ethan_ycx nan\n","briantelfer5 nan\n","gittakutyniok nan\n","margal_cartif nan\n","moradideli nan\n","g_acampora nan\n","xicheng33 nan\n","6 55 17 22\n"]}],"source":["tp, tn, fp, fn = 0, 0, 0, 0\n","for i in range (len(results)):\n","  result_name = results['pred'][i]\n","  t_id = results['actual'][i]\n","  if result_name==result_name:\n","    result_name = result_name.lower()\n","    if result_name[0] == \"@\":\n","      result_name = result_name[1:]\n","  if t_id==t_id:\n","    t_id = t_id.lower()\n","  # cases : actual, predicted\n","  # case 1 : None, None [tn]\n","  # case 2 : None, abc [fp]\n","  # case 3 : abc, None [fn]\n","  # case 4 : abc, xyz [fp]\n","  # case 4 : abc, abc [tp]\n","  # TODO - confirm if this is how we should look at tp?\n","  if result_name!=result_name:\n","    if t_id != t_id:\n","      tn += 1\n","    else:\n","      fn += 1\n","      print(t_id, result_name)\n","  else:\n","    # print(t_id, result_name)\n","    if t_id==t_id and result_name == t_id:\n","      tp += 1\n","    else:\n","      # print(t_id, result_name)\n","      fp += 1\n","\n","print(tp, tn, fp, fn)\n","precision = tp / (tp + fp)\n","recall = tp / (tp + fn)\n","f1 = (2*precision*recall) / (precision+recall)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":343,"status":"ok","timestamp":1673452284691,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"OTTVr8961fnH","outputId":"2abd7dcf-2daf-488b-d1b4-c50c8e840c15"},"outputs":[{"data":{"text/plain":["(0.2608695652173913, 0.21428571428571427, 0.23529411764705882)"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["precision, recall, f1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1673452286503,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"Ae0xsFI_hCOm","outputId":"54a256a1-f52c-4a4e-b2c3-0f70da1301d4"},"outputs":[{"data":{"text/plain":["0.61"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["accuracy = (tp + tn) / (tp + tn + fn + fp)\n","accuracy"]},{"cell_type":"markdown","metadata":{"id":"8Nu0Lbn8qqFM"},"source":["Algorithm Performance\n","\n","| algo | tp  |tn | fp | fn | Precision |  Recall | F1 | Accuracy|\n","|---|---|---|---|---|---|---|---|---|\n","| original (with error) |  5 |  69 |  2 |  28 |  0.7143 | 0.152  |  0.25 |  0.7115 |\n","| original (without error) | 6  | 55  |  17 |  22 | 0.26  | 0.21  | 0.235 | 0.61  |\n","|   |   |   |   |   |   |   |   |   |\n","\n","* Enhancement_1: ~if `twitter_ids_list` does not contain more than one candidate, no need to do similarity match.~ [doesn't work - hurts precision]\n","  * `if len(twitter_ids_list)==1: return twitter_ids_list[0]`"]},{"cell_type":"markdown","metadata":{"id":"QBbYU5CCOrxk"},"source":["# twitter search - experiment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F7iBtwQRoIYy"},"outputs":[],"source":["del ts"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":61174,"status":"ok","timestamp":1675786087544,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"pr5wI3DdOrxl","outputId":"9b3650f1-b3b9-4e6d-b8a1-238a7713d43b"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-8-685cab0f48b9>:40: DeprecationWarning: headless property is deprecated, instead use add_argument('--headless') or add_argument('--headless=new')\n","  options.headless = True\n","<ipython-input-8-685cab0f48b9>:41: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n","  self.driver = webdriver.Chrome(driver_path, options=options)\n"]},{"name":"stdout","output_type":"stream","text":["GoogleSearch initiated\n","ScholarSearch initiated\n","Scholar78kSearch initiated\n","ScholarGsSearch initiated\n","setup done\n"]}],"source":["ts = TwitterSearch('/usr/bin/chromedriver')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":55065,"status":"error","timestamp":1675786147627,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"DEw4yjD3LKrT","outputId":"419e0dc1-0ff6-40d3-ae7b-bf33697bd7a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["[Info] Found 1 scholars are in 78k data.\n","[Debug] Names: 0    Ziv Epstein\n","Name: name, dtype: object\n","[Info] Find 1 scholars using query without gs_sid in step 4.\n","[Info] In total 1 scholar is found:\n","[\n","  {\n","    \"url\": \"https://scholar.google.com/citations?hl=en&user=yG7l19UAAAAJ\",\n","    \"gs_sid\": \"yG7l19UAAAAJ\",\n","    \"coauthors\": [\n","      [\n","        \"https://scholar.google.com/citations?user=C0ANojIAAAAJ&hl=en\",\n","        \"David G. Rand\",\n","        \"Erwin H. Schell Professor, Massachusetts Institute of Technology\",\n","        \"Verified email at mit.edu\"\n","      ],\n","      [\n","        \"https://scholar.google.com/citations?user=AIbJenwAAAAJ&hl=en\",\n","        \"Gordon Pennycook\",\n","        \"Associate Professor, University of Regina\",\n","        \"Verified email at uregina.ca\"\n","      ],\n","      [\n","        \"https://scholar.google.com/citations?user=Axm9X-QAAAAJ&hl=en\",\n","        \"Antonio Alonso Arechar\",\n","        \"Economist\",\n","        \"Verified email at arechar.com\"\n","      ],\n","      [\n","        \"https://scholar.google.com/citations?user=Zri-8PwAAAAJ&hl=en\",\n","        \"Matthew Groh\",\n","        \"MIT\",\n","        \"Verified email at mit.edu\"\n","      ],\n","      [\n","        \"https://scholar.google.com/citations?user=oNQRPLYAAAAJ&hl=en\",\n","        \"Dean Eckles\",\n","        \"MIT\",\n","        \"Verified email at mit.edu\"\n","      ],\n","      [\n","        \"https://scholar.google.com/citations?user=DAivUTUAAAAJ&hl=en\",\n","        \"Mohsen Mosleh\",\n","        \"MIT - Sloan School of Management & Exeter Business School\",\n","        \"Verified email at mit.edu\"\n","      ],\n","      [\n","        \"https://scholar.google.com/citations?user=yKCX2IUAAAAJ&hl=en\",\n","        \"Iyad Rahwan\",\n","        \"Center for Humans & Machines, Max Planck Institute for Human Development\",\n","        \"Verified email at mpib-berlin.mpg.de\"\n","      ],\n","      [\n","        \"https://scholar.google.com/citations?user=ZkBTPkUAAAAJ&hl=en\",\n","        \"Manuel Cebrian\",\n","        \"Universidad Carlos III de Madrid\",\n","        \"Verified email at uc3m.es\"\n","      ],\n","      [\n","        \"https://scholar.google.com/citations?user=KJNUEgkAAAAJ&hl=en\",\n","        \"Abhimanyu Dubey\",\n","        \"Facebook AI Research\",\n","        \"Verified email at fb.com\"\n","      ],\n","      [\n","        \"https://scholar.google.com/citations?user=nxJvvD0AAAAJ&hl=en\",\n","        \"Bjarke Felbo\",\n","        \"Massachusetts Institute of Technology\",\n","        \"Verified email at felbo.dk\"\n","      ],\n","      [\n","        \"https://scholar.google.com/citations?user=wFN9eKUAAAAJ&hl=en\",\n","        \"Blakeley Hoffman Payne\",\n","        \"University of Colorado at Boulder\",\n","        \"Verified email at colorado.edu\"\n","      ],\n","      [\n","        \"https://scholar.google.com/citations?user=LCjSZ3eS8pIC&hl=en\",\n","        \"Judy Hanwen Shen\",\n","        \"Stanford University\",\n","        \"Verified email at stanford.edu\"\n","      ],\n","      [\n","        \"https://scholar.google.com/citations?user=rvHmi4YAAAAJ&hl=en\",\n","        \"William Padula\",\n","        \"University of Southern California\",\n","        \"Verified email at usc.edu\"\n","      ],\n","      [\n","        \"https://scholar.google.com/citations?user=zwLePrsAAAAJ&hl=en\",\n","        \"Alex Peysakhovich\",\n","        \"Research Scientist, FAIR\",\n","        \"Verified email at fb.com\"\n","      ],\n","      [\n","        \"https://scholar.google.com/citations?user=9mSx8yIAAAAJ&hl=en\",\n","        \"Oc\\u00e9ane Boulais\",\n","        \"Researcher, MIT Media Lab\",\n","        \"Verified email at media.mit.edu\"\n","      ],\n","      [\n","        \"https://scholar.google.com/citations?user=g0x9raEAAAAJ&hl=en\",\n","        \"Skylar Gordon\",\n","        \"Computation and Cognition, MIT\",\n","        \"Verified email at mit.edu\"\n","      ],\n","      [\n","        \"https://scholar.google.com/citations?user=Yt2H6lwAAAAJ&hl=en\",\n","        \"Sydney Levine\",\n","        \"Research Scientist, Allen Institute for AI (Research Affiliate: Harvard & MIT)\",\n","        \"Verified email at mit.edu\"\n","      ],\n","      [\n","        \"https://scholar.google.com/citations?user=P4nfoKYAAAAJ&hl=en\",\n","        \"Alex `Sandy' Pentland\",\n","        \"Professor MIT\",\n","        \"Verified email at mit.edu\"\n","      ],\n","      [\n","        \"https://scholar.google.com/citations?user=sjaZfDIAAAAJ&hl=en\",\n","        \"Esteban Moro\",\n","        \"Associate Professor, Department Mathematics, Universidad Carlos III de Madrid\",\n","        \"Verified email at math.uc3m.es\"\n","      ],\n","      [\n","        \"https://scholar.google.com/citations?user=Dq0Fom8AAAAJ&hl=en\",\n","        \"Dmitriy Smirnov\",\n","        \"Research Scientist, Netflix\",\n","        \"Verified email at netflix.com\"\n","      ]\n","    ],\n","    \"citation_table\": [\n","      \"1069\",\n","      \"13\"\n","    ],\n","    \"cites\": {\n","      \"years\": [\n","        \"2014\",\n","        \"2015\",\n","        \"2016\",\n","        \"2017\",\n","        \"2018\",\n","        \"2019\",\n","        \"2020\",\n","        \"2021\",\n","        \"2022\",\n","        \"2023\"\n","      ],\n","      \"cites\": [\n","        \"3\",\n","        \"11\",\n","        \"26\",\n","        \"27\",\n","        \"64\",\n","        \"40\",\n","        \"87\",\n","        \"252\",\n","        \"483\",\n","        \"53\"\n","      ]\n","    },\n","    \"name\": \"Ziv Epstein\",\n","    \"organization\": \"MIT\",\n","    \"homepage_url\": \"http://zive.info/\",\n","    \"email_info\": \"mit.edu\",\n","    \"domain_labels\": [\n","      \"computational social science\",\n","      \"social media\",\n","      \"artificial intelligence\",\n","      \"systems design\"\n","    ],\n","    \"extra_co_authors\": []\n","  }\n","]\n","searching homepage\n","[]\n","[]\n","searching name + email\n","[{'href': 'https://twitter.com/medialab/status/1013530209629458433?lang=bg', 'title': 'MIT Media Lab в Twitter: „Visualizing Scissors Congruence, a ...', 'description': \"Jul 1, 2018 — Visualizing Scissors Congruence, a project from Scalable Cooperation Student Ziv Epstein, is in The National Science Foundation's Vizzies\\xa0...\"}, {'href': 'http://zive.info/', 'title': 'Ziv Epstein', 'description': 'Ziv Epstein. twitter ... Ziv Epstein (he/him) is a PhD student in the MIT Media Lab, where he is co-advised by David Rand and Sandy Pentland.'}, {'href': 'https://news.mit.edu/2021/social-media-false-news-reminders-0317', 'title': 'A remedy for the spread of false news? - MIT News', 'description': 'Mar 17, 2021 — ... Ziv Epstein, a PhD candidate at the MIT Media Lab; Mohsen Mosleh, a lecturer at ... as well as a field experiment conducted on Twitter.'}, {'href': 'https://www.media.mit.edu/groups/center-for-constructive-communication/projects/', 'title': 'Projects ‹ MIT Center for Constructive Communication', 'description': 'Ziv Epstein · Hope Schroeder · Project Research ... Divergences in Following Patterns between Influential Twitter Users and their Audiences.'}, {'href': 'https://web.media.mit.edu/~zive/research_statement.pdf', 'title': 'Research Statement - MIT Media Lab', 'description': 'Ziv Epstein   zive@mit.edu   zive.info ... main finding with a large-scale Twitter field experiment interacting with over 100k users, and found.'}, {'href': 'https://www.media.mit.edu/publications/shifting-attention-to-accuracy-can-reduce-misinformation-online/', 'title': 'Shifting attention to accuracy can reduce misinformation online', 'description': 'Mar 17, 2021 — To shed light on this apparent contradiction, we carried out four survey experiments and a field experiment on Twitter; the results show\\xa0...'}, {'href': 'https://www.media.mit.edu/research/?filter=projects&tag=social-media', 'title': 'Projects - Research — MIT Media Lab', 'description': 'Matthew Groh · Ziv Epstein +1 more ... Divergences in Following Patterns between Influential Twitter Users and their Audiences.'}]\n","['medialab']\n","searching name + organization\n","[{'href': 'https://twitter.com/medialab/status/1013530209629458433?lang=bg', 'title': 'MIT Media Lab в Twitter: „Visualizing Scissors Congruence, a ...', 'description': 'За да продължиш да използваш twitter.com, активирай JavaScript или премини към поддържан браузър. Можеш да видиш списък с поддържаните браузъри в Центъра ни\\xa0...'}, {'href': 'http://zive.info/', 'title': 'Ziv Epstein', 'description': 'Ziv Epstein. twitter ... Ziv Epstein (he/him) is a PhD student in the MIT Media Lab, where he is co-advised by David Rand and Sandy Pentland.'}, {'href': 'https://www.media.mit.edu/groups/center-for-constructive-communication/projects/', 'title': 'Projects ‹ MIT Center for Constructive Communication', 'description': 'Ziv Epstein · Hope Schroeder · Project Research ... Divergences in Following Patterns between Influential Twitter Users and their Audiences.'}, {'href': 'https://web.media.mit.edu/~zive/research_statement.pdf', 'title': 'Research Statement - MIT Media Lab', 'description': 'Ziv Epstein   zive@mit.edu   zive.info ... main finding with a large-scale Twitter field experiment interacting with over 100k users, and found.'}, {'href': 'https://news.mit.edu/2021/social-media-false-news-reminders-0317', 'title': 'A remedy for the spread of false news? - MIT News', 'description': 'Mar 17, 2021 — ... Ziv Epstein, a PhD candidate at the MIT Media Lab; Mohsen Mosleh, a lecturer at ... as well as a field experiment conducted on Twitter.'}, {'href': 'https://www.media.mit.edu/publications/shifting-attention-to-accuracy-can-reduce-misinformation-online/', 'title': 'Shifting attention to accuracy can reduce misinformation online', 'description': 'Mar 17, 2021 — To shed light on this apparent contradiction, we carried out four survey experiments and a field experiment on Twitter; the results show\\xa0...'}, {'href': 'https://www.media.mit.edu/research/?filter=everything&tag=social-media', 'title': 'social media - Research — MIT Media Lab', 'description': 'Matthew Groh · Ziv Epstein +1 more ... Social Mirror is a web application that helps Twitter users interactively explore the politically active parts of\\xa0...'}]\n","['medialab', 'medialab']\n","searching name \"twitter\"\n","[{'href': 'https://twitter.com/_ziv_e/status/1372226538674876419', 'title': 'Zivvy Ξpstein on Twitter: \"       Today, our paper on distraction ...', 'description': 'Mar 17, 2021 — Out now in Nature! A fundamentally new way of fighting misinfo online: Surveys+field exp w >5k Twitter users show that gently nudging users\\xa0...'}, {'href': 'http://zive.info/', 'title': 'Ziv Epstein', 'description': 'Ziv Epstein. twitter ... Ziv Epstein (he/him) is a PhD student in the MIT Media Lab, where he is co-advised by David Rand and Sandy Pentland.'}, {'href': 'https://dblp.org/pid/181/4635.html', 'title': 'Ziv Epstein - dblp', 'description': 'Nov 10, 2022 — List of computer science publications by Ziv Epstein. ... share record. Twitter; Reddit; BibSonomy; LinkedIn; Facebook. persistent URL:.'}, {'href': 'https://tsjournal.org/index.php/jots/article/download/39/26', 'title': \"Twitter's Disputed Tags May Be Ineffective at Reducing Belief ...\", 'description': 'by J Lees · 2022 · Cited by 2 — that the tags employed by Twitter to combat the spread of fake news may ... Epstein, Ziv, Adam J. Berinsky, Rocky Cole, Andrew Gully, Gordon Pennycook,\\xa0...'}, {'href': 'https://www.researchgate.net/scientific-contributions/Ziv-Epstein-2140252774', 'title': \"Ziv Epstein's research works | Massachusetts Institute of ...\", 'description': 'One solution that is gaining attention among academics and technology companies involves harnessing the \"wisdom of crowds\" (e.g., Twitter\\'s Birdwatch\\xa0...'}, {'href': 'https://www.si.umich.edu/about-umsi/events/ds/css-seminar-ziv-epstein', 'title': 'DS/CSS Seminar: Ziv Epstein | umsi - University of Michigan', 'description': 'Apr 8, 2021 — To shed light on this apparent contradiction, we carried out four survey experiments and a field experiment on Twitter; the results show\\xa0...'}, {'href': 'https://www.nature.com/articles/s41586-021-03344-2', 'title': 'Shifting attention to accuracy can reduce misinformation online', 'description': 'by G Pennycook · 2021 · Cited by 426 — Surveys and a field experiment with Twitter users show that prompting ... These authors contributed equally: Gordon Pennycook, Ziv Epstein,\\xa0...'}]\n","['medialab', 'medialab', '_ziv_e']\n","[INFO] branch_type: name + email,name + organization,name\n","[INFO] twitter_ids: ['_ziv_e', 'medialab']\n","ranking by similarity\n","['_ziv_e', 'medialab']\n"]},{"ename":"AttributeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-8f9be3fe61fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Ziv Epstein - medialab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Rove Chishman - rovechishman\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_scholar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Ziv Epstein\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-01eb1a7775cf>\u001b[0m in \u001b[0;36msearch_scholar\u001b[0;34m(self, str_type, term)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwitter_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;31m# elif type(twitter_ids) == dict:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mtwitter_ids_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwitter_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;31m# print(\"twitter_ids_list\", twitter_ids_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mhighest_occurrence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtwitter_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtwitter_ids_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"]}],"source":["# Sebastian Gerke - sebgerke\n","# Son N. Tran - rj_saintgeorge\n","# Ziv Epstein - medialab\n","# Rove Chishman - rovechishman\n","name = ts.search_scholar('name', \"Ziv Epstein\")\n","name"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3ACpyWlo1YT"},"outputs":[],"source":["def _rank_by_similarity(twitter_url_origin_list: Union[list, str], name: str=None, name_from_gs: str=None):\n","    # process name and name_from_gs\n","    if name is not None:\n","        name = re.sub('[0-9_\\., ]', '', name.lower())\n","    if name_from_gs is not None:\n","        name_from_gs = re.sub('[0-9_\\., ]', '', name_from_gs.lower())\n","\n","    if type(twitter_url_origin_list) == list:\n","        print(\"type is list\")\n","        # else\n","        twitter_url_list = [re.sub('[0-9_\\., ]', '', item) for item in twitter_url_origin_list]\n","        print(twitter_url_list)\n","        twitter_url_map_dict = {re.sub('[0-9_\\., ]', '', item): item for item in twitter_url_origin_list}\n","        print(twitter_url_map_dict)\n","        # rank twitter_url_origin_list\n","        if name is not None and name_from_gs is not None:\n","            twitter_url_list = sorted(twitter_url_list, key=lambda x: max(get_str_similarity(x, name), get_str_similarity(x, name_from_gs)), reverse=True)\n","            print(1, twitter_url_list) \n","        elif name is not None:\n","            twitter_url_list = sorted(twitter_url_list, key=lambda x: get_str_similarity(x, name), reverse=True)\n","            print(2, twitter_url_list) \n","        elif name_from_gs is not None:\n","            twitter_url_list = sorted(twitter_url_list, key=lambda x: get_str_similarity(x, name_from_gs), reverse=True)\n","            print(3, twitter_url_list) \n","        else:\n","            # do not consider this branch at the moment\n","            pass\n","        twitter_url_origin_list = [twitter_url_map_dict[item] for item in twitter_url_list]\n","        print(twitter_url_origin_list)\n","        # print(\"twitter_url_origin_list\", twitter_url_origin_list)\n","        return twitter_url_origin_list\n","    else:\n","        print(\"type is not list\")\n","        twitter_url_origin_str = twitter_url_origin_list\n","        twitter_url_str = re.sub('[0-9_\\., ]', '', twitter_url_origin_str)\n","        rank = 0\n","        if name is not None and name_from_gs is not None:\n","            rank = max(get_str_similarity(twitter_url_str, name), get_str_similarity(twitter_url_origin_list, name_from_gs))\n","        elif name is not None:\n","            rank = get_str_similarity(twitter_url_str, name)\n","        elif name_from_gs is not None:\n","            rank = get_str_similarity(twitter_url_str, name_from_gs)\n","        \n","        return rank"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1563,"status":"ok","timestamp":1673443643987,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"OdJoadgyAlpZ","outputId":"4336f061-4b19-407a-bb94-af031dc6028a"},"outputs":[{"data":{"text/plain":["['Son', 'Tran']"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["name_tokens = re.sub('[0-9_\\.,]', '', \"Son N. Tran\").split(\" \")\n","name_tokens = [token for token in name_tokens if len(token)>1]\n","name_tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1673442557236,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"C_yMh1tH8WE4","outputId":"109ff1f7-b4aa-45b3-a98a-3b23ba83c2ec"},"outputs":[{"data":{"text/plain":["0.8421052631578947"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["rank = ts._rank_by_similarity(\"sondinhtran\", \"Son N. Tran\", \"Son N. Tran\")\n","rank"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XimvbM7YOrxl","outputId":"1661e106-2bd1-475c-9136-02aff67032fb"},"outputs":[{"name":"stdout","output_type":"stream","text":["[]\n"]}],"source":["# soup, tmp_list = ts._search_twitter_from_homepage('https://jermainewang.github.io/', name='Minjie Wang')\n","tmp = ts._search_twitter_from_homepage('http://zive.info/', name='Zivvy Epstein')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R91_D3cIOrxm","outputId":"93c4b299-942c-4556-8425-d2488e55d90e"},"outputs":[{"name":"stdout","output_type":"stream","text":["<a href=\"https://twitter.com/GraphDeep/\">twitter account</a>\n"]}],"source":["import re\n","print(soup.find_all(href=re.compile('twitter.com/'))[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ty_fmNAzOrxm","outputId":"7fb758c6-266b-4a84-d0f7-b0dd60d5c828"},"outputs":[{"name":"stdout","output_type":"stream","text":["https://twitter.com/minjie_nyu\n"]}],"source":["print(soup.find_all(href=re.compile('twitter.com/'))[0]['href'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJX1ZGgFOrxm"},"outputs":[],"source":["from selenium.webdriver.common.by import By"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hz6CFYozOrxm"},"outputs":[],"source":["from bs4 import BeautifulSoup\n","page = ts.driver.page_source\n","soup = BeautifulSoup(page, \"html.parser\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9t7X2UxOrxm","outputId":"cf04cc0b-d2e6-44db-cda3-ca35a51bff49"},"outputs":[{"name":"stdout","output_type":"stream","text":["https://twitter.com/zhijingjin?lang=en\n","\n","\n","https://twitter.com/ZhijingJin/status/1422153131585150979\n","\n","\n","https://twitter.com/ZhijingJin/status/1543329155923599360\n","\n","\n","https://twitter.com/ZhijingJin/status/1471233274311811073\n","\n","\n","https://twitter.com/ZhijingJin/status/1552356661829914634\n","\n","\n","https://twitter.com/ZhijingJin/status/1555126680557166595\n","\n","\n","https://mobile.twitter.com/ZhijingJin/status/1529038162034577408\n","\n","\n","https://twitter.com/ZhijingJin/status/1413750729991499777\n","\n","\n","https://twitter.com/zhijingjin/status/1524075310475620355\n","\n","\n","https://twitter.com/ZhijingJin/status/1559966514500866048\n","\n","\n"]}],"source":["result_block = soup.find_all('div', attrs={'class': 'g'})\n","for result in result_block:\n","    # Find link, title, description\n","    # print(result)\n","    link = result.find('a', href=True)\n","    title = result.find('h3')\n","    description_box = result.find('div', {'style': '-webkit-line-clamp:2'})\n","    if link and title and description_box:\n","        # description = description_box.find('span')\n","        # if link and title and description:\n","            # result_list.append(SearchResult(link['href'], title.text, description.text))\n","            # print(link['href'], title.text, description.text)\n","        # print(description_box.text)\n","        print(link['href'])\n","        print('\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5se_7B9KOrxn","outputId":"fd2f9451-61c0-4f12-a589-c851d0aefa10"},"outputs":[{"name":"stdout","output_type":"stream","text":["Zhijing Jin (@ZhijingJin) / Twitter ...\n","Jul 27, 2022 — \n","Mar 18, 2022 — \n","Nov 12, 2021 — \n","May 22, 2022 — \n","PhD at Max Planck Institute & ETH, affiliated at UMich (2020-2024 expected) Email: jinzhi@ethz.ch ‍  Google Scholar / CV / twitter-icon @ZhijingJin\n"]}],"source":["result_block = soup.find_all('div', attrs={'class': 'g'})\n","for result in result_block:\n","    # Find link, title, description\n","    # print(result)\n","    link = result.find('a', href=True)\n","    title = result.find('h3')\n","    description_box = result.find('div', {'style': '-webkit-line-clamp:2'})\n","    if description_box:\n","        description = description_box.find('span')\n","        if link and title and description:\n","            # result_list.append(SearchResult(link['href'], title.text, description.text))\n","            # print(link['href'], title.text, description.text)\n","            print(description.text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sbtq8D6OOrxn"},"outputs":[],"source":["result = driver.find_elements(By.TAG_NAME, 'div')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nYR8_tSvOrxo"},"outputs":[],"source":["result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Na1SrMAaOrxo"},"outputs":[],"source":["for res_item in result:\n","    # print(res_item.get_attribute('class'))\n","    print(res_item.get_attribute('textContent'))\n","    # print(res_item.find_elements(By.CLASS_NAME, 'g'))"]},{"cell_type":"markdown","metadata":{"id":"gbrEpCGZ4hHN"},"source":["# Error analysis"]},{"cell_type":"markdown","metadata":{"id":"u6MEc0ahmrVN"},"source":["#### Ideas for algorithm improvement\n","- Sometimes the organisation name contains current position, abbreviations, punctuation marks etc. which may not give good search results on google search. (eg - `PhD, University of Warwick, UK`, `EPFL <= Xerox Research <= IIT Kanpur`, `University of Niš, Faculty of Sciences and Mathematics, Serbia`, `CTO & Co-Founder @ Tensoreye`)Can include a city/country if mentioned in the organisation and search for - `name city/country twitter`\n","- Organisation name on google scholar may not be the latest. LinkedIn search helps - can include list of organisations instead of single organisation and run multiple google searchs with `name organisation_1 twitter`, `name organisation_2 twitter`, ..\n","- Name on google scholar and twitter may not match exactly if they have middle names, abbreviations in names etc. This affects the quality of search. Can take only first and last name while searching: split on \" \" and take first and last token of name\n","- For filtering out best candidate, can use twitter bio description and look for words similar to ML/AI/Computer Science/ etc [can have list of keywords here]\n","- For people with common names or similar twitter handle names for all candidates: Look for latest tweets/ re-tweets to find keywords indicating AI/ML domain like conference names, domain keywords etc.\n","- Only one branch is explore i.e. searching for only one of `name org twitter`, `name twitter` etc. - explore all branches, take, most common candidates, do similarity check on this set"]},{"cell_type":"markdown","metadata":{"id":"tU2pZSAYqpGe"},"source":["##### Examples: Errors and improvements\n","* Simple search `name twitter` gives the top result but not predicted. Why?\n","\n","  * Case 1: top 1 result - `Benny Lo twitter` search results\n","  \n","\n","    Benny Lo (@bennyplo) / Twitterhttps://twitter.com › bennyplo\n","    Benny Lo's Tweets ... Prof. Burdet! ... discussing engineering\n","    solutions to understand food intake in nutrition and future\n","    initiatives. Thank you for organising it!\n","    `\"name\": \"Benny Lo\",\n","\n","  **Need to pick handle after `https://twitter.com ›`**\n","\n","  * Case 2: top-n result but only one matches the name - `Ziv Epstein twitter` search results\n","\n","    \n","    MIT Media Lab on Twitter: \"In a new paper, Ziv Epstein ...\n","    https://twitter.com › medialab › status\n","    16-Sept-2020 — In a new paper, Ziv Epstein, Sydney Levine, David \n","    Rand, and Iyad Rahwan consider the question, \"Who gets credit \n","    for AI-generated art?\".\n","    _________________________________________________________\n","    Zivvy Ξpstein on Twitter: \" Today, our paper on distraction ...\n","    https://twitter.com › _ziv_e › status\n","    17-Mar-2021 — The paper shows how people have the capacity to \n","    discern truth from fals\n","\n","  **Need to pick this second result** _need to take top_n and some nested results and then do fuzzy name match with twitter name or ID_\n","  \n","  Algo change - `name on twitter`; fuzzy name match with original name and choose the twitter handle between `medialab` and `_ziv_e`\n","\n","  * Case 3: top-n result but all names match - `Son Tran twitter` search results (_actual GS name is Son N. Tran - searching on Son Tran gives better and different results than searching on Son N. Tran_)\n","    \n","    \n","    Son Tran (@sontrantuan) / Twitterhttps://twitter.com › \n","    sontrantuan Son Tran. @sontrantuan. A perpetual TCK constantly \n","    on the move.. London, England Joined February 2012.\n","    _________________________________________________________\n","    https://twitter.com › sondinhtran See new Tweets. Opens profile \n","    photo. Follow. Click to Follow sondinhtran. Son tran. \n","    @sondinhtran. Joined August 2011. \n","    _________________________________________________________\n","    Son Tran (@tuanson94) / Twitterhttps://mobile.twitter.com › \n","    tuanson94 See new Tweets. Opens profile photo. Follow. Click to \n","    Follow tuanson94. Son Tran. @tuanson94. Son. Joined September 2021.\n","    \n","  **Correct result [2nd] - need to look for domain-specific keywords in latest tweets/retweets**    \n","\n","* `organization` too long - can do NER on it to pick up relevant org name\n","\n","    \n","    \"name\": \"Yujia Xie\",\n","    \"organization\": \"Researcher in machine learning, Microsoft\",\n","      \"homepage_url\": null,\n","      \"email_info\": \"microsoft.com\",\n","      \"domain_labels\": [\n","        \"machine learning\",\n","        \"deep learning\",\n","        \"optimal transport\"\n","      ],\n","      \"extra_co_authors\": []\n","    }\n","      ]\n","    []\n","    []\n","    []\n","    [INFO] branch_type: None\n","    [INFO] twitter_ids: None\n","\n","* Actual - `ahafizk20` ; predicted : `@ahafizk20` -> remove `@` in front of handle names\n","* [Ignore - was due to error in the code] ~Although correct twitter ID is predicted, the candidate is rejected because of~\n","`if self._rank_by_similarity(item[0], term, scholar_search_result[0][\"name\"]) >= 0.15: candidate_list.append(item[0])`\n","  * can tune similarity threshold\n","  * improve similarity matching - can match part of first name and last name\n","\n","\n","* Generate better candidates \n","  * Candidates for `Son N. Tran`; actual `twitter ID=sondinhtranran`\n","  \n","  \n","        {'tasschoolmed': 1, 'rj_saintgeorge': 1, 'search': 1, 'alana_mann': 1, 'thetassiegirl': 1, 'tassiethinker': 1, 'janealty1': 1}\n"]},{"cell_type":"markdown","metadata":{"id":"NRuSvP8qpcNB"},"source":["#### Error patterns\n","- False Positives: Predicting wrong twitter handle when actual\n","  - does not exist: \n","  - exists and is different from the predicted: \n","- False Negatives: Does not predict any twitter handle"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VyQ-LTXgoyIl"},"outputs":[],"source":["df = pd.read_csv('/content/drive/My Drive/tweets-dataset/gs_scholars_candidate_twitter_accounts-v1-all-branches.tsv',sep='\\t')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PQ08QEb8o_zh"},"outputs":[],"source":["from pandas.core.internals.managers import ensure_block_shape\n","gs_link_prefix = \"https://scholar.google.com/citations?user=\" \n","twitter_link_prefix = \"https://twitter.com/\"\n","\n","twitter_id_in_top_10 = 0\n","twitter_id_in_top_1 = 0\n","twitter_id_with_no_candidates = 0\n","twitter_id_not_in_candidates = 0\n","null_id_with_no_candidates = 0\n","null_id_with_candidates = 0\n","\n","indices_not_found = []\n","\n","with open(\"/content/drive/My Drive/tweets-dataset/gs_scholars_candidate_twitter_accounts-v1-all-branches.tsv\", \"r\") as f:\n","  reader = csv.reader(f, delimiter=\"\\t\")\n","  for i, line in enumerate(reader):\n","    if i==0:\n","      continue\n","    index = line[0]\n","    name = line[1]\n","    org = line[2]\n","    gs_url = line[3]\n","    twitter_url = line[5]\n","    candidate_ids = [id.lower() for id in line[6:] if len(id)>0]\n","    \n","    if twitter_url==\"N/A\":\n","      if len(candidate_ids) == 0:\n","        null_id_with_no_candidates += 1\n","      else:\n","        null_id_with_candidates += 1\n","    else:\n","      twitter_id = twitter_url.split(\"twitter.com/\")[-1]\n","      if len(candidate_ids) == 0:\n","        twitter_id_with_no_candidates += 1\n","      else:\n","        if twitter_id.lower() in candidate_ids:\n","          twitter_id_in_top_10 += 1\n","          if twitter_id.lower() == candidate_ids[0]:\n","            twitter_id_in_top_1 += 1\n","        else:\n","          twitter_id_not_in_candidates += 1"]},{"cell_type":"code","source":["print(f'twitter_id_in_top_10 :          {twitter_id_in_top_10}')\n","print(f'twitter_id_in_top_1 :           {twitter_id_in_top_1}')\n","print(f'twitter_id_with_no_candidates : {twitter_id_with_no_candidates}')\n","print(f'twitter_id_not_in_candidates :  {twitter_id_not_in_candidates}')\n","print(f'null_id_with_no_candidates :    {null_id_with_no_candidates}')\n","print(f'null_id_with_candidates :       {null_id_with_candidates}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y6oi4SSd7ULE","executionInfo":{"status":"ok","timestamp":1677686826957,"user_tz":-330,"elapsed":6,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}},"outputId":"c6c0015f-b97c-4b10-b988-c14a181de47e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["twitter_id_in_top_10 :          129\n","twitter_id_in_top_1 :           86\n","twitter_id_with_no_candidates : 5\n","twitter_id_not_in_candidates :  34\n","null_id_with_no_candidates :    81\n","null_id_with_candidates :       252\n"]}]},{"cell_type":"code","source":["precision_at_10 = twitter_id_in_top_10 / (twitter_id_in_top_10 + twitter_id_not_in_candidates + null_id_with_candidates)\n","precision_at_10"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ooyN37NKaKRL","executionInfo":{"status":"ok","timestamp":1677686845937,"user_tz":-330,"elapsed":1195,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}},"outputId":"577b7a00-5d90-44d1-8752-b34f874f8ec0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.3108433734939759"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["recall_at_10 = twitter_id_in_top_10 / (twitter_id_in_top_10 + twitter_id_with_no_candidates + twitter_id_not_in_candidates)\n","recall_at_10"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S9Io7KRndT9J","executionInfo":{"status":"ok","timestamp":1677686846346,"user_tz":-330,"elapsed":10,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}},"outputId":"baf75e52-efa5-4827-d22d-670a9e0c6536"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.7678571428571429"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["f_at_10 = (2*precision_at_10*recall_at_10) / (precision_at_10 + recall_at_10)\n","f_at_10"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o5TTjiArdwQq","executionInfo":{"status":"ok","timestamp":1677686846743,"user_tz":-330,"elapsed":6,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}},"outputId":"ce588916-c15c-446d-9958-f65cf89b22d1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.44253859348198976"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["precision_at_1 = twitter_id_in_top_1 / (twitter_id_in_top_10 + twitter_id_not_in_candidates + null_id_with_candidates)\n","precision_at_1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c_sbp2Euajt8","executionInfo":{"status":"ok","timestamp":1677686846743,"user_tz":-330,"elapsed":5,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}},"outputId":"18ab4b2c-54bb-4de9-dc50-e8777ae145fa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.20722891566265061"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["recall_at_1 = twitter_id_in_top_1 / (twitter_id_in_top_10 + twitter_id_with_no_candidates + twitter_id_not_in_candidates)\n","recall_at_1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EGoIoGEYazaZ","executionInfo":{"status":"ok","timestamp":1677686850305,"user_tz":-330,"elapsed":7,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}},"outputId":"739b53e9-511f-4f6d-9bd2-dd84f5ddbb21"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5119047619047619"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["f_at_1 = (2*precision_at_1*recall_at_1) / (precision_at_1 + recall_at_1)\n","f_at_1"],"metadata":{"id":"P-rWb6EJh94R","executionInfo":{"status":"ok","timestamp":1677686850305,"user_tz":-330,"elapsed":6,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"}},"outputId":"e42e1ade-8f69-4757-f50e-ad37ecf142b3","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.2950257289879932"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"YG20hFZoYDKh"},"source":["* 32 - improve string matching algo to avoid mapping clashes between different twitter IDs\n","* 37\n","* 68 - Search for -> firstname website(after http://) \"twitter\"\n","* 76 -  name extracted-entit-from-org \"twitter\"\n","```\n","import spacy\n","nlp = spacy.load('en_core_web_sm')\n","doc = nlp(\"Senior Staff Member, MIT Lincoln Laboratory\")\n","doc.ents[0]\n","```\n","* 110 - correct\n","* 113\n","* 115\n","* 134 - Search for -> firstname website(after http://) \"twitter\"\n","```\n","ans = re.search('[a-zA-Z\\s]*', 'Xiaocheng Tang (唐小程)').group(0).strip()\n","```\n","* 150 - improve string matching algo, match twitter bio with google scholar info, frequency of appearing in google search result is important\n","* 178 - is a protected account\n","* 200 - search name on twitter - use tweepy\n","* 202 - username is very different from name (uomian49, Mushtaq Raza) - improve ranking algo\n","* 222 - exclude words of length one from the name\n","_twitter name is different from gs scholar name (Benjamin J Ainscough, Ben Ainscough) - excluding 'J' from googlesearch gives correct result_\n","* 224 - name (William R. Gray-Roncal, Ph.D.) and organisation name too complex (Johns Hopkins University Applied Physics Laboratory) - looks like they have 2 accounts - wgrbrains, WillGrayRoncal- also ambigous in tagging \n","* 226 - extract email from 'email' (Verified email at umich.edu - <a href=\"http://vivekjoshi.ai/\" rel=\"nofollow\" class=\"gsc_prf_ila\">Homepage</a>)\n","```\n","re.search(\"Verified email at ([^\\s]*)\", 'Verified email at umich.edu - <a href=\"http://vivekjoshi.ai/\" rel=\"nofollow\" class=\"gsc_prf_ila\">Homepage</a>').group(0).split(\"Verified email at \")[-1]\n","```\n","* 229 - private account\n","* 241 - search twitter users, browse through latest few tweets to find AI/ML/conference keywords\n","* 248 - twitter user is very unlikely to be found by search (has 0 following, 0 followers, only 2 tweets since 2010)\n","* 255 - search on twitter, match expanded URL with homepage, description with GS bio\n","* 259 - search on twitter, match user._json['entities']['url']['urls'][0]['expanded_url'] or ['display_url] with homepage, 'description' with GS bio (people who do not have recent tweets or have 0 tweets are not shown in google search)\n","```\n","print(user._json['description'])\n","print(user._json['entities']['url']['urls'][0]['display_url'])\n","print(user._json['entities']['url']['urls'][0]['expanded_url'])\n","```\n","```\n","re.search(\"href=\\\"([^\\s]*)\\\"\", gs_scholar['email']).group(0).split(\"href=\")[-1].replace(\"\\\"\",\"\").split(\"://\")[-1].strip(\"/\") == \"https://www.rcv.sejong.ac.kr\".split(\"://\")[-1].strip(\"/\")\n","```\n","* 264 - search on twitter, look for ML/AI related keywords in description and latest few tweets\n","* 271 - search on twitter\n","* 284 - private account\n","* 288 - \n","* 290\n","* 297\n","* 309\n","* 341 - private account\n","* 354 - \n","* 364 - "]},{"cell_type":"markdown","metadata":{"id":"H9Ls6k6lYDCo"},"source":[]},{"cell_type":"markdown","metadata":{"id":"YzGtaaqfJ22P"},"source":["# Test"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":415,"status":"ok","timestamp":1676303639801,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"2kyoVRoha8gz","outputId":"ff8a8323-d022-4e40-f3c5-810741923fd2"},"outputs":[{"data":{"text/plain":["['32',\n"," '37',\n"," '68',\n"," '76',\n"," '110',\n"," '113',\n"," '115',\n"," '134',\n"," '150',\n"," '178',\n"," '200',\n"," '202',\n"," '222',\n"," '224',\n"," '226',\n"," '229',\n"," '241',\n"," '248',\n"," '255',\n"," '259',\n"," '264',\n"," '271']"]},"execution_count":195,"metadata":{},"output_type":"execute_result"}],"source":["indices_not_found"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68317,"status":"ok","timestamp":1676833306844,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"ZneOBag1VtSX","outputId":"1165d271-644c-4fe2-8048-3853ce3e0550"},"outputs":[{"name":"stderr","output_type":"stream","text":["<ipython-input-8-8b5ec295fe04>:27: DeprecationWarning: headless property is deprecated, instead use add_argument('--headless') or add_argument('--headless=new')\n","  options.headless = True\n","<ipython-input-8-8b5ec295fe04>:35: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n","  self.driver = webdriver.Chrome(driver_path, options=options)\n"]}],"source":["ts = TwitterSearch('/usr/bin/chromedriver')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rysy-TABNESO"},"outputs":[],"source":["gs_scholar = ts.scholar_search.get_scholar(query=\"Hao DONG\", simple=True, top_n=1, print_true=False)[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1676807242629,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"Wj5xcDGqOhJM","outputId":"7110a3fa-09c8-4ffc-d824-cae2d7c1018a"},"outputs":[{"data":{"text/plain":["{'academic': True,\n"," 'academic_age': 11.0,\n"," 'academic_lifespan': 8.0,\n"," 'cit_sum_before_year': {'2013': 2,\n","  '2014': 5,\n","  '2015': 7,\n","  '2016': 11,\n","  '2017': 16,\n","  '2018': 35,\n","  '2019': 73,\n","  '2020': 112,\n","  '2021': 151},\n"," 'citation_table': ['161', '7'],\n"," 'domain_labels': ['remote sensing',\n","  'artificial intelligence',\n","  'machine learning',\n","  'pattern recognition'],\n"," 'gender': 'M',\n"," 'gs_sid': 'gC1ZxXUAAAAJ',\n"," 'organization': 'Ph.D., Wuhan University',\n"," 'organization_code': '5183021838615386716',\n"," 'paper_num': 14,\n"," 'cites': {'years': ['2013',\n","   '2014',\n","   '2015',\n","   '2016',\n","   '2017',\n","   '2018',\n","   '2019',\n","   '2020',\n","   '2021',\n","   '2022'],\n","  'cites': ['2', '3', '2', '4', '5', '19', '38', '39', '39', '10']},\n"," 'coauthors': None,\n"," 'name': 'Hao DONG',\n"," 'url': 'https://scholar.google.com/citations?hl=en&user=gC1ZxXUAAAAJ',\n"," 'top_num_coauthors': nan,\n"," 'num_domain': 4.0,\n"," 'top_topic_diversity': nan,\n"," 'num_coauthors_all_wwo_ai': 22,\n"," 'organization_top': 0.0,\n"," 'email': 'Verified email at whu.edu.cn'}"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["gs_scholar"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":335,"status":"ok","timestamp":1676805390183,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"Xkzuie1rP2hK","outputId":"721c876e-f280-43ac-87fc-a9583101b493"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Verified email at iarai.ac.at - <a href=\"https://yonghaoxu.github.io/\" rel=\"nofollow\" class=\"gsc_prf_ila\">Homepage</a>'"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["gs_scholar['email']"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":386,"status":"ok","timestamp":1676302871189,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"KdMxlzDavRiZ","outputId":"bedf3f5c-4716-47a6-a5b9-18c3b727a1fb"},"outputs":[{"data":{"text/plain":["(Department of Applied Information Engineering, Suwa University of Science)"]},"execution_count":176,"metadata":{},"output_type":"execute_result"}],"source":["nlp('Professor, Department of Applied Information Engineering, Suwa University of Science').ents[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8764,"status":"ok","timestamp":1676442497825,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"Hp3gXnggwdiP","outputId":"8dfbe9e7-f337-4c9f-8e0a-a78ce7b8ba18"},"outputs":[{"name":"stdout","output_type":"stream","text":["[<a href=\"https://twitter.com/githubstatus\">@githubstatus</a>]\n"]},{"data":{"text/plain":["['githubstatus']"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["ts._search_twitter_from_homepage('http://ben.ainscough.net/', name='Benjamin J Ainscough', name_from_gs='Benjamin J Ainscough')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5or9qa6jU7LA"},"outputs":[],"source":["# name = re.search('[a-zA-Z\\s]*', 'Xiaocheng Tang (唐小程)').group(0).strip()\n","url_fragment = ts._urlsearch.format(f'Vivek Joshi umich.edu \"twitter\"')\n","result = ts._search_google_helper(url_fragment)\n","twitter_ids = ts.filter_result(result, \"Vivek Joshi\", web_source='google')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":410,"status":"ok","timestamp":1676803473452,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"mInLBWJpK-7g","outputId":"ec520587-1acd-4fd4-f8fb-7a8cca032ec7"},"outputs":[{"name":"stdout","output_type":"stream","text":["('vivek_jj', 1)\n"]}],"source":["for item in twitter_ids.items():\n","  print(item)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":181},"executionInfo":{"elapsed":417,"status":"error","timestamp":1676442753262,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"M0JZKUh-ZpDp","outputId":"a94146c2-61d9-4dc6-9233-7f395e3bbde6"},"outputs":[{"ename":"AttributeError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-2ebadaf0d1c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtwitter_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'items'"]}],"source":["for item in twitter_ids.items():\n","  print(item)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":611,"status":"ok","timestamp":1676442879669,"user":{"displayName":"Navreet Kaur","userId":"16808969901099687634"},"user_tz":-330},"id":"yXyHEXxzxyEt","outputId":"002a2de2-0937-493f-80ef-2516c2dbe15d"},"outputs":[{"name":"stdout","output_type":"stream","text":["('brianjnowak', 1)\n"]}],"source":["for item in twitter_ids.items():\n","  print(item)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YFQT5RmUsNU_"},"outputs":[],"source":["t_ids = ts.search_scholar('name', 'Lihi Zelnik-Manor')\n","print(t_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2Pa23GJNUQ3H"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["PEWY6iMCOrxf","VlhwyVP3QbSU","CK0fRZjArvz4","sgM6xDo9P47w","rt7MW4cCP-Jo","43Ozad0mQCKZ","0GF_xG8UVOK6","ivicehnTyRRO","iIl812kGQR2a","z7SUA2oFqicu","b9c5CEQtVvm7","QBbYU5CCOrxk"],"provenance":[{"file_id":"1auhNPM8A6flAc5ai9GipgdsF6Xp2rRWQ","timestamp":1673365201119}],"toc_visible":true},"interpreter":{"hash":"ffc4551cdfa24de1e4d6ff6a879c16b8d7cadd8b756628488549bbbf21e2c19d"},"kernelspec":{"display_name":"Python 3.7.13 ('res')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"}},"nbformat":4,"nbformat_minor":0}